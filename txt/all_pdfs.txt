big data and cognitive computing
Review
Big Data and Its Applications in Smart Estate
and the Disaster Management Life Cycle:
A Systematic Analysis
Hafiz Suliman Munawar
1,
*, Siddra Qayyum
2
, Fahim Ullah
1
and Samad Sepasgozar
1
1
Faculty of Built Environment, University of South Wales, Kensington, Sydney, NSW 2052, Australia;
f.ullah@unsw.edu.au (F.U.); sepas@unsw.edu.au (S.S.)
2
School of Project Management, University of Sydney, Camperdown, Sydney, NSW 2006, Australia;
siddra.qayyum@sydney.edu.au
*Correspondence: h.munawar@unsw.edu.au; Tel.:+61-404-897-857
Received: 10 March 2020; Accepted: 24 March 2020; Published: 26 March 2020


Abstract:Big data is the concept of enormous amounts of data being generated daily in different fields
due to the increased of technology and internet sources. Despite the various advancements and
the hopes of better understanding, big data management and analysis remain a challenge, calling for
more rigorous and detailed research, as well as the identifications of methods and ways in which big
data could be tackled and put to good . The existing research lacks in discussing and evaluating
the pertinent tools and technologies to analyze big data in an efficient manner which calls for a
comprehensive and holistic analysis of the published articles to summarize the concept of big data
and see field-specific applications. To address this gap and keep a recent focus, research articles
published in last decade, belonging to top-tier and high-impact journals, were retrieved the
search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of
139 relevant research articles. Different analyses were conducted on the retrieved papers including
bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and
affiliated institutes contributing the most to the field of big data. The comparative analyses show that,
conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields
and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and
computing, data management, data refining, data patterns, and machine learning. The results further
show that major characteristics of big data can be summarized the seven Vs, which include
variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing
methods for big data analysis, their shortcomings, and the possible directions were also explored that
could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast
and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis,
and visualization of the large heterogeneous data, which can be tackled through authentication
such as Kerberos and encrypted files, logging of attacks, secure communication through Secure
Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models,
dividing computations into sub-tasks, checkpoint applications for recursive tasks, and Solid
State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big
data management, two frameworks exist including Hadoop and Apache Spark, which must be simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift,
and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e.,
smart estate and disaster management, were investigated, and a framework for field-specific
applications, as well as a merger of the two areas through big data, was highlighted. The proposed
frameworks show that big data can tackle the ever-present issues of customer regrets related to poor
quality of information or lack of information in smart estate to increase the customer satisfaction an intermediate organization that can process and keep a check on the data being provided to
the customers by the sellers and estate managers. Similarly, for disaster and its risk management,
Big Data Cogn. Comput.2020,4, 4; doi:10.3390/bdcc4020004www.mdpi.com/journal/bdcc Big Data Cogn. Comput.2020,4, 42 of 53
data from social media, drones, multimedia, and search engines can be to tackle natural disasters
such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger
framework for smart estate and disaster risk management show that big data generated from the
smart estate in the form of occupant data, facilities management, and building integration and
maintenance can be shared with the disaster risk management and emergency response teams to help
prevent, prepare, respond to, or recover from the disasters.
Keywords:big data; data analytics; machine learning; big data management; big data frameworks;
big data storage; smart estate management; property management; disaster management; disaster
risk management
1. Introduction
More than 2.5 quintillion bytes of data are generated every day, and it is expected that 1.7 MB of
data will be created by each person every second in 2020 [1,2]. This exponential growth in the rate of
data generation is due to increased of smart phones, computers, and social media. With the wide of technology, technological advancement, and acceptance, high-speed and massive data are being
generated in various forms, which are difficult to process and analyze [3], giving rise to the term “big
data”. Almost 95% of businesses are producing unstructured data, and they spent $187 billion dollars
in 2019 for big data management and analytics [4].
Big data is generated and in every possible field and walk of life, including marketing,
management, healthcare, business, and other ventures. With the introduction of techniques
and cost-effective solutions such as the data lakes, big data management is becoming increasingly
complicated and complex. Fang [5] defines data lake as a methodology enabled by a massive
data repository on low-cost technologies that improve the capture, refinement, archival, and
exploration of raw data within an enterprise. These data lakes are in line with the sustainability goals of
organizations, and they contain the mess of raw unstructured or multi-structured data that, for the most
part, have unrecognized value for the firm. This value, if recognized, can open sustainability-oriented
avenues for big data-reliant organizations. The of big data in technology and business is relatively
; however, many researchers are giving significant importance to it and found various useful
methods and tools to visualize the data [6]. To understand the generated data and make sense of it,
visualization techniques along with other pertinent technologies are , which help in understanding
the data through graphical means and in deducing results from the data [7]. It is worth highlighting
that data analyses are not limited to data visualizations only; however, the current paper focuses
on visualization aspects of data analyses. Furthermore, as data continue growing bigger and bigger,
traditional methods of information visualization are becoming outdated, inefficient, and handicapped
in analyzing this enormously generated data, thus calling for global attention to develop better, more
capable, and efficient methods for dealing with such big data [8,9]. Today, there is extensive of -time- applications, whose procedures require -time processing of the information
for which advanced data visualization methods of learning are . Systems operating on the
-time processing of the data need to be much faster and more accurate because the input data
are constantly generated at every instant, and results are required to be obtained in parallel [8]. Big
data has various applications in banking, smart estate, disaster risk management, marketing,
and healthcare industries, which are risky compared to other industries and require more reliability,
consistency, and effectiveness in the results, thus demanding more accurate data analytics tools [10,11].
Investments in big data analyses are baked with the aim of gaining a competitive edge in one’s own
field. For example, business having huge amounts of data and knowing how to these data to
their own advantage have leverage in the market to proceed toward their goals and leave behind
competitors. This includes attracting more customers, addressing the needs of existing ones, more Big Data Cogn. Comput.2020,4, 43 of 53
personalization, and data immersion to keep the customers motivated to their systems. Similarly,
every other field requires correct of information, which in turn requires tools and technologies that
could possibly ensure analysis of data clusters and patterns by arranging the available information in
an organized manner and isolating meaningful results from the large datasets.
The process of big data analytics constitutes a complete lifecycle comprising the identification of
data sources, data repository, data cleaning and noise reduction, data extraction, data validation, data
mining, and data visualizations [12]. The first stage deals with the identification of data sources and
pertinent data collection. In this stage, different data sources to collect the desired data are determined,
and data are gathered from them which is pertinent to the problem domain and severity. Data collected
from diverse resources contain more hidden patterns and relationships which are of interest to the
experts and may be in structured or unstructured form. Specialized tools and technologies are needed
to extract useful data, keywords, and information from these resources. In the second stage, these
data are stored in a database or a data repository NoSQL databases [13]. Organizations such
as Apache and Oracle developed various frameworks which allow analytic tools to get and process
data from these databases. The third stage deals with data cleaning and noise reduction [12]. In this
stage, the redundant, irrelevant, empty, or corrupt data objects are eliminated from the collected data,
which reduces the size, as well as complexity, of the data. The next stage deals with data extraction,
where the data in different or unidentified formats are extracted and transformed into a common or
compatible format, so that the data can be read and by the data analytics tool [14]. This also
involves extracting data from the relevant fields and delivering the data to the analytics engine to
decrease the data volume. In the fifth stage, validation rules, specific to the business case, are applied to
the data, to validate their relevance and need. This is a difficult task to perform due to the complexity
of the extracted data. To simplify the data processing in this step, an aggregation is applied,
which combines data from multiple sets into lesser numbers on same field names. At the sixth
stage, hidden and unique patterns from the data are established data mining techniques to make
important business decisions. These data mining methods depends on the nature of problem, which
can be predictive, descriptive, or diagnostic [14]. Finally, at the last stage, the data are visualized by
displaying the results of analysis in a graphical form, which makes it simple and easy to understand
for the viewers.
Big data analytics is a highly promising field in the present era; however, it presents several
challenges to the experts and professionals due to the inherent complexities and complicated operations.
These include problems related to the data discrepancy, redundancy, integrity, security, memory, space,
processing time, organization, visualization, and heterogeneous sources of data [15]. It is now quite
challenging to manage, organizes, and represent the huge data repositories in an efficient manner.
Similarly, data pre-processing methods transformation, noise reduction, filtering, and classification
have their own set of challenges. All these factors make the process of big data analysis even more
perplexing. To deal with the issues related to big data analytics and to bring ease in big data analysis
tasks, many tools and technologies were developed and released for mainstream . The aim of this
paper is to shed light on the concept of big data, specify its defining characteristics, and discuss the
current tools and technologies being for big data analytics. By performing a comparative analysis
of these tools, the paper gives concluding remarks about some of the best technologies developed
for efficient big data analytics. Overall, this paper reviews the basics of big data along with existing
methods of data analytics. Various stages data acquisition, storage, cleaning, and visualization are
involved in the data analytics process [16], which are discussed in this paper along with the comparison
of available tools for each stage. Due to the ability of learning patterns intelligently, machine learning
has a major role in data analytics [17], which is discussed in addition to the issues that surround
its usage. In addition, the challenges faced by big data analysis pertinent to storage or security are
also discussed.
Big data has applications in various filed including smart estate [18] and disaster
management [19], which were explored and highlighted in current study. These areas were selected Big Data Cogn. Comput.2020,4, 44 of 53 on their novelty, demand, and interrelationships or interdependencies. For example, among the
four key phases of disaster risk management, three (prevention, preparedness, and response) can be
addressed through big data originating from smart estate. estate managers keep a record
of the of people a facility through strata management and the associated facilities in
the buildings that can be helpful in preventing disasters or addressing them once they occur. Smart estate is receiving great attention from researchers around the world and is a nascent area that
was recently defined by Ullah . [18] as the usage of various electronic sensors to collect and
supply data to consumers, agents, and estate managers, which can be to manage assets
and resources efficiently in an urban area. Furthermore, the key focus of smart estate is on
disruptive technologies such as big data, making it a candidate for exploration in the current study.
Moreover, the regrets among customers of estate are increasing mainly due to the poor quality
of information provided to them through online means and platforms [20], which can be regulated
and enhanced through applications of big data. Furthermore, big data can be shared, integrated,
and mined for give people a deeper understanding of the status of smart estate operations and
help them make more informed decisions for renting or purchasing residential or commercial spaces
that can optimize the allocation of urban resources, reduce the operational costs, and promote a safe,
efficient, green, harmonious, and intelligent development of the smart estate and cities as a whole the seven Vs (variety, volume, variability, value, visualization, veracity, and velocity) of big
data [21]. Thus, it is imperative to investigate the applications of big data for addressing the information
needs of smart estate stakeholders. Similarly, disaster risk management is a critical area when it
comes to technological involvement and utilizations, especially for dealing with issues such as flood
detection, bushfires assessments, and associated rescue operations [19]. Disaster risk is the potential
loss of life, injury, or destroyed or damaged assets that occur for a system, society, or community in a
specific period. It is determined probabilistically as a of hazard, exposure, and capacity [22].
Disaster risk management is the application of disaster risk reduction policies and strategies, to prevent disaster risks, reduce existing disaster risks, and manage residual risks, contributing to the
strengthening of resilience and reduction of losses. The associated actions can be categorized into
prospective disaster risk management, corrective disaster risk management, and compensatory disaster
risk management [23,24]. There are four phases of disaster risk management: prevention, mitigation,
response, and recovery [25]. Enormous amounts of visual data are generated and changed during
each phase of disaster risk management, which makes it difficult for human-operated machinery
and equipment to analyze and respond accordingly. For example, for the prevention stage, Cheng . [26] presented the idea of Bluetooth-enabled sensors installed on building walls that can help
detect and prevent fire risks and hazards by sensing temperatures of the buildings and walls big
data analysis. Yang . [27] highlighted that social media- big data analytics and mining
can help mitigate ongoing disasters and reduce the associated risks. Ofli . [28] argued that aerial
imagery and drone- photography can be to respond to ongoing disasters, thereby reducing
the risks of potential life and property losses through human–computer-integrated machine learning
and other big data applications for disaster risk management. Similarly, Ragini . [29] proposed a
methodology to visualize and analyze the sentiments on the various basic needs of the people affected
by the disaster a combined subjective phrase and machine learning algorithm through social
media for ensuring big data- effective disaster response and recovery. Therefore, big data and
its associated technologies such as machine learning, image processing, artificial intelligence, and
drone- surveillance can help facilitate the rescue measures and help save lives and finances. While
there are several applications and potential uses of big data in disaster risk management and mitigation,
there are certain limitations as well. Disaster response needs more improved operations, and lack
of big data availability for supply networks is a major limitation [30]. Furthermore, it is challenging
for traditional disaster management systems to collect, integrate, and process large volumes of data
from multiple sources in time. Updating of the traditional systems may need additional finances,
which is a constraint for developing countries. Moreover, the constraint of generating results in a small Big Data Cogn. Comput.2020,4, 45 of 53
amount of time for emergency rescue and response, growing big data management issues, and limited
computational power makes the current traditional disaster management inadequate for the efficient
and successful application of high-tech big data systems [31]. The technical expertise and skill set
required for extracting fast, swift, and meaningful data from the available big data is another challenge
faced by the disaster risk management team.
2. Materials and Methods
A detailed literature retrieval was carried out a combination of different key words on
the search engines of some of the most common and popular academic search engines, indexing
published papers of high-impact-factor journals and top-tier conferences for each of the three focal
points: big data (S1), big data application in smart estate management (S2), and big data in disaster
management (S3). These search engines include Google Scholar, Scopus, IEEE Xplore, Elsevier, Scopus,
Science Direct, ACM, Springer, and MDPI for S1 and Scopus for S2 and S3. After choosing a set of
platforms for article retrieval, next step was to formulate a set of keywords or queries to be in the
search engines of platform. Different queries were formulated various key terms such as big
data, data analysis, datasets, data analytic tools, data volume, data variety, data handling, data usage,
and data creation for S1. Some resultant queries formulated these keywords were “big data”,
“big data analytic tools”, “big data volume”, “big data analysis”, etc. Similarly, for S2, the keywords
included “big data smart estate”, “big data smart property management”, “big data estate
management”, “big data estate development”, and “big data property development”. Lastly, for
S3, the keywords included “big data disaster management” and “big data in disaster”. The aim was
to extract research papers explaining the concept of big data and its most distinctive characteristics,
as well as articles proposing or discussing the existing analytic tools for big data for S1. For S2 and
S3 the aims were to check the applications of big data in smart estate and disaster management
respectively. Hence, the search queries were formed, keeping in mind the major objectives and the
research questions of this study. The search results revealed more than 200,000 articles published in the
last decade (2010–2020), which were subsequently narrowed down according to predefined inclusion
and exclusion criteria for S1. Upon narrowing down the results to identify the articles that fitted the
scope of current study, the search was further refined by themes and a combination of keywords
that revealed only the papers that were a perfect fit for the research questions of this study. As a result,
a total of 179,962 papers were retrieved on the refined themes and keywords. 1 illustrates
the methodology for collecting, screening, and filtering these research articles. Accordingly, for
S2 and S3, the numbers of initially retrieved articles were 1548 and 1261, respectively.
This paper adopts the systematic review , commonly as a useful in
relevant files of construction and property management [32,33], and it provides a high level of evidence
on the usefulness of big data techniques and the potential applications in the field. Furthermore, this
study also critically reviews some key papers and evaluates opinions and suggested applications.
Critical reviews are also widely in the field [34,35]. In this paper, the first step of the review
process was to query formulation where the search phrases “S1”, “S2”, and “S3” were defined. The OR
operation between terms shows that papers on at least one of these queries had to be retrieved.
After formulating a set of keywords, the queries were in the search engines of the highlighted
platforms to retrieve relevant journal and conference papers. These articles were filtered on
four predefined criteria, which were up-to-date focus (2010 and onward), presence of the keywords in
the title or abstract, English language, and no duplications. A final analysis was done by examining
the content of each article to verify its relevance and the need for this study by the research team,
comprising all the authors; it took four months to complete the task. After this step, a final set of
research articles were selected for further analyses and inclusion in the current study. Table 1 illustrates
the of articles that were selected at the end of the article retrieval phase 1 for S1, S2, and S3. In
subsequent phases, further shortlisting was performed, and the final of reviewed articles was
reduced accordingly. Big Data Cogn. Comput.2020,4, 46 of 53
Big Data Cogn. Comput. 2020, 4, 4 6 of 53 1. Methodology for shortlisting research articles for the study. Table 1. Initial article retrieval—phase 1 (year 2010–2020). Search Engine Search Phrases Articles Retrieved Out of Scope Search Phrases Articles Retrieved Out of Scope Search Phrases Articles Retrieved Out of Scope Google Scholar, ACM, Science Direct, IEEE Xplore, Springer, MDPI S1 202,895 12,993 - - - - - - Scopus, Elsevier S1* 26,739 7045 S2 2386 838 S3 1963 702 Total Articles 200,000 20,038 1548 838 1261 702 Final Retrieved 1,799,620 1548 1261 Note: S1: “Big Data” OR “Technology for big data filtering” OR “Refining big data”, S1*: (TITLE-ABS-KEY(Tools for big data analysis) OR (big data analytics tools) OR (big data visualization technologies) AND PUBYEAR > 2009, S2: (TITLE-ABS-KEY(big data estate ) OR (big data property management) OR (big data estate management) OR (big data estate development) OR (big data property development)) AND PUBYEAR > 2009, S3: (TITLE-ABS-KEY(big data disaster management) OR (big data disaster)) AND PUBYEAR > 2009. The aim of this paper is to shed light on big data analysis and methods, as well as point toward the directions that can possibly be achieved with the rise in technological means available to us for analyzing data. In addition, the applications of big data in newly focused smart estate and the high demand in disaster and risk management are also explored on the reviewed literature. The enormity of papers present exploring big data were linked with the fact that, each year, from 2010 and onward, the of original research articles and reviews exponentially increased. A keyword analysis was performed the VosViewer software for the articles retrieved to highlight the focus of the big data articles published during the last decade. The results shown in 2 highlight that the most repeated keywords in these articles comprised data analytics, data handling, data visualization tools, data mining, artificial intelligence, machine learning, and others. Thus, 2 highlights the focus of the big data research in last decade. 1.Methodology for shortlisting research articles for the study.
Table 1.Initial article retrieval—phase 1 (year 2010–2020).
Search Engine
Search
Phrases
Articles
Retrieved
Out of
Scope
Search
Phrases
Articles
Retrieved
Out of
Scope
Search
Phrases
Articles
Retrieved
Out of
Scope
Google Scholar,
ACM, Science
Direct, IEEE
Xplore,
Springer, MDPI
S1202,89512,993------
Scopus,
Elsevier
S1*26,7397045S22386838S31963702
Total Articles200,00020,03815488381261702
Final Retrieved1,799,62015481261
Note: S1: “Big Data” OR “Technology for big data filtering” OR “Refining big data”, S1*: (TITLE-ABS-KEY(Tools
for big data analysis) OR (big data analytics tools) OR (big data visualization technologies) AND PUBYEAR>
2009, S2: (TITLE-ABS-KEY(big data estate ) OR (big data property management) OR (big data estate
management) OR (big data estate development) OR (big data property development)) AND PUBYEAR>2009,
S3: (TITLE-ABS-KEY(big data disaster management) OR (big data disaster)) AND PUBYEAR>2009.
The aim of this paper is to shed light on big data analysis and methods, as well as point toward
the directions that can possibly be achieved with the rise in technological means available to us
for analyzing data. In addition, the applications of big data in newly focused smart estate and the
high demand in disaster and risk management are also explored on the reviewed literature. The
enormity of papers present exploring big data were linked with the fact that, each year, from 2010 and
onward, the of original research articles and reviews exponentially increased. A keyword
analysis was performed the VosViewer software for the articles retrieved to highlight the focus of
the big data articles published during the last decade. The results shown in 2 highlight that the
most repeated keywords in these articles comprised data analytics, data handling, data visualization
tools, data mining, artificial intelligence, machine learning, and others. Thus, 2 highlights the
focus of the big data research in last decade. Big Data Cogn. Comput.2020,4, 47 of 53
Big Data Cogn. Comput. 2020, 4, 4 7 of 53 2. Most frequent keywords in the big data articles from 2010 to 2020. 3 presents similar analyses to 2 for S2 and highlights that, in the case of the focus on smart estate and property management, recent literature revolves around keywords such as housing, decision-making, urban area, forecasting, data mining, behavioral studies, human–
computer interactions, artificial intelligence, energy utilizations, economics, learning system, data mining, and others. This shows a central focus on data utilizations for improving human decisions, which is in line with recent articles such as Ullah . [18], Felli . [36], and Ullah . [20], where it was highlighted that smart estate consumers and tenants have regrets related to their buy or rent decisions due to the poor quality or lack of information provided to them. 2.Most frequent keywords in the big data articles from 2010 to 2020. 3 presents similar analyses to 2 for S2 and highlights that, in the case of the focus
on smart estate and property management, recent literature revolves around keywords such as
housing, decision-making, urban area, forecasting, data mining, behavioral studies, human–computer
interactions, artificial intelligence, energy utilizations, economics, learning system, data mining, and
others. This shows a central focus on data utilizations for improving human decisions, which is in
line with recent articles such as Ullah . [18], Felli . [36], and Ullah . [20], where it was
highlighted that smart estate consumers and tenants have regrets related to their buy or rent
decisions due to the poor quality or lack of information provided to them. Big Data Cogn. Comput.2020,4, 48 of 53
Big Data Cogn. Comput. 2020, 4, 4 8 of 53 3. Most frequent keywords in the big data articles on estate and property from 2010 to 2020. 4 shows the same analyses for S3, where the keywords published in retrieved articles are highlighted and linked for the last decade on the integration of big data applications for disaster and its risk management. Keywords such as information management, risk management, social networking, artificial intelligence, machine learning, floods, remote sensing, data mining, digital storage, smart city, learning systems, and GIS are evident from 4. Again, these keywords focus on the area of information management and handling for addressing the core issues such as disaster management and disaster risk reduction. 3.Most frequent keywords in the big data articles on estate and property from 2010
to 2020. 4 shows the same analyses for S3, where the keywords published in retrieved articles are
highlighted and linked for the last decade on the integration of big data applications for disaster and its
risk management. Keywords such as information management, risk management, social networking,
artificial intelligence, machine learning, floods, remote sensing, data mining, digital storage, smart
city, learning systems, and GIS are evident from 4. Again, these keywords focus on the area of
information management and handling for addressing the core issues such as disaster management
and disaster risk reduction. Big Data Cogn. Comput.2020,4, 49 of 53
Big Data Cogn. Comput. 2020, 4, 4 9 of 53 4. Most frequent keywords in the big data articles on disaster management from 2010 to 2020. 5 presents the rough trend that was initially observed when narrowing down papers needed for the temporal review. A steep rise in big data can be seen in the years 2013–2014, 2015–
2016, and 2017–2018, while a less substantial incline was seen in 2016–2017. From here onward, the search was further refined, and only those papers which truly suited the purpose of this review were selected. 5. Big data papers published per year as indexed on Scopus and Web of Science from 2010–
2019. 4.Most frequent keywords in the big data articles on disaster management from 2010
to 2020. 5 presents the rough trend that was initially observed when narrowing down papers
needed for the temporal review. A steep rise in big data can be seen in the years 2013–2014, 2015–2016,
and 2017–2018, while a less substantial incline was seen in 2016–2017. From here onward, the search
was further refined, and only those papers which truly suited the purpose of this review were selected.
Big Data Cogn. Comput. 2020, 4, 4 9 of 53 4. Most frequent keywords in the big data articles on disaster management from 2010 to 2020. 5 presents the rough trend that was initially observed when narrowing down papers needed for the temporal review. A steep rise in big data can be seen in the years 2013–2014, 2015–
2016, and 2017–2018, while a less substantial incline was seen in 2016–2017. From here onward, the search was further refined, and only those papers which truly suited the purpose of this review were selected. 5. Big data papers published per year as indexed on Scopus and Web of Science from 2010–
2019. 5.Big data papers published per year as indexed on Scopus and Web of Science from 2010–2019. Big Data Cogn. Comput.2020,4, 410 of 53 5 also shows and confirms the recent focus of researchers on big data, as well as its
analytics and management. Thus, the argument of focusing the review on the last decade was further
strengthened and verified as per the results of reviewed papers, where the growth since 2010 can be seen
in terms of published articles on the retrieval criteria defined and utilized in the current study.
From fewer than 200 articles published in the year 2010 to more than 1200 in 2019, the big data articles
saw tremendous growth, pointing to the recent focus and interests of the researchers. In addition to
this, GoogleTrends, an investigation was carried out with the search filters of worldwide search
and time restricted from 1 January 2010 to 1 March 2020 to show the recent trends of search terms,
big data, disaster big data, and estate big data, as shown in 6. The comparison shows the
monthly trends for disaster-related big data and estate big data searches, highlighting that estate-related big data searches (47) were double the searches for disaster big data (23). A significant
rise can be seen in big data for estate papers during February–April 2014, September–November
2016, and July–September 2018. Similarly, for big data usage in disaster management, spikes in the
trend can be seen during mid-2013, late 2014, mid-2015, early 2017, and early 2018. The is also
consistent with the big data trend in 2, where an average of publications occurred in
2016–2017. It is no surprise that the search patterns peaked in 2016–2017 and, as a result, many articles
were published and ultimately retrieved in the current study.
Big Data Cogn. Comput. 2020, 4, 4 10 of 53 5 also shows and confirms the recent focus of researchers on big data, as well as its analytics and management. Thus, the argument of focusing the review on the last decade was further strengthened and verified as per the results of reviewed papers, where the growth since 2010 can be seen in terms of published articles on the retrieval criteria defined and utilized in the current study. From fewer than 200 articles published in the year 2010 to more than 1200 in 2019, the big data articles saw tremendous growth, pointing to the recent focus and interests of the researchers. In addition to this, GoogleTrends, an investigation was carried out with the search filters of worldwide search and time restricted from 1 January 2010 to 1 March 2020 to show the recent trends of search terms, big data, disaster big data, and estate big data, as shown in 6. The comparison shows the monthly trends for disaster-related big data and estate big data searches, highlighting that estate-related big data searches (47) were double the searches for disaster big data (23). A significant rise can be seen in big data for estate papers during February–April 2014, September–November 2016, and July–September 2018. Similarly, for big data usage in disaster management, spikes in the trend can be seen during mid-2013, late 2014, mid-2015, early 2017, and early 2018. The is also consistent with the big data trend in 2, where an average of publications occurred in 2016–2017. It is no surprise that the search patterns peaked in 2016–2017 and, as a result, many articles were published and ultimately retrieved in the current study. 6. Worldwide big data, disaster big data, and estate big data search trends in last decade. The next stage was on screening the retrieved articles on well-defined criteria on four rules. Firstly, only articles published from 1 January 2010 and onward were selected, because the aim was to keep a recent focus and to cover articles published in the last decade, as the concept of big data and its usage became common only recently, and the last few years saw a rapid rise in technologies being developed for big data management and analysis. Secondly, only articles written in the English language were selected; thus, articles written in any other language were excluded. Thirdly, only journal articles including original research papers and reviews were included. Articles written as letters, editorials, conference papers, webpages, or any other nonstandard format were eliminated. Lastly, no duplicate or redundant articles could be present and, thus, when the same article was retrieved from multiple search engines or sources, it was discarded. Finally, a total of 182 published articles were narrowed down after the screening phase for S1 (135), 18 for S1* and 28 for S2, and 19 for S3. These papers were then critically analyzed one by one to determine their fit within the scope of the research objectives and questions, with the aim of bringing the existence of big data to light in such a way that the concept of big data in the modern world could be understood. 6.Worldwide big data, disaster big data, and estate big data search trends in last decade.
The next stage was on screening the retrieved articles on well-defined criteria on four rules. Firstly, only articles published from 1 January 2010 and onward were selected, because
the aim was to keep a recent focus and to cover articles published in the last decade, as the concept
of big data and its usage became common only recently, and the last few years saw a rapid rise in
technologies being developed for big data management and analysis. Secondly, only articles written
in the English language were selected; thus, articles written in any other language were excluded.
Thirdly, only journal articles including original research papers and reviews were included. Articles
written as letters, editorials, conference papers, webpages, or any other nonstandard format were
eliminated. Lastly, no duplicate or redundant articles could be present and, thus, when the same
article was retrieved from multiple search engines or sources, it was discarded. Finally, a total of
182 published articles were narrowed down after the screening phase for S1 (135), 18 for S1* and 28
for S2, and 19 for S3. These papers were then critically analyzed one by one to determine their fit
within the scope of the research objectives and questions, with the aim of bringing the existence of Big Data Cogn. Comput.2020,4, 411 of 53
big data to light in such a way that the concept of big data in the modern world could be understood.
Subsequently, the roots of big data, how data are generated, and the enormity of data existing today
were identified and tabulated as a result of the rigorous review, along with the applications in smart estate, property, and disaster risk management. This was followed by reviewing and tabulating
the big data tools which currently exist for analyzing and sorting the big data. After critical analysis,
out of the previously shortlisted 182 papers, 139 were selected to be reviewed in greater detail. This
shortlist procedure included papers focusing on big data reviews, big data tools and analytics, and
big data in smart estate and disaster management. Short papers, editorial notes, calls for issues,
errata, discussions, and closures were excluded from the final papers reviewed for content analyses.
These papers were not only reviewed for their literature but were also critically analyzed for the
information they provide and the leftover gaps that may require addressing in the future. To follow a
systematic review , the retrieved articles were divided into three major groups of “big data”,
“big data analytic tools and technologies”, and “applications of big data in smart estate, property
and disaster management”. The papers belonging to the big data category explore the concept of big
data, as well as its definitions, features, and challenges. The second category of papers introduces or
discusses the tools and technologies for effective and efficient analysis of big data, thus addressing
the domain of big data analytics. Table 2 presents the distribution of articles retrieved in each phase,
among these two categories.
Table 2.
Articles retrieved after each phase, as well as those filtered and shortlisted for final
content analyses.
Categories/PhaseArticles RetrievalFiltered ArticlesFinal Content Analyses
Big data concepts and definitions75,2435233
Big data analytic tools/technologies104,7198359
Applications of big data in smart estate,
property and disaster management
28094747
Total articles182,771182139
Note: The filters applied were publications from 2010 and onward, the presence of keywords in the title or abstract,
English language, and no duplications. Exclusions included short papers, editorial notes, calls for issues, errata,
discussions, and closures.
3. Results
3.1. Review Results
Once the 139 articles were shortlisted, different analyses were conducted on these retrieved articles.
Firstly, the articles were divided into five types: original research and big data technologies, review,
conference, case study, and others, as shown in 7. Expectedly, the shortlisted articles mainly
focused on big data technologies (59), followed by others (29), review (23), conference (18), and case
study (10). Similar analyses were conducted by Martinez-Mosquera . [37]; however, none of
the previously published articles explored big data applications in the context of smart estate or
disaster and risk management, which is the novelty of the current study. The current study further
provides an integrated framework for the two fields. Big Data Cogn. Comput.2020,4, 412 of 53
Big Data Cogn. Comput. 2020, 4, 4 12 of 53 7. Article types reviewed in the study. After classification of articles into different types, keyword analyses were conducted to highlight the most repeated keywords in the journals. These were taken from the keywords mentioned under the keyword categories in the investigated papers. A minimum inclusion criterion of at least 10 occurrences was for shortlisting the most repeated keywords. When performing the analysis, some words were merged and counted as single terms; for example, the terms data and big data were merged since all the papers focused on big data. Similarly, the terms disaster, disaster management, earthquake, and natural disaster were merged and included in disaster risk management. The relevance score in Table 3 was calculated by dividing the of occurrences of a term by the total occurrences to highlight its share. Table 3. Most repeated keywords in the big data papers from 2010–2020. Term Occurrences Relevance Score Analysis system 37 0.26 Investigation 27 0.20 Disaster risk management 26 0.19 Big data 23 0.16 estate technologies and urban area 16 0.12 Implementation challenges 10 0.07 After highlighting the most repeated keywords, journals contributing the most to the shortlisted papers were studied. Table 4 shows the top five journals/sources from which the articles were retrieved. An inclusion criterion of at least 15 documents was applied as the filter for shortlisting the top sources. Consequently, the majority of articles hailed from lecture notes in computer science followed by IOP conference series and others. Table 4. Top sources on of papers reviewed from 2010–2020. Source Documents Citations Lecture notes in computer science 27 35 IOP conference series: earth and environmental science 21 27 ACM international conference proceeding series 19 4 Advances in intelligent systems and computing 17 4 IEEE international conference on big data, big data 2017 16 27 Others 39 288 Similarly, once the sources were highlighted, the following analyses were aimed at highlighting the top contributing authors, countries, and organizations contributing to the study area. 8 shows the contributions by authors in terms of the of documents and their citations. A 0
10
20
30
40
50
60
Original
Research/Big
Data
Technologies
ReviewConferenceCase studyOthers 7.Article types reviewed in the study.
After classification of articles into different types, keyword analyses were conducted to highlight
the most repeated keywords in the journals. These were taken from the keywords mentioned under the
keyword categories in the investigated papers. A minimum inclusion criterion of at least 10 occurrences
was for shortlisting the most repeated keywords. When performing the analysis, some words
were merged and counted as single terms; for example, the terms data and big data were merged since
all the papers focused on big data. Similarly, the terms disaster, disaster management, earthquake,
and natural disaster were merged and included in disaster risk management. The relevance score in
Table 3 was calculated by dividing the of occurrences of a term by the total occurrences to
highlight its share.
Table 3.Most repeated keywords in the big data papers from 2010–2020.
TermOccurrencesRelevance Score
Analysis system370.26
Investigation270.20
Disaster risk management260.19
Big data230.16 estate technologies and urban area160.12
Implementation challenges100.07
After highlighting the most repeated keywords, journals contributing the most to the shortlisted
papers were studied. Table 4 shows the top five journals/sources from which the articles were retrieved.
An inclusion criterion of at least 15 documents was applied as the filter for shortlisting the top sources.
Consequently, the majority of articles hailed from lecture notes in computer science followed by IOP
conference series and others. Big Data Cogn. Comput.2020,4, 413 of 53
Table 4.Top sources on of papers reviewed from 2010–2020.
SourceDocumentsCitations
Lecture notes in computer science2735
IOP conference series: earth and
environmental science
2127
ACM international conference
proceeding series
194
Advances in intelligent systems and
computing
174
IEEE international conference on big
data, big data 2017
1627
Others39288
Similarly, once the sources were highlighted, the following analyses were aimed at highlighting the
top contributing authors, countries, and organizations contributing to the study area. 8 shows
the contributions by authors in terms of the of documents and their citations. A minimum of six documents with at least six citations was the filter applied to shortlist these authors.
Big Data Cogn. Comput. 2020, 4, 4 13 of 53 minimum of six documents with at least six citations was the filter applied to shortlist these authors. 8. Authors’ names, as well as the of documents and citations, of the 139 reviewed papers from 2010 to 2020. After highlighting the top contributing authors, countries with top contributions to the field of big data were investigated, as shown in 9. A minimum inclusion criterion was set at 10 documents from a specific country among the shortlisted papers. The race is led by China with 34 papers, followed by the United States of America (USA) with 24 papers among the shortlist. However, when it comes to the citations, the USA is leading with 123 citations, followed by China with 58 citations. 9. Country-wise contributions to and citations of the 139 reviewed articles from 2010 to 2020. After highlighting the top countries contributing to the field of big data and its applications to estate and disaster management, in the next step, affiliated institutes were investigated for authors contributing to the body of knowledge. A minimum inclusion criterion of three articles was set as the shortlist limit. Table 5 shows the list of organizations with the of documents contributed by them and the associated citations to date. This is led by Japan, followed by the USA, in terms of of citations, with a tie for the of papers, i.e., six documents were discovered for these countries. 0
10
20
30
40
Wang Y.
Wang J.
Li Z.
Liu Y.
Li X.Li Y.
Wang H.
Zhang Y.
Zhang H.
Zhang X.
Shibasaki R.
Chen H.
Zhang J.
Li M.
Wang C. of DocumentsCitations
0
20
40
60
80
100
120
140
ChinaUnited
States
JapanIndiaUnited
Kingdom
IndonesiaSouth
Korea
DocumentsCitations 8.Authors’ names, as well as the of documents and citations, of the 139 reviewed
papers from 2010 to 2020.
After highlighting the top contributing authors, countries with top contributions to the field of big
data were investigated, as shown in 9. A minimum inclusion criterion was set at 10 documents
from a specific country among the shortlisted papers. The race is led by China with 34 papers, followed
by the United States of America (USA) with 24 papers among the shortlist. However, when it comes to
the citations, the USA is leading with 123 citations, followed by China with 58 citations.
Big Data Cogn. Comput. 2020, 4, 4 13 of 53 minimum of six documents with at least six citations was the filter applied to shortlist these authors. 8. Authors’ names, as well as the of documents and citations, of the 139 reviewed papers from 2010 to 2020. After highlighting the top contributing authors, countries with top contributions to the field of big data were investigated, as shown in 9. A minimum inclusion criterion was set at 10 documents from a specific country among the shortlisted papers. The race is led by China with 34 papers, followed by the United States of America (USA) with 24 papers among the shortlist. However, when it comes to the citations, the USA is leading with 123 citations, followed by China with 58 citations. 9. Country-wise contributions to and citations of the 139 reviewed articles from 2010 to 2020. After highlighting the top countries contributing to the field of big data and its applications to estate and disaster management, in the next step, affiliated institutes were investigated for authors contributing to the body of knowledge. A minimum inclusion criterion of three articles was set as the shortlist limit. Table 5 shows the list of organizations with the of documents contributed by them and the associated citations to date. This is led by Japan, followed by the USA, in terms of of citations, with a tie for the of papers, i.e., six documents were discovered for these countries. 0
10
20
30
40
Wang Y.
Wang J.
Li Z.
Liu Y.
Li X.Li Y.
Wang H.
Zhang Y.
Zhang H.
Zhang X.
Shibasaki R.
Chen H.
Zhang J.
Li M.
Wang C. of DocumentsCitations
0
20
40
60
80
100
120
140
ChinaUnited
States
JapanIndiaUnited
Kingdom
IndonesiaSouth
Korea
DocumentsCitations 9.Country-wise contributions to and citations of the 139 reviewed articles from 2010 to 2020. Big Data Cogn. Comput.2020,4, 414 of 53
After highlighting the top countries contributing to the field of big data and its applications to estate and disaster management, in the next step, affiliated institutes were investigated for authors
contributing to the body of knowledge. A minimum inclusion criterion of three articles was set as the
shortlist limit. Table 5 shows the list of organizations with the of documents contributed by
them and the associated citations to date. This is led by Japan, followed by the USA, in terms of of citations, with a tie for the of papers, i.e., six documents were discovered for these countries.
Table 5.List of affiliated organizations and the of contributing documents included in the 139
reviewed articles from 2010 to 2020.
OrganizationDocumentsCitations
Center for Spatial Information Science, University of Tokyo, Japan680
School of Computing and Information Sciences, Florida International University,
Miami, Fl 33199, United States
647
International Research Institute of Disaster Science (Irides), Tohoku University,
Aoba 468-1, Aramaki, Aoba-Ku, Sendai, 980-0845, Japan
510
University of Tokyo, Japan434
Earthquake Research Institute, University of Tokyo, Tokyo, Japan414
Department of Geography, University of Wisconsin-Madison, Madison, Wi 53706,
United States
361
Department of Computing Science, University of Aberdeen, Aberdeen, United
Kingdom
335
Department of Geography, University of South Carolina, Columbia, Sc 29208,
United States
334
School of Remote Sensing and Information Engineering, Wuhan University,
Wuhan, 430079, China
313
National Institute of Informatics, Tokyo, Japan39
State Key Laboratory of Information Engineering in Surveying, Mapping and
Remote Sensing, Wuhan University, Wuhan, 430079, China
38
Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences,
Beijing, 100094, China
34
Department of Computer and Information Sciences, Fordham University, York, Ny 10458, United States
33
School of Computer Science and Technology, Guangzhou University, Guangzhou,
510006, China
33
Research Institute of Electrical Communication, Tohoku University, Sendai, Japan30
University of Chinese Academy of Sciences, Beijing, 100049, China30
3.2. Big Data and Its Seven Vs
Big data is the name given to datasets containing large, varied, and complex structures with issues
related to storage, analysis, and visualization for data processing [7]. Massive amounts of data are
generated from a variety of sources audios, videos, social networking, sensors, and mobile phones,
which are stored in the form of databases that require different applications for the analyses [38]. Big
data is characterized by its high volume, sharing, creation, and removal in seconds, along with the high
inherent variations and complexities [16]. Thus, it can be structured, unstructured, or semi-structured
and vary in the form of , audio, image, or video [39]. Previously, methods for the storage
and analysis of big data were slow in speed because of the low processing capabilities and lack of
technology. Until 2003, humans were able to create a mere five exabytes, whereas, today, in the era of
disruption and technological advancements, the same amount of data is created in the span of two
days. The rapidness of data creation comes with a set of difficulties that occur in storage, sorting, and
categorization of such big data. The expansion of data usage and generation reaches its heights today,
and, in 2013, the data were reported to be 2.72 zettabytes, exponentially increasing to date [6]. Big Data Cogn. Comput.2020,4, 415 of 53
Initially, big data was characterized by its variety, volume, and velocity, which were known as
the three Vs of data [6]; however, later value and veracity were later added to the previously defined
aspects of the data [40]. Recently, variability and visualization were also added to the characteristics of
big data by Sheddon . [41]. These seven Vs along with the hierarchy, integrity, and correlation
can help integrate the functions of smart estate including safe, economical, and more intelligent
operation, to help the customers make better and more informed decisions [21]. These seven Vs for
defining the characteristics of big data are illustrated and summarized in 10. Each of these Vs is
explained in the subsequent sections.
Big Data Cogn. Comput. 2020, 4, 4 15 of 53 10. The seven Vs of big data. 3.2.1. Variety Variety is one of the important characteristics of big data that refers to the collection of data from different sources. Data vary greatly in the form of images, audio, videos, numbers, or [39], forming heterogeneity in the datasets [42]. Structured data refer to the data present in tabular form in spreadsheets, and these data are easy to sort because they are already tagged, whereas , images, and audio are examples of unstructured data that are random and relatively difficult to sort [6]. Variety not only exist in formats and data types but also in different kinds of uses and ways of analyzing the data [43]. Different aspects of the variety of big data are summarized in Table 6. The existence of data in diverse shapes and forms adds to its complexity. Therefore, the concept of a relational database is becoming absurd with the growing diversity in the forms of data. Thus, integration or the big data directly in a system is quite challenging. For example, on the worldwide web (WWW), people various browsers and applications which change the data before sending them to the cloud [44]. Furthermore, these data are entered manually on the interface and are, therefore, more prone to errors, which affects the data integrity. Thus, variety in data implies more chances of errors. To address this, the concept of data lakes was proposed to manage the big data, which provides a schema-less repository for raw data with a common interface; however, this is prone to data swamping if the data are just dumped into a data lake without any metadata management. Tools such as Constance were proposed and highlighted by Hai . [45] for sophisticated metadata management over raw data extracted from heterogeneous data sources. on three functional layers of ingestion, maintenance, and querying, Constance can implement the interface between the data sources and enable the major human–machine interaction, as well as dynamically and incrementally extract and summarize the current metadata of the data lake that can help address and manage disasters and the associated risks [46]. Such data lakes can be integrated with urban big data for smarter estate management, where, just the human and non-human resources of smart estate, urban big data also emerge as an important strategic resource for the development of intelligent cities and strategic directions [21]. Such urban big data can be converged, analyzed, and mined with depth via the Internet of things, cloud computing, and artificial intelligence technology to achieve the goal of intelligent administration of smart estate. 10.The seven Vs of big data.
3.2.1. Variety
Variety is one of the important characteristics of big data that refers to the collection of data
from different sources. Data vary greatly in the form of images, audio, videos, numbers, or [39],
forming heterogeneity in the datasets [42]. Structured data refer to the data present in tabular form in
spreadsheets, and these data are easy to sort because they are already tagged, whereas , images,
and audio are examples of unstructured data that are random and relatively difficult to sort [6]. Variety
not only exist in formats and data types but also in different kinds of uses and ways of analyzing the
data [43]. Different aspects of the variety of big data are summarized in Table 6. The existence
of data in diverse shapes and forms adds to its complexity. Therefore, the concept of a relational
database is becoming absurd with the growing diversity in the forms of data. Thus, integration or the big data directly in a system is quite challenging. For example, on the worldwide web
(WWW), people various browsers and applications which change the data before sending them
to the cloud [44]. Furthermore, these data are entered manually on the interface and are, therefore,
more prone to errors, which affects the data integrity. Thus, variety in data implies more chances of
errors. To address this, the concept of data lakes was proposed to manage the big data, which provides
a schema-less repository for raw data with a common interface; however, this is prone to data
swamping if the data are just dumped into a data lake without any metadata management. Tools such
as Constance were proposed and highlighted by Hai . [45] for sophisticated metadata management
over raw data extracted from heterogeneous data sources. on three functional layers of ingestion, Big Data Cogn. Comput.2020,4, 416 of 53
maintenance, and querying, Constance can implement the interface between the data sources and
enable the major human–machine interaction, as well as dynamically and incrementally extract and
summarize the current metadata of the data lake that can help address and manage disasters and the
associated risks [46]. Such data lakes can be integrated with urban big data for smarter estate
management, where, just the human and non-human resources of smart estate, urban big data
also emerge as an important strategic resource for the development of intelligent cities and strategic
directions [21]. Such urban big data can be converged, analyzed, and mined with depth via the Internet
of things, cloud computing, and artificial intelligence technology to achieve the goal of intelligent
administration of smart estate.
Table 6.The seven Vs, as well as their key aspects and context of usage.
The 7 Vs and Definitions
Types of Data and How to
Tackle/Handle Them
Context and Features
Variety refers to the structural
heterogeneity in a dataset. [6,39,42,43]
Structured
Semi-structured
Unstructured: multi-model DBMS
Form: comprises different forms of data,
, images, audio, video and social
media
Structure: structured and unstructured
data
Volume refers to the scale of data or large
amount of data generated every second
[6,16,39,42]
Machine generated data:
: stream data or progressive
loading
Scale: scale of data coming from various
sources
Size: large size (terabytes and petabytes)
Magnitude: large magnitude
Velocity refers to the ability to successfully
process data at a high speed [6,39,42,43]
Incremental and streaming processing
Speed: speed of data generation, speed of
data processing
Rate: rate of data generation, rate of
change of data
Value refers to the value found out of big
data (i.e., customer wants, trends, needs)
[39,42,43]
Performance tools, analytics tools,
personal experience and analysis.
Patterns: hidden patterns and
dependencies in data
Decision-making: ability of data to make
accurate decision
Usefulness: ability of data to provide
useful information and knowledge
Veracity refers to the extent to which the
data are accurate, precise, and applicable
without having any anomaly [39,42,43]
Data cleaning tools (Trifacta Wrangler,
Drake, TIBCO Clarity, Winpure, Data
Ladder, Data Cleaner)
Uses: datasets are for
decision-making
Uncertainty: uncertainty or inaccuracy of
data
Unreliability: unreliability inherent in big
data
Ambiguity: incompleteness, ambiguity
and incoherency of big data
Variability refers to inconsistencies in the
data and the speed at which big data is
loaded in your database [41–43]
Statistical tools to measure range,
interquartile range, variance, and
standard deviation
Opportunities: dynamic opportunities
that are available by interpreting
unstructured data
Variation: variation in the rate of flow of
data
Irregularity: irregularity, periodicity and
incoherence of big data
Interpretations: changing meaning of the
data due to different interpretations
Visualization refers to the representation
of data in different visual forms, such as
data clustering, tree maps, circular
network diagrams [9,39,43,47]
Visualization tools, such as: Google
Charts, Tableau, Grafana, Chartist.js,
FusionCharts, Datawrapper, Infogram,
ChartBlocks, and D3.
Uses: statistical models, graphics, and
databases to plot data
Modeling: modeling and graphical
analysis of big data to depict relationships
and decisions
Interpretations: interpreting trends and
patterns present in big data
Artistic display: display -time changes
and relationships within data in artistic
ways
3.2.2. Volume
Volume is another key of big data which is defined as the generation of data every
second in huge amounts. It is formed by the amount of data collected from different sources, which Big Data Cogn. Comput.2020,4, 417 of 53
require rigorous efforts, processing, and finances. Currently, data generated from machines are large in
volume and are increasing from gigabytes to petabytes. An estimate of 20 zettabytes of data creation
is expected by the end of 2020, which is 300 times more than that of 2005 [39]. Thus, traditional
methods for storage and analysis of data are not suitable for handling today’s voluminous data [6].
For examples, it was reported that, in one second, almost one million photographs are processed by
Facebook, and it stores 260 billion photographs, which takes storage space of more than 20 petabytes,
thus requiring sophisticated machines with exceptional processing powers to handle such data [42].
Data storage issues are solved, to some extent, by the of cloud storage; however, this adds the risk
of information security, as well as data and privacy breaches, to the set of worries [16].
The big volume of data is created from different sources such as , images, audio, social media,
research, healthcare, weather reports etc. For example, for a system dealing with big data, the data could
come from social media, satellite images, web servers, and audio broadcasts that can help in disaster
risk management. Traditional ways of data handling such as the SQL cannot be in this case as the
data are unorganized and heterogeneous and contain unknown variables. Similarly, unstructured data
cannot be directly arranged into tables before usage in a relational database management system such
as Oracle. Moreover, such unstructured data have a volume in the range of petabytes, which creates
further problems related to storage and memory. The volume of big data is summarized in
Table 6 where a coherence of terms can be seen in most of the reviewed studies.
Smart estate organizations such as Vanke Group and Fantasia Group in China are big data applications for handling a large volume of estate data [48]. Fantasia came up with an
e-commerce platform that combines commercial tenants with customers through an app on cell phones.
This platform holds millions of homebuyers’ data that help Fantasia in efficient digital marketing,
as well as improving the financial sector, hotel services, culture, and tourism. Similarly, big data
applications help Vanke Group by handling a volume of 4.8 million property owners. After data
processing, Vanke put forward the concept of building city support services, combining community
logistics, medical services, and pension with these property owners’ big data.
3.2.3. Velocity
The speed of data generation and processing is referred to as the velocity of big data. It is defined
as the rate at which data are created and changed along with the speed of transfer [39]. -time
streaming data collected from websites represent the leading edge provided by big data [43]. Sensors
and digital devices mobile phones create data at an unparalleled rate, which need -time
analytics for handling high-frequency data. Most retailers generate data at a very high speed; for
example, almost one million transactions are processed by Walmart in one hour, which are to
gather customer location and their past buying patterns, which help manage the creation of customer
value and personalized suggestions for the customers [42]. Table 6 summarizes the key aspects of
velocity, presented by researchers.
Many authors defined velocity as the rate at which the data are changing, which may change
overnight, monthly, or annually. In the case of social media, the data are continuously changing at
a very fast pace. information is shared on sites such as Facebook, Twitter, and YouTube every
second, which can help disaster managers plan for upcoming disasters and associated risk, as well
as know the current impacts of occurring disasters. For example, Ragini . [29] highlighted that
sentiment analyses from social media big data analytic tools such as machine learning can be
helpful to know the needs of people facing a disaster for devising and implementing a more holistic
response and recovery plan. Similarly, Huang . [49] introduced the concept of DisasterMapper, a
CyberGIS framework that can automatically synthesize multi-sourced data from social media to track
disaster events, produce maps, and perform spatial and statistical analysis for disaster management.
A prototype was implemented and tested the 2011 Hurricane Sandy as a case study, which
recorded the disasters on hashtags posted by people social media. In all such systems, the
velocity of processing remains a top priority. Hence, in the current era, the rate of change of data is in Big Data Cogn. Comput.2020,4, 418 of 53 time, and night batches for data update are not applicable. The fast rate of change of data requires
a faster rate of accessing, processing, and transferring this data. Owing to this, business organizations
now need to make -time data-driven decisions and perform agile execution of actions to cope
with the high rate of change of such enormous data. In this context, for smart estate, Cheng . [50] proposed a big data-assisted customer analysis and advertising architecture that speeds
up the advertising process, approaching millions of users in single clicks. The results of their study
showed that, 360-degree portrait and segmentation, customer mining, and modified and
personalized precise advertising delivery, the model can reach a high advertising arrival rate, as well
as a superior advertising exposure/click conversion rate, thus capturing and processing customer data
at high speeds.
3.2.4. Value
Value is one of the defining features of big data, which refers to finding the hidden value from
larger datasets. Big data often has a low value density relative to its volume. High value is obtained by
analyzing large datasets [42]. Researchers associated different aspects and terms with this property, as
summarized in Table 6.
The value of big data is the major factor that defines its importance, since a lot of resources and
time is spent to manage and analyze big data, and the organization expects to generate some value
out of it. In the absence of value creation or enhancement, investing in bid data and its associated
techniques is useless and risky. This value has different meanings on the context and the problem.
Raw data are meaningless and are usually of no to a business unless they are processed into some
useful information. For example, for a disaster risk management-related decision-making system,
the value of big data lies in its ability to make precise and insightful decisions. If value is missing,
the system will be considered a failure and will not be adopted or accepted by the organizations or
their customers.
In the context of smart estate, big data can generate neighborhood value. As an example,
Barkham . [51] argued that some African cities facilitated mobility and to jobs through
smart estate big data-generated digital travel information. Such job opportunities enhance the
earning capacities that eventually empowers the dwellers to build better and smarter homes, thus
raising the neighborhood value. Furthermore, such big data generates increased accessibility and
better options, which can help tackle the affordability issues downtown that can help flatten the estate value curve.
3.2.5. Veracity
Veracity is defined as the uncertainty or inaccuracy in the data, which can occur due to
incompleteness or inconsistency [39]. It can also be described as the trustworthiness of the data.
Uncertain and imprecise data represent another feature of big data, which needs to be addressed tools and techniques developed for managing uncertain data [42]. Table 6 summarizes the key aspects
of veracity as explained by different authors.
Uncertainty or vagueness in data makes the data less trusted and unreliable. The of such
uncertain, ambiguous, and unreliable data is a risky endeavor and can have devastating effects on the
business and organizational repute. Therefore, organizations are often cautious of such data and
strive for inducing more certainty and clarity in the data.
In the case of smart estate decision-making, data extracted from tweets, eBay product
descriptions, and Facebook status updates introduces problems associated with misspelled words,
lack of or poor-quality information, of informal language, abundant acronyms, and subjectivity [52].
For example, when a Facebook status or tweet includes words such as “interest”, “rate”, “increase”, and
“home”, it is very hard to infer if the uploader is referring to interest rate increases and home purchases,
or if they are referring to the rate of increased interest in home purchases. Such veracity-oriented
issues in smart estate data require sophisticated software and analytics and are very hard to Big Data Cogn. Comput.2020,4, 419 of 53
address. Similar issues are also faced by disaster managers when vague words such as “disaster”,
“rate”, “flood”, or “GPS” are .
3.2.6. Variability
For the explanation of unstructured data, another characteristic of big data is called variability.
It refers to how the meaning of the same information constantly changes when it is interpreted in a
different way. It also helps in shaping a different outcome by feeds from various sources [13].
Approximately 30 million tweets are quantitatively evaluated daily for sentiment indicator assessments.
Conditioning, integration, and analytics are applied to the data for evaluation under the service of
context brokerage [16]. Table 6 presents various aspects of the variability property of big data.
Variability can be in different ways in smart estate. Lacuesta . [53] introduced a
recommender system on big data generated by heart rate variability in different patients, and
they recommended places that allow the person to live with the highest wellness state. Similarly, Lee
and Byrne [54] investigated the impact of portfolio size on estate funds and argued that big data
with larger variability can be to assess the repayment capabilities of larger organizations. In the
case of disaster management, Papadopoulos . [55] argued that the variability related to changes in
rainfall patterns or temperature can be to plan effectively for hydro-meteorological disasters and
associated risks.
3.2.7. Visualization
For the interpretation of patterns and trends present in the database, visualization of the data
is conducted. Artificial intelligence (AI) has a major role in visualization of data as it can precisely
predict and forecast the movements and intelligently learn the patterns. A huge amount of money
is invested by many companies in the field of AI for the visualization of large quantities of complex
data [41,47]. Table 6 presents the key aspects of big data visualization.
Visualization can help attract more customers and keep the existing ones motivated to the
system more due to the immersive contents and ability to connect to the system. It helps in giving a
boost to the system and, consequently, there is no surprise in organizations investing huge sums in this
aspect of big data. For such immersive visualization in smart estate, Felli . [36] recommended
360 cameras and mobile laser measurements to generate big data, thereby visualizing resources to help
boost property sales. Similarly, Ullah . [18] highlighted the of virtual and augmented realties,
four-dimensional (4D) advertisements, and immersive visualizations to help transform the estate
sector into smart estate. For disaster management, Ready . [56] introduced a virtual reality
visualization of pre-recorded data from 18,000 weather sensors placed across Japan that utilized HTC
Vive and the Unity engine to develop a novel visualization tool that allows users to explore data from
these sensors in both a global and local context.
3.3. Big Data Analytics
Raw data are worthless, and their value is only increased when they are arranged into a sensible
manner to facilitate the extraction of useful information and pertinent results. For the extraction
of useful information from fast-moving and diverse big data, efficient processes are needed by the
organization [42]. As such, big data analytics is concerned with the analysis and extraction of hidden
information from raw data not processed previously. It is also defined as the combination of data
and technology that filters out and correlates the useful data and gains insight from it, which is not
possible with traditional data extraction technologies [57]. Currently, big data analytics is as
the principal method for analyzing raw data because of its potential to capture large amounts of
data [58]. Different aspects of big data analytics such as capture, storage, indexing, mining, and
retrieval of multimedia big data were explored in the multimedia area [59]. Similarly, various sources
of big data in multimedia analytics include social networks, smart phones, surveillance videos, and
others. Researchers and practitioners are considering the incorporation of advanced technologies and Big Data Cogn. Comput.2020,4, 420 of 53
competitive schemes for making efficient decisions the obtained big data. Recently, the of
big data for company decision-making gained much attention, and many organizations are eager to
invest in big data analytics for improving their performance [60]. Gathering varied data and the of
automatic data analytics helps in taking appropriate informed decisions that were previously taken by
the judgement and perception of decision-makers [61]. Three features for the definition of big data
analytics are the information itself, analytics application, and results presentation [58,62]. Big data
analytics is adopted in various sectors of e-government, businesses, and healthcare, which facilitates
them in increasing their value and market share [63]. For enhancing relationships with customers,
many retail companies are extensively big data capabilities. Similarly, big data analytics is for improving the quality of life and moderating the operational cost in the healthcare industry [11,64].
In the field of business and supply chain management, data analytics helps in improving business
monitoring, managing the supply chain, and enhancing the industry automation [58]. Similarly,
Pouyanfar . [59] referred to the event where Microsoft beat humans at the ImageNet Large-Scale
Visual Recognition Competition in 2015 and stressed the need for advanced technology adoption for
the analysis of visual big data. The process of information extraction from big data can be divided into
two processes: data management and analytics. The first process includes the supporting technologies
that are required for the acquisition of data and their retrieval for analysis, while the second process
extracts insight and meaningful information from the bulk of data [42]. Big data analytics includes a
wide range of data which may be structured or unstructured, and several tools and techniques are
present for the pertinent analyses. The broader term of data analytics is divided into sub-classes that
include analytics, audio analytics, video analytics, and social media analytics [59].
3.3.1. Analytics
Techniques that are for the extraction of information from textual data are referred to as analytics. analytics can analyze social network feeds on a specific entity to extract and predict
users’ opinions and emotions to help in smart decision-making. Generally, analytics can be divided
into sentiment analysis, summarization, information extraction, and question answering [59]. Many
big companies Walmart, eBay, and Amazon rely on the of big data analytics for managing
their vast data and enhancing communication with their customers [65]. News, email, blogs, and
survey forms are some of the examples of the textual data obtained from various sources and by
many organizations. Machine learning, statistical analysis, and computational linguistics are in textual analysis of the big data [42]. Named entity recognition (NER) and relation extraction (RE)
are two functions of information extraction which are to recognize named entities within raw
data and classify them in predefined classes such as name, date, and location. Recent solutions for
NER prefer to statistical learning approaches that include maximum entropy Markov models
and conditional random fields [66]. Piskorski . [67] discussed traditional methods of information
extraction along with future trends in this field. Extractive and abstractive approaches for the
summarization of are , in which the former involves the extraction of primary units
from the and joining them together, whereas the latter involves the logical extraction of
information from the [42]. Gambhir . [68] surveyed recent techniques for summarization
and deduced that the optimization- [69] and progressive [70] gave the best
scores for Recall-Oriented Understudy for Gisting Evaluation (ROUGE)-1 and ROUGE-2. For the
analysis of positive or negative sentiments toward any product, service, or event, sentiment analysis
techniques are which fall into three categories of document level, sentence level, and aspect-
techniques [42]. For the extraction of essential concepts from a sentence, Dragoni . a fuzzy
framework which included WordNet, ConceptNet, and SenticNet [71]. Similarly, SparkText, which
is an efficient mining framework for large-scale biomedical data, was developed on the Apache
Spark infrastructure, as well as on the Cassandra NoSQL database that utilizes several well-known
machine-learning techniques [59]. In the case of smart estate management, Xiang . [72] analytics to explore important hospitality issues of hotel guest experience and satisfaction. A large Big Data Cogn. Comput.2020,4, 421 of 53
quantity of consumer reviews extracted from Expedia.com were investigated to deconstruct hotel guest
experience and examine its association with satisfaction ratings, which revealed that the association
between guest experience and satisfaction appears very strong. Similarly, analytics can be to
investigate smart estate investor psychology, as well as information processing and stock market
volatility [73]. Similarly, mining through cyber GIS frameworks such as DisasterMapper can
synthesize multi-source data, spatial data mining [74–76], mining, geological visualization, big
data management, and distributed computing technologies in an integrated environment to support
disaster risk management and analysis [49].
3.3.2. Audio Analytics
The compression and packaging of audio data into a single format is referred to as audio analytics.
It involves the extraction of meaningful information from audio signals. Audio files mainly exist in the
format of uncompressed audio, lossless compressed audio, and lossy compressed audio [77]. Audio
analytics are extensively in the healthcare industry for the treatment of depression, schizophrenia,
and other medical conditions that require patients’ speech patterns [32]. Moreover, it was for
analyzing customer calls and infant cries, revealing information regarding the health status of the
baby [42]. In the case of smart estate, audio analytics can be helpful in property auctioning [78].
Similarly, the of visual feeds digital cameras and associated audio analytics on
conversations between the estate agent and the prospective buyer can help boost estate
sales [79]. In the case of disaster risk management and mitigation, audio analytics can help in event
detection, collaborative answering, surveillance, threat detection, and telemonitoring [77].
3.3.3. Video Analytics
A major concern for big data analytics is video data, as 80% of unstructured data comprise images
and videos. Video information is usually larger in size and contains more information than , which
makes its storage and processing difficult [77]. Server- architecture and edge- architecture
are two main approaches for video analytics, where the latter architecture is relatively higher in
cost but has lower processing power compared to the former architecture [42]. Video analytics can
be in disaster risk management for accident cases and investigations, as well as disaster area
identification and damage estimation [80]. In the case of smart estate, video analytics can be for threat detection, security enhancements, and surveillance [81]. Applications such as the Intelligent
Vision Sensor turn video imagery into actionable information that can be in building automation
and business intelligence applications [82].
3.3.4. Social Media Analytics
Information gathered from social media websites is analyzed and to study the behavior
of people through past experiences. Analytics for social media is classified into two approaches:
content- analytics, which deals with the data posted by the , and structure- analytics,
which includes the synthesis of structural attributes [42]. Social media analytics is an interdisciplinary
research field that helps in the development of a decision-making framework for solving the performance
measurement issues of the social media. analysis, social network analysis, and trend analysis have
major applications in social media analytics. classification support vector machine (SVM) is for mining. For the study of relationships between people or organizations, social network
analysis is which helps in the identification of influential users. Another analysis method famous
in social media analytics is trend analysis, which is for the prediction of emerging topics [83].
The of mobile phone apps and other multimedia- applications is an advantage provided by
big data. In the case of smart estate management, big data was to formulate and introduce
novel recommender systems that can recommend and shortlist places for users interested in exploring
cultural heritage sites and museums, as well as general tourism, machine learning and artificial
intelligence [84]. The recommender system keeps a track of the users’ social media browsing including Big Data Cogn. Comput.2020,4, 422 of 53
Facebook, Twitter, and Flickr, and it matches the cultural objects with the users’ interest. Similarly,
multimedia big data extracted from social media can enhance both -time detection and alert
diffusion in a well-defined geographic area. The application of a big data system on incremental
clustering event detection coupled with content- and bio-inspired analyses can support spreading
alerts over social media in the case of disasters, as highlighted by Amato . [85].
3.4. Data Analytics Process
With the large growth in the amount of data every day, it is becoming difficult to manage these
data with traditional methods of management and analysis. Big data analytics receives much attention
due to its ability to handle voluminous data and the availability of tools for storage and analysis
purposes. Elgendy . [43] described data storage, processing, and analysis as three main areas for
data analytics. In addition, data collection, data filtering and cleaning, and data visualizations are
other processes of big data analytics. Further data ingestion is an important aspect of data analysis;
however, the current study focuses on the analytic processes only.
3.4.1. Data Collection
The first step for the analysis of big data is data acquisition and collection. Data can be acquired
through different tools and techniques from the web, Excel, and other databases as shown in Table 7.
The table lists a set of tools for gathering data, the type of analysis task they can perform, and the
corresponding application or framework where they can be deployed. Sentiment analysis from data
refers to finding the underlying emotion or tone. The tools developed to perform sentiment analysis
can automatically detect the overall sentiment behind given data, e.g., negative, positive, or neutral.
Content analysis tools analyze the given unstructured data with the aim of finding its meaning and
patterns and to transform the data into some useful information. Semantria is a sentiment analysis tool,
which is deployable over the web on cloud. Its plugin can be installed in Excel and it is also available
as a standalone application programming interface (API). Opinion crawl is another tool to extract
opinions or sentiments from data but can only be deployed over the web. Open is a content
analysis tool which can be within software called Captiva. This is an intelligent capture system,
which collects data from various sources electronic files and papers and transforms the data into a
digital form, making them available for various business applications. Trackur is another standalone
sentiment analysis application. It is a monitoring tool that monitors social media data and collects
reviews about various brands to facilitate the decision-makers and professionals of these companies in
making important decisions about their products.
Table 7.Comparison of data collection tools.
ToolsDeploy AbilityAnalysisLimitation
SemantriaWeb, API, ExcelSentimentCrashes on large datasets [86]
Opinion
crawl
WebSentiment
Cannot be for advanced SEO audits
[87]
Open textCaptivaContent
Requires lot of technical configuration for
document sharing on servers [88]
TrackurTrackurSentimentRecurring cost of subscription [89]
3.4.2. Data Storage
For the accommodation of collected structured and unstructured data, databases and data
warehouses are needed, for which NoSQL databases are predominantly . There are other
databases as well; however, the current study only focuses on NoSQL databases. Features and
applications of some NOSQL databases, as well as their categories, features, and applications, are
discussed in Table 8. A further four categories as defined by Martinez-Mosquera . [37] are to Big Data Cogn. Comput.2020,4, 423 of 53
classify the databases which are column-oriented, document-oriented, graph, and key value. Apache
Cassandra is a NoSQL database management system, which can handle big data over several parallel
servers. This is a highly fault-tolerant system as it has no single point of failure (SPOF), which means
that it does not reach any such state where entire system failure occurs. It also provides the feature of
tunable consistency, which means that the client application decides how up to date or consistent a row
of data must be. MangoDB is another distributed database available over the cloud which provides
the feature of load balancing; this improves the performance by sending multiple concurrent requests
of clients to multiple database servers, to avoid overloading a single server.
Table 8.Comparison of NoSQL data storage tools.
NoSQL DatabasesNoSQL CategoryFeaturesApplicationsLimitation
Apache Cassandra
Column-oriented
Fault-tolerant; scalable;
decentralized; tunable
consistency
Facebook for inbox
search; online trading
It will not work for ACID properties
(atomicity, consistency, isolation, and
durability) [80]
HBase
Elastic; consistent;
fault-tolerant
Facebook messages
Does not support SQl Structure and
there is no query optimizer [90]
MongoDB
Document-oriented
Horizontally scalable
fast; load balancing
Asset tracking system;
textbook management
system
Memory restrictions for both Linux-
and Windows- environments
[91]
CouchDB
Seamless flow of data;
ease of ;
developer-friendly
International Business
Machines (IBM)
Slower than in-memory DBMS; slow
response in viewing large datasets in
creating replica of databases, where it
fails [92]
Terrastore
Elastic; scalable;
extensible; simple
configuration
Event processing
Only document-oriented database;
not mature enough yet [93]
Hive
Graph for structured
dataset; ad hoc report
generation
Network traffic
classification; Facebook
Update/delete operations are not
supported in hive; materialized view
is not available [94]
Neo4j
Fast read and write;
horizontally and
vertically scalable
Time-varying social
network data
Searching for ranges is not possible in
Neo4j [95]
AeroSpike
Value
Powering -time,
extreme-scale data
solutions
Ecommerce and retail,
Adobe Solutions
Geospatial precision is not accurate;
incremental backup and restore
operations are still not available [96]
Voldemort
Distributed key-value
storage system
LinkedIn
Does not satisfy arbitrary relations
while satisfying ACID properties
(atomicity, consistency, isolation, and
durability); it is not an object database
that maps object reference graphs
transparently [97]
CouchDB is a clustered database which means that it enables the execution of one logical database
server on multiple servers or virtual machines (VMs). This set-up improves the capacity and availability
of the database without modifying the APIs. Terratore is a database for storing documents, which is
accessible through the HTTP protocol. It supports both single-cluster and multi-cluster deployments
and offers advanced data scaling features. The documents are stored by partitioning and then
distributing them across various nodes. Hive is a data warehouse which is built on top of the Hadoop
framework and offers data query features by providing an interface such as the SQL for different files
and data stored within the Hadoop database [98]. Hbase is a distributed and scalable database for big
data which allows random and -time to the data for both reading and writing. Neo4j is a
graph database which enables the to perform graphical modeling of big data. It allows developers
to handle data by a graph query language called Cypher which enables them to perform create,
read, update, and delete (CRUD) operations on data. Big Data Cogn. Comput.2020,4, 424 of 53
3.4.3. Data Filtering
In order to extract structured data from unstructured data, the data are filtered through some
tools which filter out the useful information necessary for the analyses. Some data filtering tools and
their features are compared in Table 9.
Table 9.Comparison of data filtering tools.
ToolsInput DataSoftwareFeaturesOutput Data Form
Import.io [99]
CSV or Excel
(XLSX) file
Web-
Allows scheduling of
data; supports
combination of days,
time weeks; web
scraping
Structured data;
data reports
Parsehub [99]Excel (XLSX) fileCloud-
Search through
pop-ups, tabs and
forms; graphics app
interface
Comma-separated
values (CSV);
Google sheets
Mozenda [16]Input listWeb-
Automatic list
identification; web
scraping [36]
JavaScript object
notation (JSON);
CSV
Content Grabber
[16] or dropdown
field
Web-
Point and click
interface; scalable;
error handling [36]
Extensible markup
language (XML);
CSV
Octoparse [99]Keywords/textCloud-
Web scraping without
coding; -friendly;
scheduled extraction
CSV; API
Import.io is a web data integration tool which transforms unstructured data into a structured
format so that they can be integrated into various business applications. After specifying the target
website URL, the web data extraction module provides a visual environment for designing automated
workflows for harvesting data, going beyond HTML parsing of static content to automate end-
interactions yielding data that would otherwise not be immediately visible. ParseHub is a free, easy to
, and powerful web scraping tool which allows users to get data from multiple pages, as well as
interact with AJAX, forms, dropdowns, etc. Mozenda is a web scraping tool which allows a to
scrape , files, images, and PDF content from web pages with a point-and-click feature. It organizes
data files for publishing and exporting them directly to TSV, comma-separated values (CSV), extensible
markup language (XML), Excel (XLSX), or JavaScript object notation (JSON) through an API. Content
Grabber is a cloud- web scraping tool that helps businesses of all sizes with data extraction.
Primary features of Content Grabber include agent logging, notifications, a customizable interface,
scripting capabilities, scripting, agent debugger, error handling, and data export. Octoparse is a
cloud- data scraping tool which turns web pages into structured spreadsheets within clicks
without coding. Scraped data can be downloaded in CSV, Excel, or API format or saved to databases.
3.4.4. Data Cleaning
Collected data contain a lot of errors and imperfections that affect the results leading to wrong
analysis. Errors and imperfections of the data are removed through data cleaning tools. Some
data cleaning tools are listed in Table 10. DataCleaner is a data quality analysis application and
solution platform for DQ solutions. At its core lies a strong data profiling engine which is extensible,
thereby adding data cleansing, transformations, enrichment, deduplication, matching, and merging.
MapReduce is a programming model and an associated implementation for processing and generating
big datasets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a
map procedure, which performs filtering and sorting, such as sorting students by first name into queues, Big Data Cogn. Comput.2020,4, 425 of 53
with one queue for each name, and a reduce method, which performs a summary operation such as
counting the of students in each queue, yielding name frequencies. OpenRefine (previously
Google Refine) is a powerful tool for working with messy data that cleans the data, transforms the
data from one format into another, and extends the data with web services and external data. It works
by running a small server on the host computer, and the internet browser can be to interact with
it. Reifier helps improve business decisions through better data. By matching and grouping nearly
similar records together, a business can identify the right customers for cross-selling and upselling,
improve market segmentation, automate lead identification, adhere to compliance and regulation,
and prevent fraud. Trifacta accelerates data cleaning and preparation with a modern platform for
cloud data lakes and warehouses. This ensures the success of your analytics, ML, and data onboarding
initiatives across any cloud, hybrid, or multi-cloud environment.
Table 10.Comparison of data cleaning tools.
ToolsFeaturesSourceTechnologies
Data Cleaner
Missing values search; duplicate
detection
Hadoop database [16]
Fishers discrimination
criterion (FDC)
Map ReduceSorting; clusteringHadoop database [16]Functional programming
Open RefineTransformation; faster paceWeb services [16,100]Java
ReifierFast deployment; high accuracyVarious databases [100]All relational
Trifecta Wrangler
Transformation; fewer formatting
times; suggests common
aggregations
Web services [100]NoSQL databases
3.4.5. Data Analysis and Visualization
For the extraction of meaningful information from raw data, visualization techniques are applied.
Several tools and techniques are for information visualization, depending on the type of
data and the intended visual outcome associated with the dataset. Most of the tools perform the
extraction, analysis, and visualization in integrated fashion data mining and artificial intelligence
techniques [16]. Advantages and disadvantages of some data visualization tools are discussed in
Table 11. Tableau products query relational databases, online analytical processing cubes, cloud
databases, and spreadsheets to generate graph-type data visualizations. The products can also extract,
store, and retrieve data from an in-memory data engine. Power BI is a business analytics service by
Microsoft that aims to provide interactive visualizations and business intelligence capabilities with
an interface simple enough for end users to create their own reports and dashboards. Plotly’s team
maintains the fastest growing open-source visualization libraries for R, Python, and JavaScript. These
libraries seamlessly interface with our enterprise-ready deployment servers for easy collaboration,
code-free editing, and deploying of production-ready dashboards and apps. Gephi is the leading
visualization and exploration software for all kinds of graphs and networks. It is an open-source and
free data visualization tool which runs on Windows, Mac OS X, and Linux. Similarly, Microsoft Excel
can perform calculations, graphing, pivot tables, and a macro programming language called Visual
Basic for applications. In the smart estate context, 360 cameras, VR- and AR- immersive
visualizations, 4D advertisements, etc. can help boost property sales by keeping the customers
more immersed and involved in the property inspections [36]. In addition, novel features such as
virtual furnishing and VR-powered abilities to move the furniture and items around virtually are the
applications of data visualizations in smart estate [18,20,101]. Big Data Cogn. Comput.2020,4, 426 of 53
Table 11.Comparison of data visualization tools.
ToolsFeaturesLimitationsAvailability
Tableau
Fast and flexible; wide variety of
charts; mapping longitude and
latitude
License for server and desktop is
needed; coding skills are
required [102]
Open source
Microsoft Power BI
Flexible and persuasive;
code-free data visualization
Work account is necessary for
sign-in; 250 MB is the limited
size of Workbook [103]
Open
source/cloud-
service
Plotly
Web Plot Digitizer (WPD) is a
tool of plotly which
automatically extracts data from
static images
Upload size of file must be up to
50 MB; no offline client is
available [104]
Open source
Gephi
Handles large complex datasets;
no programming skills
Only works for graph
visualization [105]
Open source
Excel
Capable of managing
semi-structured data; powerful
visualization tool
Available only with Office 365
subscription; not free [106]
Open source
3.5. Frameworks for Data Analysis
There are two main frameworks that are utilized for data analytics. These include the Hadoop
Framework and Apache Spark.
3.5.1. Hadoop Framework
For the analysis of big data, Hadoop is a popular open-source software that is by many
organizations. The Hadoop framework is governed by Google architecture that processes large datasets
in distributed environments [39]. It consists of two stages: storage and analysis. The task of storage is
carried out by its own Hadoop Distributed File System (HDFS) that can store TB or PB of data with
high streaming [107]. The complete architecture of the HDFS is presented on the webpage
of DataFlair [108]. Similarly, for the analysis of obtained data, MapReduce is by the Hadoop
framework that allows writing programs in order to transform large datasets into more management
datasets. MapReduce routines can be customized for the analysis and exploration of unstructured
data across thousands of nodes [107]. MapReduce splits the data into manageable chunks and then
maps these splits accordingly. The of splits is reduced accordingly and stored on a distributed
cache for subsequent utilizations. Additionally, the data are stored in a master-salve pattern. The
NameNode manages the DataNodes and stores the metadata in the cluster. All the changes to the
file system, size, location, and hierarchy are recorded by it. Any deleted files and blocks in the HDFS
are recorded in the Edit Log and stored in the nodes. The actual data are stored in the DataNode and
respond to the request of the clients. DataNode creates, deletes, and replicates the blocks on the
decisions of NameNode. The activities are processed and scheduled with the help of YARN, which is
controlled by Resource Manager and Node Manager. Resource Manager is a cluster-level component
and runs on the master machine, while NodeManager is a node-level component which monitors
resource consumption and tracks log management.
3.5.2. Apache Spark
Apache Spark is another data processing engine that has a performing model similar to MapReduce
with an added ability of data-sharing abstraction. Previously, processing of wide-range workloads
needed separate engines SQL, machine learning, and streaming, but Apache Spark solved this issue
with the Resilient Distributed Datasets (RDD) extension. RDD provides data sharing and automatic
recovery from failures by lineage which saves time and storage space. For details of Apache
Spark, the work of Zaharia . [109] is useful. Big Data Cogn. Comput.2020,4, 427 of 53
3.5.3. Hadoop Framework vs. Apache Spark
Both data analysis engines perform the task of analyzing raw data efficiently, but there exist
some differences in their performance. The PageRank algorithm and logistic regression algorithm for
machine learning were to compare the performance of both analysis tools. The performance
of Hadoop and Apache Spark the PageRank algorithm and logistic regression algorithm is
illustrated in 11a,b, respectively. Spark Core is a key component of Apache Spark and is the
base engine for processing large-scale data. It facilitates building additional libraries which can be for streaming and different scripts. It performs multiple functions such as memory management,
fault recovery, networking with storage systems, and scheduling and monitoring tasks. In Apache
Spark, -time streaming of data is processed with the help of Spark Streaming, which gives high
throughput without any obstacles. A module of ApacheSpark is Spark SQL, which integrates
relational processing with functional programming and extends the limits of traditional relational data
processing. It also facilitates querying data. GraphX provides parallel computation and API for graphs.
It extends the Spark RDD abstraction with the help of the Resilient Distributed Property Graph, giving
details on the vertex and edge of the graph. Furthermore, the MLiB facilitates performing
machine learning processes in Apache Spark.
Big Data Cogn. Comput. 2020, 4, 4 27 of 53 3.5.3. Hadoop Framework vs. Apache Spark Both data analysis engines perform the task of analyzing raw data efficiently, but there exist some differences in their performance. The PageRank algorithm and logistic regression algorithm for machine learning were to compare the performance of both analysis tools. The performance of Hadoop and Apache Spark the PageRank algorithm and logistic regression algorithm is illustrated in 11a and 11b, respectively. Spark Core is a key component of Apache Spark and is the base engine for processing large-scale data. It facilitates building additional libraries which can be for streaming and different scripts. It performs multiple functions such as memory management, fault recovery, networking with storage systems, and scheduling and monitoring tasks. In Apache Spark, -time streaming of data is processed with the help of Spark Streaming, which gives high throughput without any obstacles. A module of ApacheSpark is Spark SQL, which integrates relational processing with functional programming and extends the limits of traditional relational data processing. It also facilitates querying data. GraphX provides parallel computation and API for graphs. It extends the Spark RDD abstraction with the help of the Resilient Distributed Property Graph, giving details on the vertex and edge of the graph. Furthermore, the MLiB facilitates performing machine learning processes in Apache Spark. (a) (b) 11. Comparison of performance for (a) PageRank and (b) logistic regression algorithm. Statistics depict from the algorithm that the of iterations in the Hadoop framework is greater than that in Apache Spark. Similarly, most machine learning algorithms work iteratively. MapReduce uses coarse-grained tasks which are heavier for iterative algorithms, whereas Spark Mesos, which runs multiple iterations on the dataset and yields better results [110]. A comparison of some important parameters for both frameworks is shown in Table 12. Overall, Hadoop and Apache Spark do not need to compete with each other; rather, they complement each other. Hadoop is the best economical solution for batch processing and Apache Spark supports data streaming with distributed processing. A combination of the high processing speed and multiple integration support of Apache Spark with the low cost of Hadoop provides even better results [110]. Table 12. Parameter comparison of Hadoop and Apache Spark. RDD—Resilient Distributed Datasets. Parameters Hadoop Framework Apache Spark Language Java Scala Memory 24 GB 8 GB to hundreds of GBs Network 1 GB Ethernet all-to-all 10 GB or more Security Authentication via LDAP (Lightweight Directory Protocol) Via shared secret Fault Tolerance Data replication RDD 11.Comparison of performance for (a) PageRank and (b) logistic regression algorithm.
Statistics depict from the algorithm that the of iterations in the Hadoop framework is
greater than that in Apache Spark. Similarly, most machine learning algorithms work iteratively.
MapReduce uses coarse-grained tasks which are heavier for iterative algorithms, whereas Spark Mesos, which runs multiple iterations on the dataset and yields better results [110]. A comparison of
some important parameters for both frameworks is shown in Table 12. Overall, Hadoop and Apache
Spark do not need to compete with each other; rather, they complement each other. Hadoop is the best
economical solution for batch processing and Apache Spark supports data streaming with distributed
processing. A combination of the high processing speed and multiple integration support of Apache
Spark with the low cost of Hadoop provides even better results [110]. Big Data Cogn. Comput.2020,4, 428 of 53
Table 12.Parameter comparison of Hadoop and Apache Spark. RDD—Resilient Distributed Datasets.
ParametersHadoop FrameworkApache Spark
LanguageJavaScala
Memory24 GB8 GB to hundreds of GBs
Network1 GB Ethernet all-to-all10 GB or more
Security
Authentication via LDAP (Lightweight Directory Protocol)
Via shared secret
Fault ToleranceData replicationRDD
SpeedFast100×faster than Hadoop
ProcessingBatch processingReal-time processing
3.6. Machine Learning in Data Analytics
Machine learning is a domain of artificial intelligence (AI) for extracting knowledge from
voluminous data in order to make or reach intelligent decisions. It follows a generic algorithm for
building logic on the given data without the need for programming. Basically, machine learning is a
data analytics technique that uses computational methods for teaching computers to learn information
from the data [3]. Many researchers explored the field of machine learning in data analytics such as
Ruiz . [17], who discussed the of machine learning for analysis of massive data. -Jarrah . [111] presented a review of theoretical and experimental literature of data modeling. Dorepalli . [112] reviewed the types of data, learning methods, processing issues, and applications of machine
learning. Moreover, machine learning is also in statistics, engineering, and mathematics to resolve
various issues of recognition systems and data mining [113]. Typically, machine learning has three
sub-domains that are supervised learning, unsupervised learning, and reinforcement learning, as
discussed in Table 13.
Table 13.Sub-domains of machine learning.
Learning TypesLearning AlgorithmsProcessing TasksApplications
Supervised learning
Support vector machine
(SVM); naïve Bayes;
hidden Markov model
Classification
Speech recognition;
medical imaging
RegressionAlgorithmic trading
Unsupervised learning
K-means; Gaussian
mixture model
Clustering
Gene sequence analysis;
market research
Prediction
Reinforcement learningQ-learning; R-learningDecision-making
Stock market price
prediction
All machine learning techniques are efficient in processing data; however, as the size of the data
grows, the extraction and organization of discriminative information from the data pose a challenge
to the traditional methods of machine learning. Thus, to cope up with the growing demand of data
processing, advanced methods for machine learning are being developed that are intelligent and
much efficient for solving big data problems [113]. As such, one developed method is representation
learning [114], which eases the task of information extraction by capturing a greater of
input configurations from a reasonably small data size. Furthermore, deep belief networks (DBNs)
and convolution neural networks (CNNs) are extensively for speech and hand-written digit
recognition [115]. Deep learning methods with higher processing power and advanced graphic
processors are on large databases [113]. Traditional methods of machine learning possess
centralized processing, which is addressed with the of distributed learning that distributes the
data among various workstations, making the process of data analysis much faster. Classical methods
of machine learning mostly the same feature space for training and testing of the dataset, which Big Data Cogn. Comput.2020,4, 429 of 53
creates a problem for the older techniques to tackle heterogeneity in the dataset. In set-ups, transfer
learning intelligently applies the previously gained knowledge to the problem and provides
faster solutions. In most applications, there may exist abundant data with missing labels. Obtaining
labels from the data is expensive and time-consuming, which is solved active learning [112].
This creates a subset of instances from the available data to form labels which give high accuracy and
reduce the cost of obtaining labeled data. Similarly, kernel- learning proved to be a powerful
technique that increases the computational capability of non-linear learning algorithms. An excellent
feature of this learning technique is that it can map the sample implicitly only a kernel ,
which helps in the direct calculation of inner products. It provides intelligent mathematical in the formation of powerful nonlinear variants of statistical linear techniques. Although many of
the achievements made in machine learning facilitated the analysis of big data, there still exist some
challenges. Learning from data that has high speed, volume, and different types is a challenge for
machine learning techniques [113]. Some of the challenges for machine learning are discussed in
Table 14 along with possible remedies.
Table 14.Issues and possible solutions of machine learning for big data.
IssuesPossible Solutions
Volume
Parallel computing [116]
Cloud computing [40]
Variety
Data integration; deep learning methods;
dimensionality reduction [117]
Velocity
Extreme learning machine (ELM) [118]
Online learning [119]
Value
Knowledge discovery in databases (KDD); data
mining technologies [120]
Uncertainty and incompletenessMatrix completion [121]
AI and machine learning methods are being increasingly integrated in systems dealing with a
wide variety of issues related to disasters. This includes disaster prediction, risk assessment, detection,
susceptibility mapping, and disaster response activities such as damage assessment after the occurrence
of a disaster. In Nepal, in April 2015, an earthquake of 7.8 magnitude hit 21 miles offthe southeast coast
of Lamjung. The standby task force was successful in mobilizing 3000 volunteers across the country
within 12 hours after the quake, which was possible due to the revolutionized AI system in Nepal.
Volunteers in that area started tweeting and uploading crisis-related photographs on social media.
Artificial Intelligence for Disaster Response (AIDR) those tagged tweets to identify the needs
of people on categories such as urgent need, damage to infrastructure, or even help regarding
resource deployment. Similarly, Qatar developed a tool known as the Qatar Computing Research
Institute (QCRI) for disaster management. The tool was developed by the Qatar Foundation to increase
awareness and to develop education and science in a community. For disaster risk management, QCRI
aims to provide its services by increasing the efficiency of agencies and volunteer facilities. The tool
has an AI system installed which helps in recognizing tweets and texts regarding any devastated area
or crisis. The QCRI then provides an immediate solution to overcome the crisis [122]. OneConcern
is a tool developed to analyze disaster situations. The tool creates a comprehensive picture of the
location during an emergency operation. This image is by emergency centers to investigate the
situation and provide an immediate response in the form of relief goods or other rescue efforts. The
tool also helps in the creation of a planning module that can be useful in identifying and determining
the areas prone to a disaster. The vulnerable areas can then be evacuated to avoid loss of life. Until
now, OneConcern identified 163,696 square miles area and arranged shelter for 39 million people. It Big Data Cogn. Comput.2020,4, 430 of 53
also examined 11 million structures and found 14,967 faults in their construction, thereby providing
precautionary measures before a natural disaster hit.
3.7. Big Data Challenges and Possible Solutions
Massive data with heterogeneity pose many computational and statistical challenges [123]. Basic
issues such as security and privacy, storage, heterogeneity, and incompleteness, as well as advanced
issues such as fault tolerance, are some challenges posed by big data.
3.7.1. Security and Privacy
With the enormous rate of data generation, it becomes challenging to store and manage the data traditional methods of data management. This gives rise to an important issue which is the
privacy and security of the personal information. Many organizations and firms collect personal
information of their clients without their knowledge in order to increase value to their businesses,
which can have serious consequences for the customers and organizations if accessed by hackers and
irrelevant people [124]. Verification and trustworthiness of data sources and identification of malicious
data from big databases are challenges. Any unauthorized person may steal data packets that are sent
to the clients or may write on a data block of the file. To deal with this, there are solutions such as the of authentication methods, Kerberos, and encrypted files. Similarly, logging of attack detection
or unusual behavior and secure communication through a Secure Sockets Layer (SSL) and Transport
Layer Security (TLS) are potential solutions [125].
3.7.2. Heterogeneity and Incompleteness
Within big databases, data are gathered from different sources that vary greatly, leading to
heterogeneity in the data [39]. Unstructured, semi-structured, and structured data differ in their
properties and associated information extraction techniques. Transformation from unstructured data to
structured data is a crucial challenge for data mining. Moreover, due to malfunctioning of any sensor
or fault in systems, the issue of incomplete data poses another challenge [125]. Potential solutions to
this issue include data imputation for missing values, building learning models, and filling the data
with the most frequent values.
3.7.3. Fault Tolerance
Failure or damage may occur during the analysis of big data, which may require restarting the
cumbersome process from scratch. Fault tolerance sets the range for any failure in order to recover data
without wasting time and cost. Maintaining a high fault tolerance for heterogeneous complex data is
extremely difficult, and it is impossible to achieve 100% reliable tolerance. To tackle this issue, potential
solutions include dividing the whole computation into sub-tasks and the application of checkpoints
for recursive tasks [124].
3.7.4. Storage
Earlier, data were stored on hard disk drives (HDDs) which were slower in I/O performance. As
data grew bigger and bigger, most technologies switched to cloud computing, which generates data
at a high speed, the storage of which is a problem for analytics tools [39]. To tackle this, the of
solid-state drives (SDDs) and phase change memory (PCM) are potential solutions [126].
4. Applications of Big Data and Pertinent Discussions
The growth of data increased enormously during last two decades, which encouraged global
researchers to explore machine learning algorithms and artificial intelligence to cope with the
big data. Various applications of big data are found in medicine, astrology, banking, and finance
departments for managing their big databases [10,127]. In the healthcare industry, huge amounts of Big Data Cogn. Comput.2020,4, 431 of 53
data are created for record keeping and patient care, which are in improving healthcare facilities by
providing population management and disease surveillance at reduced cost [128]. Similarly, machine
learning models for early disease diagnosis and prediction of disease outbreak and genomic medicine
are now being popularly [129]. As an example, Chen . [130] experimented on a hospital to
study the outbreak of cerebral infarction a CNN- machine learning model which achieved
a prediction accuracy of 94.8%. Now, big data also incorporates psychiatric research that gathers data
for the person’s anxiety attacks and irregular sleep patterns to diagnose any psychological illness [131].
Similarly, GPS-enabled trackers were developed for asthma patients by Asthmapolis that record inhaler
usage by the patients. These recorded data are gathered in a central database to analyze the needs
of individual patients [132]. In the field of agriculture, smart farming and precision agriculture are major
technological advancements that incorporate cloud computing and machine learning algorithms [133].
In this context, Singh . proposed a model for forecasting moisture in soil by time series
analysis [134]. Data generated from various sources wind direction predictors, GPS-enabled
tractors, and crop sensors are to elevate agricultural operations. Primarily Europe and North
America big data applications for agriculture, but most countries are still deprived of them [135].
Similarly, other industries such as the aviation industry are growing rapidly and producing large
amounts of data from weather sensors, aircraft sensors, and air. The application of big data analytics
for aviation is necessary as latest aircrafts the Boeing 787 obtains 1000 or more flight parameters,
whereas older aircrafts Legacy captured only 125+parameters [136]. Similarly, social media
platforms Facebook, Instagram, and Twitter generate data, its analysis is necessary to understand
and gather public opinion or feedback about any product or service [18,137], which can be analyzed machine learning applications of big data. Machine learning algorithms are to analyze
the behavior of the via -time analysis of the content browsed by them, and relevant online
advertisements are recommended accordingly. Moreover, the detection of spam data mining
techniques also employs the of machine learning [138]. In addition, Hadoop and machine learning
algorithms are by banks for analysis of loan data to check the reliability of lending organizations,
thereby increasing profitability and innovation [139]. Recent studies in the field of construction, city,
and property management specially reported that compatibility, interoperability, value, and reliability
are critical factors of digital technology adoption and implementation [140–144]. The network intrusion
traffic challenge was resolved efficiently by Suthaharan . [145] machine learning and
big data technologies. Distributed manufacturing industries big data approaches to find opportunities [146]. Similarly, electrical power industries implement big data approaches for electricity
demand forecasting [147]. Processes of decision-making, value creation [148], innovation, and supply
chain [149] were significantly enhanced big data analytics techniques. Zhou . investigated
a trajectory detection method to improve taxi services big data from GPS [150]. Applications
of big data are also found in creating competitive advantages by troubleshooting, personalization,
and detection of areas that require improvement [151]. For predictive modeling, high-cardinality
features are not very often because of their randomness. To address this, Moeyersoms . [152]
introduced transformation functions in a churn predictive model that included high-cardinal features.
4.1. Big Data Applications for Smart Estate and Property Management
Big data recently made its way into the estate and property management industry and
was in various forms such as visualization of properties and 360 videos [36], virtual and
augmented realities [153], stakeholder management [20], online customer management [101,154], and
the latest disruptive Big9 technologies including artificial intelligence, robotics, and scanners that are
transforming it from traditional to smart estate [18]. This was also applied to domains of smart
cities, especially in the fields of informatics and information handling [155]. Among the practical
aspects and money-making perspectives, the newly introduced idea of bitcoin houses is an amazing
application of big data in the smart estate industry [156]. Believed to be the first income-generating
house, the idea of a bitcoin house revolves around big data that has more than 40 containers of data Big Data Cogn. Comput.2020,4, 432 of 53
miners installed at the house, which can generate 100% off-grid electricity and earnings of over $1M per
month, with the potential to be the first self-paying home mortgage house in the world. Similarly, Kok . [157] suggested an automated valuation model to produce the value of properties instantly.
In their study, a model was developed with an absolute error of 9%, which compares favorably with
the accuracy of traditional appraisals, and which can produce an instant value at every moment in
time at a very low cost to automate the estate industry and move toward a smart estate and
property industry big data. The model bases its roots in the concepts of machine learning and
artificial intelligence to analyze the big data. Among the companies utilizing big data in estate, Du . [48] highlighted estate and property companies in China such as Xinfeng, CICC, Haowu, and
others who successfully started utilizing big data for addressing stakeholder needs such as property
information, buyer demand, transaction data, page view, buyer personal information, and historical
transaction information. Likewise, Barkham . [51] stated the cities and their smart estate
initiatives powered by big data including The Health and Human Services Connect center in York for improved efficiency of public services, Data Science for Social Good in Chicago, Transport for
London, IBM operations center for city safety in Brazil, and others. Table 15 lists the key stakeholders
of estate in accordance with Ullah . [18] as the customers that include buyers and users of
the estate services, the sellers including owners and agents, and the government and assessment
agencies. The table further lists the names, the focus of different organizations, the required resources,
and examples of how big data is utilized by these organizations in the world for addressing the needs
of smart estate stakeholders.
Table 15.Organizations big data in smart estate, focusing on stakeholders and
required resources.
StakeholderFocusResources RequiredImplementing Organization with Examples/Uses
Customer
[48,51,158]
Personalization
Customer data surveys,
feedback analyses
Airbnb London: Creating collective intelligence
databases from customers reviews and feedbacks
Cross-matching
Data warehouse, buyer
click patterns
Haowu China: A bigdata warehouse was established
where the buyer demand is matched with the house
available;
BuildZoom USA: Matches commercial or residential
owner projects with appropriate contractors in
proximity who specialize in the job at hand and have
high ratings
Property information
Predictive analytics tools, to government
information
Data Science for Social Good Chicago USA: Leads
contamination identification in houses before they
occur predictive analytics
Buyer demand
Buyer survey, social
media analytics
Xinfeng China: Five bigdata application systems are
created to recommend certain houses and evaluate
the housing price
Owners, sellers, and
agents [51,159,160]
Building performance
databases
Building maintenance
data, occupant data
ArchiBus USA: Benchmarking, preventive
maintenance, predictive maintenance, and
anticipation of budgetary needs
Property value analysis
Government reports,
local contracts, property
insights
CoreLogic Australia: Prepares reports, generates
value estimates, verifies information, and conducts
highly targeted marketing
Resident, strata, and
enterprise management
Analytics tools
Accenture Ireland: Provides consultancy and system
integration services to enterprises and building rapid
learning models
Online transaction
Customer surveys,
demand analysis
Truss USA: Marketplace to help small- and
medium-sized business owners find, tour, and lease
space that uses three-dimensional (3D) virtual tours
Potential clients/business
Property insights,
government databases
SmartList Australia: Combines property, market,
and consumer data to identify properties that are
more likely to be listed and sold; helps agents get
more opportunities from fewer conversations Big Data Cogn. Comput.2020,4, 433 of 53
Table 15.Cont.
StakeholderFocusResources RequiredImplementing Organization with Examples/Uses
Government and
regulatory
authorities [51,161]
Fraud detection
Drones, processing
systems
Tax Agency Spain: Data analyzed drones from
4000 municipalities and discovered 1.69 million
properties paying insufficient taxes on constructions, expansions, and pools
Privacy and securityGovernment data
MyNanjing App China: Connects citizens, public
administrative departments, state-owned enterprises
providing public services, and private companies
across Nanjing, China with security ensured by the
government
Public services
Central database
linkages
Health and Human Services Connect Initiative York: This service allows clients to walk into
different agencies without the need for duplicating
the paperwork
Big data can be generated by software and tools owned by agencies and the sellers of properties,
which gives personalized suggestions and recommendations to the prospective buyers or users of
the service to make better and informed decisions. However, it is important to have a centralized
independent validation system in check that can be operated by the government or assessment agencies
to protect the privacy of the users, along with verification of the data and information provided to the
prospective buyers. In this way, trust can be generated between the key estate stakeholders, i.e.,
the sellers and buyers, which can reduce, if not eliminate, the regrets related to ill-informed decisions
made by the buyers or users. A conceptual model is presented in 12 for this purpose. As
highlighted by Joseph and Varghese [158], there is a risk of big data brokers misleading the consumers
and exploiting their interests; therefore, regulators and legislators should begin to develop consumer
protection strategies against the strong growth for big data brokers. The model in 12 supports
this argument and presents an intermediary organization for keeping an eye on the misuse of data and
manipulations by big data agents and brokers.
Big Data Cogn. Comput. 2020, 4, 4 33 of 53 Privacy and security Government data MyNanjing App China: Connects citizens, public administrative departments, state-
owned enterprises providing public services, and private companies across Nanjing, China with security ensured by the government Public services Central database linkages Health and Human Services Connect Initiative York: This service allows clients to walk into different agencies without the need for duplicating the paperwork Big data can be generated by software and tools owned by agencies and the sellers of properties, which gives personalized suggestions and recommendations to the prospective buyers or users of the service to make better and informed decisions. However, it is important to have a centralized independent validation system in check that can be operated by the government or assessment agencies to protect the privacy of the users, along with verification of the data and information provided to the prospective buyers. In this way, trust can be generated between the key estate stakeholders, i.e., the sellers and buyers, which can reduce, if not eliminate, the regrets related to ill-
informed decisions made by the buyers or users. A conceptual model is presented in 12 for this purpose. As highlighted by Joseph and Varghese [158], there is a risk of big data brokers misleading the consumers and exploiting their interests; therefore, regulators and legislators should begin to develop consumer protection strategies against the strong growth for big data brokers. The model in 12 supports this argument and presents an intermediary organization for keeping an eye on the misuse of data and manipulations by big data agents and brokers. 12. Conceptual model for big data utilization in estate transactions with decentralized validation to ensure data integrity. 4.2. Big Data Applications for Disaster and Risk Management Big data systems proved to be valuable resources in disaster preparedness, management, and response. The disaster risk management authorities can big data to monitor the population in case of an emergency. For example, areas having a high of elderly people and children can be closely tracked so that they can be rescued as a priority. Additional post-disaster activities logistics and resource planning and -time communications are also facilitated by big data. 12.Conceptual model for big data utilization in estate transactions with decentralized
validation to ensure data integrity.
4.2. Big Data Applications for Disaster and Risk Management
Big data systems proved to be valuable resources in disaster preparedness, management, and response.
The disaster risk management authorities can big data to monitor the population in case of an emergency.
For example, areas having a high of elderly people and children can be closely tracked so that
they can be rescued as a priority. Additional post-disaster activities logistics and resource planning
and -time communications are also facilitated by big data. Agencies associated with early disaster Big Data Cogn. Comput.2020,4, 434 of 53
management also big data technologies to predict the reaction of citizens in case of a crisis [162]. In
the current era, big data- technologies are growing at an exponential rate, and research suggests
that approximately 90% of data in the world were produced in the last two years [163]. The emergency
management authorities can these data to make more informed and planned decisions in both pre- and
post-disaster scenarios. The data were combined with geographical information and -time imagery
for disaster risk management in emergencies [19]. During the Haiti earthquake incident, big data was to rescue people in the post-disaster scenario. By conducting an analysis on the data available
regarding the earthquake, maps were created to identify the vulnerable and affected population from
the area [164]. At this time, the concept of digital humanitarian was first introduced, which involves
the of technology crowdsourcing to generate maps of affected areas and people [165]. Since
then, it is a norm to technology for disaster risk management and response. Various research studies
were done on analyzing the sentiments of people at the time of disaster to identify their needs during
the crisis [19,122,162,164–166]. Advanced methods of satellite imagery, machine learning, and predictive
analysis are applied to gather information regarding any forthcoming disaster along with its consequences.
Munawar . [19] captured multispectral aerial images an unmanned aerial vehicle (UAV) at the
target site. Significant landmark objects bridges, roads, and buildings were extracted from these images edge detection [167], Hough transform, and isotropic surround suppression techniques [168,169].
The resultant images were to train an SVM classifier to identify the occurrence of flood in a test
image. Boakye . proposed a framework that uses big data analytics to predict the results of a natural
disaster in the society [162]. Machine learning and image processing also provide heat maps of the affected
area, which are helpful in providing timely and quick aid to affected people [166]. Table 16 shows the uses
of big data for disaster risk management, as well as the phases and features of big data.
Table 16.Big data sources, features, and phases specific to various disaster types.
PhaseFeatures
Source of Big Data
and Tools/Tech
Disaster Type
Company/Study Area and
Application
Prevention
Risk assessment
and mitigation
Call detail records
(CDR): GPS,
n-th-order Markov
chain models
Earthquake [170]
Rwanda: data mining, Markov chains,
statistical analysis to automate the prediction
of behavioral reaction of people to a disaster
Disaster prediction
Sensor web,
satellite,
simulations:
stepped frequency
microwave
radiometer (SFMR)
Hurricane
[171–173]
NOAA, Florida: model development, physics
implementation to improve hurricane
forecasts
Preparedness
Tracking and
detection
Combined data
types, Internet of
things (IoT):
LiDAR, GPS
Volcano [174,175]
Mt. Etna, Italy: distinguish ground objects
from natural and anthropic features digital terrain model (DTM) and digital
surface model (DSM)
Warning systems
Social media, IoT,
simulation:
SPARQL endpoints,
and client
Tsunami [176,177]
Eastern Mediterranean: IoT- early
warning system multi-semantic
representation model
Response
Damage
assessment
UAV, satellite, IoT:
3D modeling
Typhoon [178]
Distributed mobility algorithm to guarantee
quality of service (QoS)
Damage estimation
Census data:
capability (CA),
probabilistic
framework
Earthquake [162]
Seaside, Oregon: dynamic Bayesian network
to determine the state of well-being
Landmark (roads,
bridges, buildings)
detection
UAV imagery:
unmanned aerial
vehicle
Flood [19]
Pakistan: Hough transform, edge detection
and isotropic surround suppression to
identify significant landmark objects in post
disaster conditions
Post-disaster
communications
Social media,
satellite, sensor
web, GPS: GPS
General natural
disaster [178–180]
Network Science, CTA: team phone
consisting of a self-rescue system and a
messaging system
Digital
humanitarian
Crowdsourced data, Twitter data:
Twitter
Earthquake [164]
Haiti: chi-square method for content analysis Big Data Cogn. Comput.2020,4, 435 of 53
Table 16.Cont.
PhaseFeatures
Source of Big Data
and Tools/Tech
Disaster Type
Company/Study Area and
Application
Recovery
Relief missions
EM-DAT database:
statistical analysis
Earthquake
[181–183]
Center for Research on the Epidemiology of
Disasters (CFRED): big data analysis to
analyze the role of various factors in
increasing the death toll of natural disasters
Sentiment analysis
in the disaster
recovery process
Twitter Data:
Apache Spark big
data framework,
Python language
Flood [29]
India, Pakistan: sentiment analysis to
determine the needs of people during the
disaster
Social media is one of the best resources to gather -time data at the time of crisis. It is being
increasingly for communication and coordination during emergencies [184]. This calls for a
system to be able to effectively manage these data and filter the data related to the needs and requests
of the people during the post-disaster period. To be able to provide timely help, the big data generated
from the social networks should be mined and analyzed to determine factors which areas need the
most relief services and should be prioritized by the relief workers, and what services are required by
the people there [137]. In this , we propose a framework that extracts the data from various
social media networks Facebook, Twitter, news APIs, and other sources. The extracted data are
mostly in the unstructured form and need to undergo cleaning and pre-processing to remove irrelevant
and redundant information. This also involves removing URLs, emoticons, symbols, hashtags, and
words from a foreign language. After applying these pre-processing steps, the data need to be filtered
so that only relevant data are retained. During a post-disaster period, the basic needs of the people are
related to food, water, medical aid, and accommodation. Hence, some keywords related to these four
categories must be defined, so that only the data related to them are extracted. For example, the terms
related to the keyword “food” may be “hunger, starved, eat”. A wide range of terms related to each
keyword need to be defined so that maximum data related to them are extracted. It is also crucial to
gather these data along with information related to the geographical location, so that location-wise
aid could be provided. After gathering these data, the next step will be to train a machine learning
model, to predict which area needs emergency services and which facilities are needed by the people
over there. Before supplying data for classification, the data must be represented in the form of a
feature vector so that they can be interpreted by the algorithm. A unigram-, bigram-, or trigram- can be for generation of a feature vector from the data. The basic workflow of the
system is presented in 13.
The integration of big data into disaster risk management planning can open many avenues.
At the time of disasters floods, bush fires, storms, etc., there is a bulk of data generated as reports, statistics, and social media posts, which all provide a tally of injuries, deaths, and other losses
incurred [77,83,137]. An overview of the suggested system is provided by 14. The collective
historical data containing analytics of previous disasters are shared with the local authorities such
as fire brigades, ambulances, transportation management, and disaster risk management officials.
Acquisition of information leads to the formulation of plans to tackle the disaster and cope with the
losses. This plan of action is generated on the analysis of big data. Firstly, the data are processed
to pick specifics of current disaster, while analyzing the issue helps in moving toward a response. This
step involves more than one plan of action to have backup measures for coping with unforeseen issues.
All these steps are fundamentally guided and backed with information gained through the rigorous
processing of big data gathered as a bulk of raw information in the first step. The response stage is a
merger of several simultaneous actions including management of disaster, evaluation of the plan, and
-time recovery measures for overcoming the disaster and minimizing losses. This method not only
holds the potential for creating an iterative process which can be applied to various disasters but can
also create an awareness and sense of responsibility among people regarding the importance of big
data in disaster response and effective risk management. Big Data Cogn. Comput.2020,4, 436 of 53
Big Data Cogn. Comput. 2020, 4, 4 36 of 53 algorithm. A unigram-, bigram-, or trigram- can be for generation of a feature vector from the data. The basic workflow of the system is presented in 13. 13. Proposed framework for utilizing social media big data for emergency and disaster relief. The integration of big data into disaster risk management planning can open many avenues. At the time of disasters floods, bush fires, storms, etc., there is a bulk of data generated as reports, statistics, and social media posts, which all provide a tally of injuries, deaths, and other losses incurred [77,83,137]. An overview of the suggested system is provided by 14. The collective historical data containing analytics of previous disasters are shared with the local authorities such as fire brigades, ambulances, transportation management, and disaster risk management officials. Acquisition of information leads to the formulation of plans to tackle the disaster and cope with the losses. This plan of action is generated on the analysis of big data. Firstly, the data are processed to pick specifics of current disaster, while analyzing the issue helps in moving toward a response. This step involves more than one plan of action to have backup measures for coping with unforeseen issues. All these steps are fundamentally guided and backed with information gained through the rigorous processing of big data gathered as a bulk of raw information in the first step. The response stage is a merger of several simultaneous actions including management of disaster, evaluation of the plan, and -time recovery measures for overcoming the disaster and minimizing losses. This method not only holds the potential for creating an iterative process which can be applied to various disasters but can also create an awareness and sense of responsibility among people regarding the importance of big data in disaster response and effective risk management. 13.Proposed framework for utilizing social media big data for emergency and disaster relief.
Big Data Cogn. Comput. 2020, 4, 4 37 of 53 14. Big data utilization for disaster risk management. on the applications of big data in smart estate and disaster management, a merging point can be highlighted where the input big data from smart estate can help plan for disaster risks and manage them in case of occurrence, as shown in 15. The data of building occupants are usually maintained by the building managers and strata management. These data coupled with the data from building integration, maintenance, and facility management constitutes smart estate big data controlled by the estate managers. These data, if refined and shared with the disaster managers and response teams by the smart estate management agencies and managers, can help in planning for disaster response. For example, the data related to available facilities at the building can help prepare the occupants for upcoming disasters through proper training and awareness, who can respond to these disasters in an efficient way. Similarly, knowledge of smart building components and the associated building management data can help address the four key areas of disaster risk management: prevent, prepare, respond, and recover. The proposed merging framework is inspired by the works of Grinberger . [185], Lv . [186], Hashem . [187], and Shah . [30]. Grinberger . [185] data obtained from smart estate in terms of occupant data in terms of socioeconomic attributes such as income, age, car ownership, and building data on value and floor space to investigate the disaster preparedness response for a hypothetical earthquake in downtown Jerusalem. Lv . [186] proposed a model for big data obtained from multimedia usage by estate users to develop a disaster management plan for service providers such as traffic authorities, fire, and other emergency departments. Hashem . [187] proposed an integrated model on wireless sensing technologies that can integrate various components of smart cities for industrial process monitoring and control, machine health monitoring, natural disaster prevention, and water quality monitoring. Similarly, Shah . [30] proposed a disaster-resilient smart city concept that integrates IoT and big data technologies and offers a generic solution for disaster risk management activities in smart city incentives. Their framework is on a combination of the Hadoop Ecosystem and Apache Spark that supports both -time and offline analysis, and the implementation model consists of data harvesting, data aggregation, data pre-
processing, and a big data analytics and service platform. A variety of datasets from smart buildings, city pollution, traffic simulators, and social media such as Twitter are utilized for the validation and evaluation of the system to detect and generate alerts for a fire in a building, pollution level in the city, emergency evacuation path, and the collection of information about natural disasters such as earthquakes and tsunamis. Furthermore, Yang . [25] proposed -time feedback loops on nature 14.Big data utilization for disaster risk management. on the applications of big data in smart estate and disaster management, a merging
point can be highlighted where the input big data from smart estate can help plan for disaster
risks and manage them in case of occurrence, as shown in 15. The data of building occupants
are usually maintained by the building managers and strata management. These data coupled with Big Data Cogn. Comput.2020,4, 437 of 53
the data from building integration, maintenance, and facility management constitutes smart estate big data controlled by the estate managers. These data, if refined and shared with the
disaster managers and response teams by the smart estate management agencies and managers,
can help in planning for disaster response. For example, the data related to available facilities at
the building can help prepare the occupants for upcoming disasters through proper training and
awareness, who can respond to these disasters in an efficient way. Similarly, knowledge of smart
building components and the associated building management data can help address the four key
areas of disaster risk management: prevent, prepare, respond, and recover. The proposed merging
framework is inspired by the works of Grinberger . [185], Lv . [186], Hashem . [187], and
Shah . [30]. Grinberger . [185] data obtained from smart estate in terms of occupant
data in terms of socioeconomic attributes such as income, age, car ownership, and building data on value and floor space to investigate the disaster preparedness response for a hypothetical
earthquake in downtown Jerusalem. Lv . [186] proposed a model for big data obtained from
multimedia usage by estate users to develop a disaster management plan for service providers
such as traffic authorities, fire, and other emergency departments. Hashem . [187] proposed an
integrated model on wireless sensing technologies that can integrate various components of
smart cities for industrial process monitoring and control, machine health monitoring, natural disaster
prevention, and water quality monitoring. Similarly, Shah . [30] proposed a disaster-resilient smart
city concept that integrates IoT and big data technologies and offers a generic solution for disaster
risk management activities in smart city incentives. Their framework is on a combination of
the Hadoop Ecosystem and Apache Spark that supports both -time and offline analysis, and the
implementation model consists of data harvesting, data aggregation, data pre-processing, and a big
data analytics and service platform. A variety of datasets from smart buildings, city pollution, traffic
simulators, and social media such as Twitter are utilized for the validation and evaluation of the system
to detect and generate alerts for a fire in a building, pollution level in the city, emergency evacuation
path, and the collection of information about natural disasters such as earthquakes and tsunamis.
Furthermore, Yang . [25] proposed -time feedback loops on nature disasters to help estate
and city decision-makers make -time updates, along with a precision and dynamic rescue plan
that helps in in all four phases of disaster risk management: prevention, mitigation, response, and
recovery; this can help the city and estate planners and managers to take prompt and accurate
actions to improve the city’s resilience to disasters.
Big Data Cogn. Comput. 2020, 4, 4 38 of 53 disasters to help estate and city decision-makers make -time updates, along with a precision and dynamic rescue plan that helps in in all four phases of disaster risk management: prevention, mitigation, response, and recovery; this can help the city and estate planners and managers to take prompt and accurate actions to improve the city’s resilience to disasters. 15. Smart estate big data as an input to disaster management. This is a two-way process where data from smart estate can help prepare for disasters and vice vera. Big data in preparedness and emergency planning may increase urban resilience as it will help to produce more accurate emergency and response plans. As such, Deal . [188] argued that, for achieving the holistic results for developing urban resilience and promoting preparedness among the communities for disaster, there is a need to be able to translate big data at scales and in ways that are useful and approachable through sophisticated planning support systems. Such systems must possess a greater awareness of application context and needs; furthermore, they must be capable of iterative learning, be capable of spatial and temporal reasoning, understand rules, and be accessible and interactive. Kontokosta and Malik [189] introduced the concept of benchmarking neighborhood resilience by developing a resilience to emergencies and disasters index that integrates physical, natural, and social systems through big data collected from large-scale, heterogeneous, and high-resolution urban data to classify and rank the relative resilience capacity embedded in localized urban systems. Such systems can help improve urban resilience by preparing and producing accurate emergency responses in the case of disasters. Similarly, Klein . [190] presented the concept of a responsive city, in which citizens, enabled by technology, take on an active role in urban planning processes. As such, big data can inform and support this process with evidence by taking advantage of behavioral data from infrastructure sensors and crowdsourcing initiatives to help inform, prepare, and evacuate citizens in case of disasters. Furthermore, the data can be overlaid with spatial information in order to respond to events in decreasing time spans by automating the response process partially, which is a necessity for any resilient city management. Owing to these systems and examples, it can be inferred that smart estate and disaster risk management can act as lifelines to each other, where big data generated in one field can be to help strengthen the other, which, if achieved, can help move toward integrated city and urban management. 4.3. Discussion The current review provides a systematic view of the field of big data applications in smart estate and disaster and risk management. This paper reviewed 139 articles on big data concepts and tools, as well as its applications in smart estate and disaster management. Initially, the seven Vs 15.Smart estate big data as an input to disaster management. Big Data Cogn. Comput.2020,4, 438 of 53
This is a two-way process where data from smart estate can help prepare for disasters and
vice vera. Big data in preparedness and emergency planning may increase urban resilience as it
will help to produce more accurate emergency and response plans. As such, Deal . [188] argued
that, for achieving the holistic results for developing urban resilience and promoting preparedness
among the communities for disaster, there is a need to be able to translate big data at scales and in
ways that are useful and approachable through sophisticated planning support systems. Such systems
must possess a greater awareness of application context and needs; furthermore, they must be
capable of iterative learning, be capable of spatial and temporal reasoning, understand rules, and
be accessible and interactive. Kontokosta and Malik [189] introduced the concept of benchmarking
neighborhood resilience by developing a resilience to emergencies and disasters index that integrates
physical, natural, and social systems through big data collected from large-scale, heterogeneous, and
high-resolution urban data to classify and rank the relative resilience capacity embedded in localized
urban systems. Such systems can help improve urban resilience by preparing and producing accurate
emergency responses in the case of disasters. Similarly, Klein . [190] presented the concept of a
responsive city, in which citizens, enabled by technology, take on an active role in urban planning
processes. As such, big data can inform and support this process with evidence by taking advantage of
behavioral data from infrastructure sensors and crowdsourcing initiatives to help inform, prepare, and
evacuate citizens in case of disasters. Furthermore, the data can be overlaid with spatial information
in order to respond to events in decreasing time spans by automating the response process partially,
which is a necessity for any resilient city management. Owing to these systems and examples, it can be
inferred that smart estate and disaster risk management can act as lifelines to each other, where
big data generated in one field can be to help strengthen the other, which, if achieved, can help
move toward integrated city and urban management.
4.3. Discussion
The current review provides a systematic view of the field of big data applications in smart estate and disaster and risk management. This paper reviewed 139 articles on big data concepts and
tools, as well as its applications in smart estate and disaster management. Initially, the seven
Vs of big data were explored with their applications in smart estate and disaster management.
This was followed by big data analytics tools including , audio, video, and social media analytics
with applications in smart estate and disaster management. Next, big data analytics processes
comprising data collection, storage, filtering, cleaning, analysis, and visualization were explored along
with the technologies and tools for each stage. Then, the two main frameworks for big data
analytics, i.e., Hadoop and Apache Spark, were reviewed and compared on their parameters and
performance. Afterward, the applications of machine learning for big data were explored. This was
followed by the challenges faced by big data, and potential solutions to its implementation in different
fields were discussed. Lastly, a dedicated explored the applications of big data in various fields
with a specific focus on smart estate and disaster management and how big data can be to
integrate the two fields. These findings and critical analyses distinguish this review from previous
reviews. Another difference of this review compared with previous attempts is the focus of the present
review on the applications of big data in smart estate and disaster management that highlights the
potential for integrating the two fields. The findings and major analyses are discussed below.
Firstly, it was found that the definition of big data continues to vary, and no exact size is defined to
specify the volume of data that qualifies as big data. The concept of big data was found to be relative,
and any data that cannot be handled by the traditional databases and data processing tools are classified
as big data. In terms of the papers published in the area of big data, there as a significant growth in
the of articles in the last 10 years. A total of 139 relevant papers were investigated in detail,
consisting of original research on big data technologies (59), reviews (23), conferences (18), and case
studies (10). The analyses revealed that the keywords most frequently in big data papers were
dominated by analysis system, investigations, disaster risk management, estate technologies, urban Big Data Cogn. Comput.2020,4, 439 of 53
area, and implementation challenges. Furthermore, the publications were dominated by the journal
lecture notes in computer science followed by the IOP conference series. In terms of the author-specific
contributions Wang Y. and Wang J. lead the reviewed articles with 13 and 11 contributions and
24 citations each. Similarly, in country-specific analysis, China leads the reviewed articles with 34
publications followed by the United States with 24 articles; however, in terms of citations, the USA
leads the table with 123 citations followed by China with 58 citations. Furthermore, in terms of the
affiliated organizations of authors contributing the most to the articles reviewed, the Center for Spatial
Information Science, University of Tokyo, Japan and the School of Computing and Information Sciences,
Florida International University, Miami, Fl 33199, United States lead the race with six articles each,
followed by the International Research Institute of Disaster Science (Irides), Tohoku University, Aoba
468-1, Aramaki, Aoba-Ku, Sendai, 980-0845, Japan with five articles.
In the next step, a seven Vs model was discussed from the literature to review the distinctive
features of big data, including variety, volume, velocity, value, veracity, variability, and visualization.
Various tools and technologies in each stage of the big data lifecycle were critically examined
to assess their effectiveness, along with implementation examples in smart estate and disaster
management. Variety can help in disaster risk management through major machine–human interactions
by extracting data from data lakes. It can help in smart estate management through urban big data
that can be converged, analyzed, and mined with depth via the Internet of things, cloud computing, and
artificial intelligence technology to achieve the goal of intelligent administration of the smart estate.
The volume of big data can be in smart estate through e-commerce platforms and digital
marketing for improving the financial sector, hotel services, culture, and tourism. For the velocity
aspect, information is shared on sites such as Facebook, Twitter, and YouTube every second that
can help disaster risk managers plan for upcoming disasters, as well as know the current impacts of the
occurring disasters, efficient data extraction tools. In smart estate, big data-assisted customer
analysis and advertising architecture can be to speed up the advertising process, approaching
millions of users in single clicks, which helps in segmentation, customer mining, and modified
and personalized precise advertising delivery to achieve high advertising arrival rate, as well as
superior advertising exposure/click conversion rate. In case of the value aspect of big data, disaster
risk management decision-making systems can be by disaster managers to make precise and
insightful decisions. Similarly, in smart estate, neighborhood value can be enhanced through
creation of job opportunities and digital travel information to promote smart mobility. In the context
of the veracity of big data, sophisticated software tools can be developed that extract meaningful
information from vague, poor-quality information or misspelled words on social media to promote
local estate business and address or plan for upcoming disasters. Variability of the big data can be to develop recommender systems for finding places with the highest wellness state or assessing
the repayment capabilities of large estate organizations. Similarly, variability related to rainfall
patterns or temperature can be to plan effectively for hydro-meteorological disasters. In the case of
the visualization aspect of big data, 360 cameras, mobile and terrestrial laser scanners [74,144,191–194],
and 4D advertisements can help boost the smart estate business. Similarly, weather sensors can be to detect ambiguities in weather that can be visualized to deal with local or global disasters.
After the seven Vs were investigated, big data analytics and the pertinent techniques including
, audio, video, and social media mining were explored. mining can be to extract useful
data from news, email, blogs, and survey forms through NER and RE. Cassandra NoSQL, WordNet,
ConceptNet, and SenticNet can be for mining. In the case of smart estate, mining can
be to explore hotel guest experience and satisfaction and estate investor psychology, whereas,
in disaster risk management, it can be to develop tools such as DisasterMapper that can synthesize
multi-source data, as well as contribute spatial data mining, mining, geological visualization, big
data management, and distributed computing technologies in an integrated environment. Audio
analytics can aid smart estate through property auctioning, visual feeds digital cameras, and
associated audio analytics on the conversation between the estate agent and the prospective Big Data Cogn. Comput.2020,4, 440 of 53
buyer to boost the estate sales. In case of disaster risk management, audio analytics can help in
event detection, collaborative answering, surveillance, threat detection, and telemonitoring. Video
analytics can be in disaster management for accident cases and investigations, as well as disaster
area identification and damage estimation, whereas, in smart estate, it can be for threat
detection, security enhancements, and surveillance. Similarly, social media analytics can help smart estate through novel recommender systems for shortlisting places that interests users related to
cultural heritage sites, museums, and general tourism machine learning and artificial intelligence.
Similarly, multimedia big data extracted from social media can enhance -time detection, alert
diffusion, and spreading alerts over social media for tackling disasters and their risks.
In the data analytics processes, steps including data collection, storage, filtering, cleaning, analysis,
and visualization were explored along with the pertinent tools present for each step. The tools for data
collection include Semantria, which is deployed through web, with the limitation of crashing on large
datasets, web-deployable Opinion crawl, which cannot be for advanced SEO audits, Open deployed through Captiva, having rigorous requirements of configurations, and Trackur, which is costly.
These tools can be for sentiment and content analyses of the estate stakeholders. Among
the tools for data storage, NoSQL tools were explored considering four categories: column-oriented,
document-oriented, graph, and key value. Apache Cassandra, HBase, MongoDB, CouchDB, Terrastore,
Hive, Neo4j, AeroSpike, and Voldemort have applications in the areas of Facebook inbox search, online
trading, asset tracking system, textbook management system, International Business Machines, and
event processing that can be applied to both smart estate and disaster management. Among the
data filtering tools, Import.io, Parsehub, Mozenda, Content Grabber, and Octoparse were explored,
which are web- and cloud- software and are helpful for scheduling of data and visualizations point-and-click approaches. The output data from these tools in the shape of data reports, google
sheets, and CSV files can be by both smart estate managers and disaster risk management
teams. Among the data cleaning tools, Data Cleaner, Map Reduce, Open Refine, Reifier, and Trifecta
Wrangler Hadoop frameworks and web services for duplicate value detection, missing value
searches among the sheets at higher pace, and accuracy levels that can help smart estate and
disaster management detect ambiguities in the reports and address the issues accordingly. Lastly, for
data visualization tools, Tableau, Microsoft Power BI, Plotly, Gephi, and Excel were explored that can
help the estate managers promote immersive visualizations and generation of -specific charts.
Other tools such as 360 cameras, VR and AR gadgets, and the associated 4D advertisements can help
boost property sales, as well as prepare the users for disaster response.
Two major frameworks for data analysis were identified which are Hadoop and Apache Spark.
By conducting a critical analysis and comparison of these two frameworks, it was inferred that Apache
Spark has several advantages over Hadoop which includes increased networking memory, the ability
to perform -time processing, faster speed, and increased storage capacity, which can help the estate consumer make better and informed decisions. Similarly, disaster managers can prepare and
respond in a better way to the upcoming or occurred disasters on well-sorted and high-quality
information. However, best results can be achieved by a combination of these frameworks as
discussed in Mavridis and Karatza [110] to incorporate the prominent features from both frameworks.
In addition, applications of machine learning such as speech recognition, predictive algorithms, and
stock market price fluctuation analyses can help estate users and investors in making smart
decisions. Furthermore, clustering, prediction and decision-making can help disaster managers cluster
the events, predict upcoming disasters, and make better decisions for dealing with them.
Following the framework exploration, the four most dominant challenges encountered while
dealing with big data were highlighted, including data security and privacy, heterogeneity and
incompleteness, fault tolerance, and storage. To deal with the first challenges, solutions such as authentication methods, Kerberos, and encrypted files are suggested. Furthermore, logging of
attacks or unusual behavior and secure communication through SSL and TLS can handle the privacy
and security concerns. Such privacy concerns, if addressed properly, can motivate estate users Big Data Cogn. Comput.2020,4, 441 of 53
to the smart features and technologies and incline them toward adopting more technologies,
thus disrupting the traditional estate market and moving toward a smart estate. Similarly,
privacy concerns, if addressed, can motivate people to help disaster risk management teams on a
volunteer basis rather than sneakily analyzing social media stuffwithout approval. To deal with
heterogeneity and incompleteness, data imputation for missing values, building learning models,
and filling data with the most frequent values are some solutions. Similarly, to tackle fault tolerance,
dividing computations into sub-tasks and checkpoint applications for recursive tasks are potential
solutions. Lastly, to tackle the challenge of storage, SDD and PCM can be .
Finally, in terms of the applications of big data, it is evident that, in almost all fields, ranging
from technology to healthcare, education, agriculture, business, and even social life, big data plays
an important role. Since data are generated every second, it is important to know how to them
well. In healthcare settings, patient information and medical outcomes are recorded on a regular basis,
which add to the generation of data in the healthcare sector. Arranging and understanding these data
can help in identifying key medical procedures, their outcomes, and possibly ways in which patient
outcomes could be enhanced through certain medicines. Similarly, education, business, technology,
and agriculture can all benefit from data gathered by these fields. existing data in a positive
manner can pave a way forward for each field. Something that is already known and exists in databases
in an organized manner can help people around the world and ensure that big data could be put to
good . For example, recently, big data analytics was successfully integrated for disaster prediction
and response activities. Big data consisting of weather reports, past flood events, historic data, and
social media posts can be gathered to analyze various trends and identify the conditioning factors
leading to a disaster. These data can also be examined to determine the most disaster-prone regions
by generating susceptibility maps. Furthermore, these data can be to train a machine learning
model, which could make predictions about the occurrence of disasters and detect the effected regions
from a given test image. The of social media is a huge source of generating data. These data
are already being for various marketing researches and the analysis of human psychology and
behaviors. If these data are with safety and put to sensible , there is a chance that every field
could benefit from the inexhaustible data sources that exist on the worldwide web. Similarly, for smart estate management, big data has huge potential in the areas of technology integration, technology
adoption, smart homes and smart building integration, customer management, facilities management,
and others. As such, the customers or users can enjoy the personalization, cross-matching, property
information, and buyer demand analysis with the help of big data resources such as customer data
surveys, feedback analyses, data warehouses, buyer click patterns, predictive analytics tools, to
government information, and social media analytics. The owners, agents, or sellers can benefit from
building performance databases, property value analysis, resident, strata, and enterprise management,
online transactions, and potential clients/business identification big data resources of building
maintenance data, occupant data, government reports, local contracts, property insights, analytics tools,
customer surveys, and demand analysis. Similarly, the government and regulatory authorities can
provide more public services, detect frauds, and address and citizen privacy and security issues
through linkages of the central databases to ensure provision of services in the smart estate set-up.
For disaster risk management, the four stages of prevention, preparedness, response, and recovery
can be aided through big data utilizations. As such, big data can help in risk assessment and mitigation,
disaster prediction, tracking and detection, establishing warning systems, damage assessment, damage
estimation, landmark (roads, bridges, buildings) detection, post-disaster communications establishment,
digital humanitarian relief missions, and sentiment analysis in the disaster recovery process to help
mitigate or respond to natural disasters such as earthquakes, hurricanes, bushfires, volcanic eruptions,
tsunamis, floods, and others. Tools and technologies such as GPS, LiDAR, IoT, stepped frequency
microwave radiometer (SFMR), satellite imagery, and drone- data collection can aid the disaster
risk management processes. In addition, the fields of smart estate and disaster management can be
integrated where smart big data from estate can help the disaster risk management team prepare Big Data Cogn. Comput.2020,4, 442 of 53
and respond to the disasters. As such, the data received from building occupants, building integration,
maintenance, and facility management can be shared with the disaster management teams who can
integrate with the central systems to better respond to disasters or emergencies.
This paper provides a detailed analysis of big data concepts, its tools, and techniques, data analytics
processes, and tools, along with their applications in smart estate and disaster management, which
can help in defining the research agenda in the two main domains of smart estate and disaster
management and move toward an integrated management system. It has implications for creating a
win–win situation in the smart estate. Specifically, it can help smart estate managers, agents,
and sellers attract more customers toward the properties through immersive visualizations, thus
boosting the business and sales. The customers, on the other hand, can make better and regret-free
decisions on high-quality, transparent, and immersive information, thus raising their satisfaction
levels. Similarly, the government and regulatory authorities can provide better citizen services, ensure
safety and privacy of citizens, and detect frauds. Similarly, the proposed framework for disaster risk
management can help the disaster risk managers plan for, prepare for, and respond to upcoming
disasters through refined, integrated, and well-presented big data. In addition, the current study has
implications for research where the integration of the two fields, i.e., smart estate and disaster
management, can be explored from a integrated perspective, while conceptual and field-specific
frameworks can be developed for realizing an integrated, holistic, and all-inclusive smart city dream.
The limitation of the paper is its focus on two domains; however, future studies can also focus on
the application of big data in construction management and other disciplines. This paper reviewed
139 articles published between 2010 and 2020, but further articles from before 2010, as well as articles
focusing on smart cities, can be reviewed in the future to develop a holistic city management plan.
Among the other limitations, a focus on only two types of frameworks (Hadoop and Apache Spark)
and non-focus on other digital disruptive technologies such as the Big9 technologies discussed by
Ullah . [18] are worth mentioning. Furthermore, the current study its review on the articles
retrieved through a specific sampling method, which may not be all-inclusive and exhaustive; thus,
future studies repeated with the same keywords at different times may yield different results.
5. Conclusions
Big data became the center of research in the last two decades due to the significant rise in the
generation of data from various sources such as mobile phones, computers, and GPS sensors. Various
tools and techniques such as web scraping, data cleaning, and filtering are applied to big databases to
extract useful information which is then to visualize and draw results from unstructured data.
This paper reviewed the existing concept of big data and the tools available for big data analytics,
along with discussing the challenges that exist in managing big data and their possible solutions.
Furthermore, the applications of big data in two novel and integrated fields of smart estate and
disaster management were explored. The detailed literature search showed that big data papers
are following an increasing trend, growing tremendously from fewer than 100 in 2010 to more than
1200 in 2019. Furthermore, in terms of the most repeated keywords in the big data papers in the
last decade, data analytics, data solutions, datasets, frameworks, visualization, algorithms, problems,
decision-making, and machine learning were the most common ones. In the systematic review,
distinctive features of big data including the seven Vs of big data were highlighted, including variety,
volume, velocity, value, veracity, variability, and visualization, along with their uses in the smart estate and disaster sectors. Similarly, in terms of data analytics, the most common sub-classes
include analytics, audio analytics, video analytics, and social media analytics. The methods for
analyzing data from these classes include the process of data collection, storage, filtering, cleaning,
analysis, and visualizations. Similarly, security and privacy, heterogeneity and incompleteness, fault
tolerance, and storage are the top challenges faced by big data managers, which can be tackled authentication methods, Kerberos, and encrypted files, logging of attacks or unusual behavior and
secure communication through SSL and TLS, data imputation for missing values, building learning Big Data Cogn. Comput.2020,4, 443 of 53
models and filling the data with most frequent values, dividing computations into sub-tasks, and
checkpoint applications for recursive tasks, and SDD and PCM, respectively.
In terms of the frameworks for data analysis, Hadoop and Apache Spark are the two most frameworks. However, for better results, it is ideal and recommended to both simultaneously to
capture the holistic essence. Furthermore, the of machine learning in big data analytics sounds
really promising, especially due to its applications in disaster risk management and rescue services. its modules of supervised, unsupervised, and reinforced learning, machine learning holds the
key to linking big data to other fields. With the continuous rise in technology, it is quite possible that
machine learning approaches will take centerstage in big data management and analysis. The way
forward is, therefore, to explore newer algorithms and software systems which can be employed for
sorting, managing, analyzing, and storing big data in a manner that could be useful.
For specific applications in smart estate and disaster management, big data can help in
disrupting the traditional estate industry and pave the way toward smart estate. This can
help reduce estate consumer regrets, as well as improve the relationships between the three
main stakeholders: buyers, sellers, and government agencies. The customers can benefit from big
data applications such as personalization, cross-matching, and property information. Similarly, the
sellers can benefit from building performance database management, property value analysis, resident,
strata, and enterprise management, online transaction, and potential clients/business identification.
Furthermore, the government and regulatory agencies can provide more security, ensure privacy
concerns are addressed, detect fraud, and provide more public services to promote smart estate. A
positive step in this direction is the adoption of big data by estate organizations such as Airbnb,
BuildZoom, ArchiBus, CoreLogic, Accenture, Truss, SmartList, and others around the world. Big
data tools and resources such as customer data surveys, feedback analyses, data warehouses, buyer
click patterns, predictive analytics, social media analytics, building maintenance data, occupant data,
government reports, local contracts, property insights, drones, artificial intelligence-powered systems,
and smart processing systems can help transform the estate sector into smart estate. Similarly,
for disaster management, the application of big data in the four stages of disaster risk management, i.e.,
prevention, preparedness, response, and recover, can help in risk assessment and mitigation, disaster
prediction, tracking and detection of damages, warning system implementation, damage assessment,
damage estimation, landmark (roads, bridges, buildings) detection, post-disaster communications,
digital humanitarian relief missions, and sentiment analyses. Several tools with the potential of
generating and/or processing big data such as -time locating systems [195,196], sensor web data,
satellite imagery, simulations, IoT, LiDAR [75,76,191,197,198], 3D modeling [75,199], UAV Imagery,
social media analytics, and crowdsourced data can help to plan for disasters and mitigate them in
the case of occurrence.
This study can be extended in the future to include research questions about integrations of
various big data technologies and analytics tools in field-specific contexts such as data lakes and
fast data. Furthermore, this paper investigated the four big data analytics processes which can be
extended to explore data ingestion in the future. The scope of the paper can be enhanced to answer
questions such as the most significant challenges posed by big data in specific fields such as estate and property management or disaster management, and how technological advancements are
being to tackle these challenges. Further applications of big data in smart estate in the
context of technology readiness by the businesses, industry preparedness for big data disruptions, and
adoption and implementation barriers and benefits can be explored in future studies. Similarly, in
disaster risk management contexts, applications of big data drones, UAVs, and satellites for
addressing bushfires, floods, and emergency response systems can also be explored in detail. Apart
from automated tools, some programming languages python and R can also be identified, and
their for big data analytics can be investigated in the light of recent research. Furthermore, this
paper discussed widely and popular tools Tableau and Excel for big data analytics; thus,
future studies can explore some less conventional tools to assess their performance outcomes. Big Data Cogn. Comput.2020,4, 444 of 53
Author Contributions:
Conceptualization, H.S.M., F.U. and S.Q.; methodology, H.S.M., F.U., S.Q. and S.S.;
software, F.U. and S.Q.; validation, H.S.M., S.Q. and F.U.; formal analysis, H.S.M., F.U., and S.Q.; investigation,
H.S.M., S.Q., F.U. and S.S.; resources, S.S.; data curation, F.U.; writing—original draft preparation, H.S.M., F.U.,
and S.Q.; writing—review and editing, F.U. and S.S.; visualization, F.U. and S.Q.; supervision, S.S. and F.U.;
project administration, H.S.M., S.Q., F.U. and S.S.; funding acquisition, S.S. All authors have read and agree to the
published version of the manuscript.
Funding:This research received no external funding.
Conflicts of Interest:The authors declare no conflicts of interest.
References
1.
Shirowzhan, S.; Sepasgozar, S.M.; Li, H.; Trinder, J.; Tang, P. Comparative analysis of machine learning and
point- algorithms for detecting 3D changes in buildings over time bi-temporal lidar data.Autom.
Constr.2019,105, 102841. [CrossRef]
2.
Ahmad, I. How Much Data Is Generated Every Minute? Available online: https://www.socialmediatoday.
com/news/how-much-data-is-generated-every-minute-infographic-1/525692/(accessed on 3 February 2020).
3.Padhi, B.K.; Nayak, S.; Biswal, B. Machine learning for big data processing: A literature review.Int. J. Innov.
Res. Technol.2018,5, 359–368.
4.Lynkova, D.39+Big Data Statistics for 2020; LEFTRONIC: San Francisco, CA, USA, 2019.
5.Fang, H. Managing data lakes in big data era: What’s a data lake and why has it became popular in data
management ecosystem. In Proceedings of the 2015 IEEE International Conference on Cyber Technology in
Automation, Control, and Intelligent Systems (CYBER), Shenyang, China, 8–12 June 2015.
6.Sagiroglu, S.; Sinanc, D. Big data: A review. In Proceedings of the 2013 International Conference on
Collaboration Technologies and Systems (CTS), San Diego, CA, USA, 20–24 May 2013.
7.Chen, C.P.; Zhang, C.-Y. Data-intensive applications, challenges, techniques and technologies: A survey on
big data.Inf. Sci.2014,275, 314–347. [CrossRef]
8.Agrawal, R.; Kadadi, A.; Dai, X.; Andres, F. Challenges and opportunities with big data visualization.
In Proceedings of the 7th International Conference on Management of Computational and Collective
IntElligence in Digital EcoSystems, Caraguatatuba, Brazil, 25–29 October 2015.
9.Procopio, M.; Scheidegger, C.; Wu, E.; Chang, R. Selective wander join: Fast progressive visualizations for
data joins.Informatics2019,6, 14. [CrossRef]
10.
Roy, R.; Paul, A.; Bhimjyani, P.; Dey, N.; Ganguly, D.; Das, A.K.; Saha, S. A short review on applications
of big data analytics. InEmerging Technology in Modelling and Graphics; Springer: Berlin, Germany, 2020;
. 265–278.
11.Baseman, J.G.; Revere, D.; Painter, I. Big data in the era of health information exchanges: Challenges and
opportunities for public health.Informatics2017,4, 39. [CrossRef]
12.
Alshboul, Y.; Nepali, R.; Wang, Y. Big data lifecycle: Threats and security model. In Proceedings of the 21st
Americas Conference on Information Systems, Fajardo, Puerto Rico, 13–15 August 2015.
13.Stefanova, S.; Draganov, I. Big Data Life Cycle in Modern Web Systems. Available online: http://conf.uni-
ruse.bg/bg/docs/cp18/3.2/3.2-15.pdf (accessed on 21 March 2020).
14.Wielki, J. Implementation of the big data concept in organizations-possibilities, impediments and challenges.
In Proceedings of the 2013 Federated Conference on Computer Science and Information Systems, Kraków,
Poland, 8–11 September 2013.
15.
Acharjya, D.P.; Ahmed, K. A survey on big data analytics: Challenges, open research issues and tools.Int. J.
Adv. Comput. Sci. Appl.2016,7, 511–518.
16.Dey, N.; Hassanien, A.E.; Bhatt, C.; Ashour, A.; Satapathy, S.C.Internet of Things and Big Data Analytics toward
Next-Generation Intelligence; Springer: Berlin, Germany, 2018.
17.Ruiz, Z.; Salvador, J.; Garcia-Rodriguez, J. A survey of machine learning methods for big data. InBiomedical
Applications on Natural and Artificial Computing; Springer: Berlin, Germany, 2017.
18.
Ullah, F.; Sepasgozar, S.M.; Wang, C. A systematic review of smart estate technology: Drivers of, and
barriers to, the of digital disruptive technologies and online platforms.Sustainability2018,10, 3142.
[CrossRef] Big Data Cogn. Comput.2020,4, 445 of 53
19.
Munawar, H.S.; Hammad, A.; Ullah, F.; Ali, T.H. After the flood: A novel application of image processing
and machine learning for post-flood disaster management. In Proceedings of the International Conference
on Sustainable Development in Civil Engineering, MUET, Jamshoro, Pakistan, 5–7 December 2019.
20.Ullah, F.; Sepasgozar, P.S.; Ali, T.H. estate stakeholders technology acceptance model (RESTAM):
-focused Big9 disruptive technologies for smart estate management. In Proceedings of the 2nd
International Conference on Sustainable Development in Civil Engineering (ICSDC 2019), Jamshoro, Pakistan,
5–7 December 2019.
21.Pan, Y.; Tian, Y.; Liu, X.; Gu, D.; Hua, G. Urban big data and the development of city intelligence.Engineering
2016,2, 171–178. [CrossRef]
22.Kelman, I. Lost for words amongst disaster risk science vocabulary?Int. J. Disaster Risk Sci.2018,9, 281–291.
[CrossRef]
23.Aitsi-Selmi, A.; Murray, V.; Wannous, C.; Dickinson, C.; Johnston, D.; Kawasaki, A.; Stevance, A.-S.; Yeung, T.
Reflections on a science and technology agenda for 21st century disaster risk reduction.Int. J. Disaster Risk
Sci.2016,7, 1–29. [CrossRef]
24.Tanner, T.; Surminski, S.; Wilkinson, E.; Reid, R.; Rentschler, J.; Rajput, S. The Triple Dividend of Resilience:
Realising Development Goals through the Multiple Benefits of Disaster Risk Management. Available online:
https://eprints.soas.ac.uk/31372/1/The_Triple_Dividend_of_Resilience.pdf (accessed on 21 March 2020).
25.Yang, C.; Su, G.; Chen, J. big data to enhance crisis response and disaster resilience for a smart city. In
Proceedings of the 2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA), Beijing, China,
10–12 March 2017.
26.Cheng, M.-Y.; Chiu, K.-C.; Hsieh, Y.-M.; Yang, I.-T.; Chou, J.-S.; Wu, Y.-W. BIM integrated smart monitoring
technique for building fire prevention and disaster relief.Autom. Constr.2017,84, 14–30. [CrossRef]
27.
Yang, T.; Xie, J.; Li, G.; Mou, N.; Li, Z.; Tian, C.; Zhao, J. Social Media Big Data Mining and Spatio-Temporal
Analysis on Public Emotions for Disaster Mitigation.ISPRS Int. J. Geo Inf.2019,8, 29. [CrossRef]
28.Ofli, F.; Meier, P.; Imran, M.; Castillo, C.; Tuia, D.; Rey, N.; Briant, J.; Millet, P.; Reinhard, F.; Parkan, M.
Combining human computing and machine learning to make sense of big (aerial) data for disaster response.
Big Data2016,4, 47–59. [CrossRef] [PubMed]
29.Ragini, J.R.; Anand, P.R.; Bhaskar, V. Big data analytics for disaster response and recovery through sentiment
analysis.Int. J. Inf. Manag.2018,42, 13–24. [CrossRef]
30.Shah, S.A.; Seker, D.Z.; Rathore, M.M.; Hameed, S.; Yahia, S.B.; Draheim, D. Towards disaster resilient smart
cities: Can Internet of Things and big data analytics be the game changers?IEEE Access2019,7, 91885–91903.
[CrossRef]
31.Akter, S.; Wamba, S.F. Big data and disaster management: A systematic review and agenda for future
research.Ann. Oper. Res.2019,283, 939–959. [CrossRef]
32.Sepasgozar, S.M.; Karimi, R.; Shirowzhan, S.; Mojtahedi, M.; Ebrahimzadeh, S.; McCarthy, D. Delay causes
and emerging digital tools: A novel model of delay analysis, including integrated project delivery and
PMBOK.Buildings2019,9, 191. [CrossRef]
33.Zhong, B.; Wu, H.; Li, H.; Sepasgozar, S.; Luo, H.; He, L. A scientometric analysis and critical review of
construction related ontology research.Autom. Constr.2019,101, 17–31. [CrossRef]
34.Sepasgozar, S.M.; Li, H.; Shirowzhan, S.; Tam, V.W. Methods for monitoring construction off-road vehicle
emissions: A critical review for identifying deficiencies and directions.Environ. Sci. Pollut. Res.2019,26,
15779–15794. [CrossRef]
35.Sepasgozar, S.M.; Blair, J. Measuring non-road diesel emissions in the construction industry: A synopsis of
the literature.Int. J. Constr. Manag.2019, 1–16. [CrossRef]
36.
Felli, F.; Liu, C.; Ullah, F.; Sepasgozar, S. Implementation of 360 videos and mobile laser measurement
technologies for immersive visualisation of estate & properties. In Proceedings of the 42nd AUBEA
Conference, Curtin, Australia, 26–28 December 2018.
37.Martinez-Mosquera, D.; Navarrete, R.; Lujan-Mora, S. Modeling and management big data in databases—A
systematic literature review.Sustainability2020,12, 634. [CrossRef]
38.
Zikopoulos, P.; Eaton, C.Understanding Big Data: Analytics for Enterprise Class Hadoop and Streaming Data;
McGraw-Hill Osborne Media: York, NY, USA, 2011.
39.Beakta, R. Big data and hadoop: A review paper.Int. J. Comput. Sci. Inf. Technol.2015,2, 13–15. Big Data Cogn. Comput.2020,4, 446 of 53
40.
Hashem, I.A.T.; Yaqoob, I.; Anuar, N.B.; Mokhtar, S.; Gani, A.; Khan, S.U. The rise of “big data” on cloud
computing: Review and open research issues.Inf. Syst.2015,47, 98–115. [CrossRef]
41.Seddon, J.J.; Currie, W.L. A model for unpacking big data analytics in high-frequency trading.J. Bus. Res.
2017,70, 300–307. [CrossRef]
42.Gandomi, A.; Haider, M. Beyond the hype: Big data concepts, methods, and analytics.Int. J. Inf. Manag.
2015,35, 137–144. [CrossRef]
43.Elgendy, N.; Elragal, A. Big data analytics: A literature review paper. In Proceedings of the Industrial
Conference on Data Mining, Hamburg, Germany, 15–19 July 2014.
44.Uddin, M.F.; Gupta, N. Seven V’s of big data understanding big data to extract value. In Proceedings of
the 2014 Zone 1 Conference of the American Society for Engineering Education, Bridgeport, CT, USA, 3–5
April 2014.
45.Hai, R.; Geisler, S.; Quix, C. Constance: An intelligent data lake system. In Proceedings of the 2016
International Conference on Management of Data, San Francisco, CA, USA, June 26–July 1 2016.
46.Yu, M.; Yang, C.; Li, Y. Big data in natural disaster management: A review.Geosciences2018,8, 165. [CrossRef]
47.Wang, J.; Zelenyuk, A.; Imre, D.; Mueller, K. Big data management with incremental K-means trees–GPU-accelerated
construction and visualization.Informatics2017,4, 24. [CrossRef]
48.Du, D.; Li, A.; Zhang, L. Survey on the applications of big data in Chinese estate enterprise.Procedia
Comput. Sci.2014,30, 24–33. [CrossRef]
49.Huang, Q.; Cervone, G.; Jing, D.; Chang, C. DisasterMapper: A CyberGIS framework for disaster management social media data. In Proceedings of the 4th International ACM SIGSPATIAL Workshop on Analytics
for Big Geospatial Data, Seattle, WA, USA, 3 November 2015.
50.Cheng, X.; Yuan, M.; Xu, L.; Zhang, T.; Jia, Y.; Cheng, C.; Chen, W. Big data assisted customer analysis
and advertising architecture for estate. In Proceedings of the 2016 16th International Symposium on
Communications and Information Technologies (ISCIT), Qingdao, China, 26–28 September 2016.
51.Barkham, R.; Bokhari, S.; Saiz, A.Urban Big Data: City Management and Estate Markets; GovLab Digest: York, NY, USA, 2018.
52.
Winson-Geideman, K.; Krause, A. Transformations in estate research: The big data revolution. In
Proceedings of the 22nd Annual Pacific-Rim Estate Society Conference, Queensland, Australia, 17–20
January 2016.
53.Lacuesta, R.; Garcia, L.; García-Magariño, I.; Lloret, J. System to recommend the best place to live on
wellness state of the employing the heart rate variability.IEEE Access2017,5, 10594–10604. [CrossRef]
54.Lee, S.; Byrne, P. The impact of portfolio size on the variability of the terminal wealth of estate funds.
Brief. Estate Financ. Int. J.2002,1, 319–330. [CrossRef]
55.Papadopoulos, T.; Gunasekaran, A.; Dubey, R.; Altay, N.; Childe, S.J.; Fosso-Wamba, S. The role of Big Data
in explaining disaster resilience in supply chains for sustainability.J. Clean. Prod.2017,142, 1108–1118.
[CrossRef]
56.
Ready, M.; Dwyer, T.; Haga, J.H. Immersive Visualisation of Big Data for River Disaster Management.
Available online: https://groups.inf.ed.ac.uk/vishub/immersiveanalytics/papers/IA_1538-paper.pdf (accessed
on 21 March 2020).
57.Ji-fan Ren, S.; Wamba, S.F.; Akter, S.; Dubey, R.; Childe, S.J. Modelling quality dynamics, business value and
firm performance in a big data analytics environment.Int. J. Prod. Res.2017,55, 5011–5026. [CrossRef]
58.Maroufkhani, P.; Wagner, R.; Ismail, W.K.W.; Baroto, M.B.; Nourani, M. Big data analytics and firm
performance: A systematic review.Information2019,10, 226. [CrossRef]
59.Pouyanfar, S.; Yang, Y.; Chen, S.-C.; Shyu, M.-L.; Iyengar, S. Multimedia big data analytics: A survey.ACM
Comput. Surv. CSUR2018,51, 1–34. [CrossRef]
60.Constantiou, I.D.; Kallinikos, J. games, rules: Big data and the changing context of strategy.J. Inf.
Technol.2015,30, 44–57. [CrossRef]
61.Gillon, K.; Aral, S.; Lin, C.-Y.; Mithas, S.; Zozulia, M. Business analytics: Radical shift or incremental change?
Commun. Assoc. Inf. Syst.2014,34, 13. [CrossRef]
62.Ge, M.; Dohnal, V. Quality management in big data.Informatics2018,5, 19. [CrossRef]
63.
Chen, H.; Chiang, R.H.; Storey, V.C. Business intelligence and analytics: From big data to big impact.MIS Q.
2012,36, 1165–1188. [CrossRef]
64.Liu, Y. Big data and predictive business analytics.J. Bus. Forecast.2014,33, 40. Big Data Cogn. Comput.2020,4, 447 of 53
65.
Khan, Z.; Vorley, T. Big data analytics: An enabler of knowledge management.J. Knowl. Manag.2017,21.
[CrossRef]
66.Jiang, J. Information extraction from . InMining Data; Springer: Berlin, Germany, 2012; . 11–41.
67.Piskorski, J.; Yangarber, R. Information extraction: Past, present and future. InMulti-Source, Multilingual
Information Extraction and Summarization; Springer: Berlin, Germany, 2013; . 23–49.
68.Gambhir, M.; Gupta, V. Recent automatic summarization techniques: A survey.Artif. Intell. Rev.2017,
47, 1–66. [CrossRef]
69.Alguliev, R.M.; Aliguliyev, R.M.; Isazade, N.R. Multiple documents summarization on evolutionary
optimization algorithm.Expert Syst. Appl.2013,40, 1675–1689. [CrossRef]
70.Ouyang, Y.; Li, W.; Zhang, R.; Li, S.; Lu, Q. A progressive sentence selection strategy for document
summarization.Inf. Process. Manag.2013,49, 213–221. [CrossRef]
71.Dragoni, M.; Tettamanzi, A.G.; da Costa Pereira, C. A fuzzy system for concept-level sentiment analysis. In
Semantic Web Evaluation Challenge; Springer: Berlin, Germany, 2014.
72.Xiang, Z.; Schwartz, Z.; Gerdes, J.H., Jr.; Uysal, M. What can big data and analytics tell us about hotel
guest experience and satisfaction?Int. J. Hosp. Manag.2015,44, 120–130. [CrossRef]
73.Jandl, J.-O. Information Processing and Stock Market Volatility-Evidence from Estate Investment Trusts.
Available online: https://aisel.aisnet.org/amcis2015/BizAnalytics/GeneralPresentations/42/(accessed on 21
March 2020).
74.Shirowzhan, S.; Sepasgozar, S.; Liu, C. Monitoring physical progress of indoor buildings mobile and
terrestrial point clouds. In Proceedings of the Construction Research Congress 2018, Orleans, LA, USA,
2–4 April 2018.
75.Shirowzhan, S.; Sepasgozar, S.M.E.; Li, H.; Trinder, J. Spatial compactness metrics and Constrained Voxel
Automata development for analyzing 3D densification and applying to point clouds: A synthetic review.
Autom. Constr.2018,96, 236–249. [CrossRef]
76.Shirowzhan, S.; Sepasgozar, S.M. Spatial analysis temporal point clouds in advanced GIS: Methods for
ground elevation extraction in slant areas and building classifications.ISPRS Int. J. Geo Inf.2019,8, 120.
[CrossRef]
77.Verma, J.P.; Agrawal, S.; Patel, B.; Patel, A. Big data analytics: Challenges and applications for , audio,
video, and social media data.Int. J. Soft Comput. Artif. Intell. Appl. IJSCAI2016,5, 41–51. [CrossRef]
78.Flake, G.W.; Gounares, A.G.; Gates, W.H.; Moss, K.A.; Dumais, S.T.; Naam, R.; Horvitz, E.J.; Goodman, J.T.
Auctioning for Video and Audio Advertising. U.S. Patent Application 11/427,316, 3 January 2008.
79.
Pratt, W. Method of Conducting Interactive Estate Property Viewing. U.S. Patent Application 10/898,661,
26 January 2006.
80.Emmanouil, D.; Nikolaos, D. Big Data Analytics in Prevention, Preparedness, Response and
Recovery in Crisis and Disaster Management. Available online: https://pdfs.semanticscholar.org/c1f1/
5011a85428ceeca788053a2e9daccc868ca2.pdf (accessed on 21 January 2020).
81.
Hampapur, A.; Bobbitt, R.; Brown, L.; Desimone, M.; Feris, R.; Kjeldsen, R.; Lu, M.; Mercier, C.; Milite, C.;
Russo, S. Video analytics in urban environments. In Proceedings of the 2009 Sixth IEEE International
Conference on Advanced Video and Signal Surveillance, Genova, Italy, 2–4 September 2009.
82.Lipton, A.J.; Clark, J.I.; Thompson, B.; Myers, G.; Titus, S.R.; Zhang, Z.; Venetianer, P.L. The intelligent vision
sensor: Turning video into information. In Proceedings of the 2007 IEEE Conference on Advanced Video
and Signal Surveillance, London, UK, 5–7 September 2007.
83.Stieglitz, S.; Dang-Xuan, L.; Bruns, A.; Neuberger, C. Social media analytics.Bus. Inf. Syst. Eng.2014,6,
89–96. [CrossRef]
84.
Su, X.; Sperlì, G.; Moscato, V.; Picariello, A.; Esposito, C.; Choi, C. An edge intelligence empowered
recommender system enabling cultural heritage applications.IEEE Trans. Ind. Inform.2019,15, 4266–4275.
[CrossRef]
85.Amato, F.; Moscato, V.; Picariello, A.; Sperli’ì, G. Extreme events management multimedia social
networks.Future Gener. Comput. Syst.2019,94, 444–452. [CrossRef]
86.
Peisenieks, J.; Skadins, R. Uses of Machine Translation in the Sentiment Analysis of Tweets. Available
online: https://www.researchgate.net/profile/Raivis_Skadis/publication/266220793_Uses_of_Machine_Translation_
in_the_Sentiment_Analysis_of_Tweets/links/542ab7eb0cf29bbc1268a7bb.pdf (accessed on 21 March 2020). Big Data Cogn. Comput.2020,4, 448 of 53
87.
Romanyshyn, M. Rule- sentiment analysis of ukrainian reviews.Int. J. Artif. Intell. Appl.2013,4, 103.
[CrossRef]
88.Pidduck, P.T.S.; Dent, M.J. System and Method for Searching on Blocks and Associated Search
Operators. U.S. Patent Application 15/911,412, 5 September 2019.
89.ArunaSafali, M.; Prasad, R.S.; Sastry, K.A. amalgamative sentiment analysis framework on social networking
site.J. Phys. Conf. Ser.2019,1228, 012010. [CrossRef]
90.Zhang, C.; Mao, B. Distributed processing practice of the 3D city model on HBase. In Proceedings of
the 2017 Fifth International Conference on Advanced Cloud and Big Data (CBD), Shanghai, China, 13–16
August 2017.
91.Wei-Ping, Z.; Ming-Xin, L.; Huan, C. MongoDB to implement textbook management system instead of
MySQL. In Proceedings of the 2011 IEEE 3rd International Conference on Communication Software and
Networks, Xi’an, China, 27–29 May 2011.
92.Chandrasekaran, K.; Marimuthu, C. Developing Software for Cloud: Opportunities and Challenges for
Developers. Available online: https://onlinelibrary.wiley.com/doi/10.1002/9781118821930.ch13 (accessed on
21 March 2020).
93.Jayagopal, V.; Basser, K. Data management and big data analytics: Data management in digital economy. In
Optimizing Big Data Management and Industrial Systems with Intelligent Techniques; IGI Global: Hershey, PA,
USA, 2019; . 1–23.
94.Gul, I. Exploring the Application Security Measures in Hive to SecureData in Column. PhD Thesis, Colorado
Technical University, Colorado Springs, CO, USA, 2019.
95.Lavanya, K.; Kashyap, R.; Anjana, S.; Thasneen, S. An Enhanced K-Means MSOINN clustering
over Neo4j with an application to weather analysis. InAlgorithms for Intelligent Systems; Springer: Berlin,
Germany, 2020.
96.Nargundkar, A.; Kulkarni, A.J. Big data in supply chain management and medicinal domain. InBig Data
Analytics in Healthcare; Springer: Berlin, Germany, 2020; . 45–54.
97.Kaya, T. Big data analytics for organizations: Challenges and opportunities and its effect on international
business education.Kurd. J. Appl. Res.2019,4, 137–150.
98.Venner, J.Pro Hadoop; Apress: York, NY, USA, 2009.
99.Octoparse. Yes, There Is Such Thing as a Free Web Scraper! Available online: https://www.octoparse.com/
blog/yes-there-is-such-thing-as-a-free-web-scraper (accessed on 3 February 2020).
100.DEORAS, S. 10 Best Data Cleaning Tools to Get the Most Out Of Your Data. Available online: https:
//analyticsindiamag.com/10-best-data-cleaning-tools-get-data (accessed on 3 February 2020).
101.Ullah, F.; Sepasgozar, S.M. A Study of Information Technology Adoption for -Estate Management: A
System Dynamic Model. Available online: https://www.worldscientific.com/doi/abs/10.1142/9789813272491_
0027 (accessed on 21 March 2020).
102.Amadio, W.J.; Haywood, M.E.Data Analytics and the Cash Collections Process: An Adaptable Case Employing Excel
and Tableau’. Advances in Accounting Education: Teaching and Curriculum Innovations (Advances in Accounting
Education, Volume 22); Emerald Publishing Limited: Bingley, UK, 2019; . 45–70.
103.Budiu, M.; Gopalan, P.; Suresh, L.; Wieder, U.; Kruiger, H.; Aguilera, M.K. Hillview: A trillion-cell spreadsheet
for big data.Proc. VLDB Endow.2019,12, 1442–1457. [CrossRef]
104.Stanˇcin, I.; Jovi ́c, A. An overview and comparison of free Python libraries for data mining and big data
analysis. In Proceedings of the 2019 42nd International Convention on Information and Communication
Technology, Electronics and Microelectronics (MIPRO), Opatija, Croatia, 20–24 May 2019.
105.Wieringa, M.; van Geenen, D.; van Es, K.; van Nuss, J. The Fieldnotes Plugin: Making Network Visualization in
Gephi Accountable (Chapter 16). Available online: https://eprints.qut.edu.au/125605/1/Good_Data_book.pdf
(accessed on 21 March 2020).
106.Anderson, D.R.; Sweeney, D.J.; Williams, T.A.; Camm, J.D.; Cochran, J.J.Modern Business Statistics with
Microsoft Excel; Cengage Learning: Boston, MA, USA, 2020.
107. Bhosale, H.S.; Gadekar, D.P. A review paper on big data and hadoop.Int. J. Sci. Res. Publ.2014,4, 1–7.
108.
DataFlair. Hadoop HDFS Architecture Explanation and Assumptions. Available online: https://data-flair.
training/blogs/hadoop-hdfs-architecture/(accessed on 3 February 2020). Big Data Cogn. Comput.2020,4, 449 of 53
109.
Zaharia, M.; Xin, R.S.; Wendell, P.; Das, T.; Armbrust, M.; Dave, A.; Meng, X.; Rosen, J.; Venkataraman, S.;
Franklin, M.J. Apache spark: A unified engine for big data processing.Commun. ACM2016,59, 56–65.
[CrossRef]
110.Mavridis, I.; Karatza, H. Performance evaluation of cloud- log file analysis with Apache Hadoop and
Apache Spark.J. Syst. Softw.2017,125, 133–151. [CrossRef]
111.-Jarrah, O.Y.; Yoo, P.D.; Muhaidat, S.; Karagiannidis, G.K.; Taha, K. Efficient machine learning for big data:
A review.Big Data Res.2015,2, 87–93. [CrossRef]
112.Saidulu, D.; Sasikala, R. Machine learning and statistical approaches for Big Data: Issues, challenges and
research directions.Int. J. Appl. Eng. Res.2017,12, 11691–11699.
113.Qiu, J.; Wu, Q.; Ding, G.; Xu, Y.; Feng, S. A survey of machine learning for big data processing.EURASIP J.
Adv. Signal Process.2016,2016, 67. [CrossRef]
114.Bengio, Y.; Courville, A.; Vincent, P. Representation learning: A review and perspectives.IEEE Trans.
Pattern Anal. Mach. Intell.2013,35, 1798–1828. [CrossRef] [PubMed]
115.Hinton, G.; Deng, L.; Yu, D.; Dahl, G.E.; Mohamed, A.-r.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.;
Sainath, T.N. Deep neural networks for acoustic modeling in speech recognition: The shared views of four
research groups.IEEE Signal Process. Mag.2012,29, 82–97. [CrossRef]
116.Parhami, B. Parallel Processing with Big Data. Available online: https://web.ece.ucsb.edu/~{}parhami/pubs_
folder/parh19b-ebdt-parallel-proc-big-data.pdf (accessed on 21 March 2020).
117.Chen, X.-W.; Lin, X. Big data deep learning: Challenges and perspectives.IEEE Access2014,2, 514–525.
[CrossRef]
118.Ding, S.; Xu, X.; Nie, R. Extreme learning machine and its applications.Neural Comput. Appl.2014,25,
549–556. [CrossRef]
119.
Wang, J.; Zhao, P.; Hoi, S.C.; Jin, R. Online feature selection and its applications.IEEE Trans. Knowl. Data Eng.
2013,26, 698–710. [CrossRef]
120.Wu, X.; Zhu, X.; Wu, G.-Q.; Ding, W. Data mining with big data.IEEE Trans. Knowl. Data Eng.2013,26,
97–107.
121.
Nie, F.; Wang, H.; Cai, X.; Huang, H.; Ding, C. Robust matrix completion via joint schatten p-norm and
lp-norm minimization. In Proceedings of the 2012 IEEE 12th International Conference on Data Mining,
Brussels, Belgium, 10–13 December 2012.
122.Meier, P. Human computation for disaster response. InHandbook of Human Computation; Springer: Berlin,
Germany, 2013; . 95–104.
123. Fan, J.; Han, F.; Liu, H. Challenges of big data analysis.Natl. Sci. Rev.2014,1, 293–314. [CrossRef]
124.
Katal, A.; Wazid, M.; Goudar, R. Big data: Issues, challenges, tools and good practices. In Proceedings of the
2013 Sixth International Conference on Contemporary Computing (IC3), Noida, India, 8–10 August 2013.
125. Jaseena, K.; David, J.M. Issues, challenges, and solutions: Big data mining.CS & IT-CSCP2014,4, 131–140.
126.Kasavajhala, V. Solid State Drive vs.Hard Disk Drive Price and Performance Study.Available
online: https://www.dell.com/downloads/global/products/pvaul/en/ssd_vs_hdd_price_and_performance_
study.pdf (accessed on 21 March 2020).
127.Munawar, H.S.; Awan, A.A.A.; Khalid, U.; Munawar, S.; Maqsood, A. Revolutionizing telemedicine by
Instilling H. 265.Int. J. Image Graphics Signal Process.2017,9, 20–27. [CrossRef]
128.Raghupathi, W.; Raghupathi, V. Big data analytics in healthcare: Promise and potential.Health Inf. Sci. Syst.
2014,2, 3. [CrossRef]
129.He, K.Y.; Ge, D.; He, M.M. Big data analytics for genomic medicine.Int. J. Mol. Sci.2017,18, 412. [CrossRef]
130.Chen, M.; Hao, Y.; Hwang, K.; Wang, L.; Wang, L. Disease prediction by machine learning over big data from
healthcare communities.IEEE Access2017,5, 8869–8879. [CrossRef]
131.Iniesta, R.; Stahl, D.; McGuffin, P. Machine learning, statistical learning and the future of biological research
in psychiatry.Psychol. Med.2016,46, 2455–2465. [CrossRef]
132.Obermeyer, Z.; Emanuel, E.J. Predicting the future—Big data, machine learning, and clinical medicine.N.
Engl. J. Med.2016,375, 1216. [CrossRef]
133.
Wolfert, S.; Ge, L.; Verdouw, C.; Bogaardt, M.-J. Big data in smart farming–a review.Agric. Syst.2017,153,
69–80. [CrossRef]
134.Singh, S.; Kaur, S.; Kumar, P. Forecasting soil moisture on evaluation of time series analysis. InAdvances
in Power and Control Engineering; Springer: Berlin, Germany, 2020; . 145–156. Big Data Cogn. Comput.2020,4, 450 of 53
135.
Faulkner, A.; Cebul, K.; McHenry, G. Agriculture Gets Smart: The Rise of Data and Robotics. Available online:
https://www.cleantech.com/wp-content/uploads/2014/07/Agriculture-Gets-Smart-Report.pdf (accessed on 21
March 2020).
136.Chandramohan, A.M.; Mylaraswamy, D.; Xu, B.; Dietrich, P. Big data infrastructure for aviation data analytics.
In Proceedings of the 2014 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),
Bangalore, India, 15–17 October 2014.
137.Shaikh, F.; Rangrez, F.; Khan, A.; Shaikh, U. Social media analytics on big data. In Proceedings of
the 2017 International Conference on Intelligent Computing and Control (I2C2), Coimbatore, India, 23–24
June 2017.
138.Sharmin, S.; Zaman, Z. Spam detection in social media employing machine learning tool for mining.
In Proceedings of the 2017 13th International Conference on Signal-Image Technology & Internet-
Systems (SITIS), Jaipur, India, 4–7 December 2017.
139.Yadav, S.; Thakur, S. Bank loan analysis customer usage data: A big data Hadoop. In
Proceedings of the 2017 2nd International Conference on Telecommunication and Networks (TEL-NET),
Noida, India, 10–11 August 2017.
140.Shirowzhan, S.; Sepasgozar, S.M.E.; Edwards, D.J.; Li, H.; Wang, C. BIM compatibility and its differentiation
with interoperability challenges as an innovation factor.Autom. Constr.2020,112, 103086. [CrossRef]
141.Sepasgozar, S.M.; Hawken, S.; Sargolzaei, S.; Foroozanfa, M. Implementing citizen centric technology in
developing smart cities: A model for predicting the acceptance of urban technologies.Technol. Forecast.
Social Chang.2019,142, 105–116. [CrossRef]
142.Sepasgozar, S.M.; Davis, S.R.; Li, H.; Luo, X. Modeling the implementation process for construction
technologies: Thematic analysis on Australian and US practices.J. Manag. Eng.2018,34, 05018005.
[CrossRef]
143.Sepasgozar, S.M.; Davis, S. Digital construction technology and job-site equipment demonstration: Modelling
relationship strategies for technology adoption.Buildings2019,9, 158. [CrossRef]
144.Sepasgozar, S.; Shirowzhan, S.; Wang, C.C. A Scanner technology acceptance model for construction projects.
Procedia Eng.2017,180, 1237–1246. [CrossRef]
145.Suthaharan, S. Big data classification: Problems and challenges in network intrusion prediction with machine
learning.ACM SIGMETRICS Perform. Eval. Rev.2014,41, 70–73. [CrossRef]
146.Srai, J.S.; Kumar, M.; Graham, G.; Phillips, W.; Tooze, J.; Ford, S.; Beecher, P.; Raj, B.; Gregory, M.; Tiwari, M.K.
Distributed manufacturing: Scope, challenges and opportunities.Int. J. Prod. Res.2016,54, 6917–6935.
[CrossRef]
147.Wang, J.; Zhang, J. Big data analytics for forecasting cycle time in semiconductor wafer fabrication system.
Int. J. Prod. Res.2016,54, 7231–7244. [CrossRef]
148.Chen, D.Q.; Preston, D.S.; Swink, M. How the of big data analytics affects value creation in supply chain
management.J. Manag. Inf. Syst.2015,32, 4–39. [CrossRef]
149.
Hazen, B.T.; Boone, C.A.; Ezell, J.D.; Jones-Farmer, L.A. Data quality for data science, predictive analytics,
and big data in supply chain management: An introduction to the problem and suggestions for research and
applications.Int. J. Prod. Econ.2014,154, 72–80. [CrossRef]
150.Zhou, Z.; Dou, W.; Jia, G.; Hu, C.; Xu, X.; Wu, X.; Pan, J. A method for -time trajectory monitoring to
improve taxi service GPS big data.Inf. Manag.2016,53, 964–977. [CrossRef]
151.Matthias, O.; Fouweather, I.; Gregory, I.; Vernon, A. Making sense of big data–can it transform operations
management?Int. J. Oper. Prod. Manag.2017,31, 37–55. [CrossRef]
152.Moeyersoms, J.; Martens, D. Including high-cardinality attributes in predictive models: A case study in
churn prediction in the energy sector.Decis. Support Syst.2015,72, 72–81. [CrossRef]
153.Ullah, F.; Samad, M.S.; Siddiqui, S. An investigation of estate technology utilization in technologically
advanced marketplace. In Proceedings of the 9th International Civil Engineering Congress (ICEC-2017),
“Striving Towards Resilient Built Environment”, Karachi, Pakistan, 22–23 December 2017.
154.Ullah, F.; Shinetogtokh, T.; Sepasgozar, P.S.; Ali, T.H. Investigation of the users’ interaction with online estate platforms in Australia. In Proceedings of the 2nd International Conference on Sustainable
Development in Civil Engineering (ICSDC 2019), Jamshoro, Pakistan, 5–7 December 2019.
155.Kolomvatsos, K.; Anagnostopoulos, C. Reinforcement learning for predictive analytics in smart cities.
Informatics2017,4, 16. [CrossRef] Big Data Cogn. Comput.2020,4, 451 of 53
156.
NextGenLivingHomes. The Bitcoin House. Available online: https://nextgenlivinghomes.com/download-
the-bitcoin-house-brochure-the-first-income-generating-home-in-the-world/(accessed on 1 March 2020).
157.Kok, N.; Koponen, E.-L.; Martínez-Barbosa, C.A. Big data in estate? From manual appraisal to automated
valuation.J. Portf. Manag.2017,43, 202–211. [CrossRef]
158.Joseph, G.; Varghese, V. Analyzing Airbnb customer experience feedback mining. InBig Data and
Innovation in Tourism, Travel, and Hospitality; Springer: Berlin, Germany, 2019; . 147–162.
159. CoreLogic. Available online: https://www.corelogic.com.au/(accessed on 1 March 2020).
160.Archibus. Automate Preventive Upkeep|With Building Operations Tools. Available online: https://archibus.
com/products/building-operations (accessed on 1 March 2020).
161.Ju, J.; Liu, L.; Feng, Y. Citizen-centered big data analysis-driven governance intelligence framework for smart
cities.Telecommun. Policy2018,42, 881–896. [CrossRef]
162.Boakye, J.; Gardoni, P.; Murphy, C. opportunities in big data analytics to more accurately predict
societal consequences of natural disasters.Civ. Eng. Environ. Syst.2019,36, 100–114. [CrossRef]
163.Marr, B. How Much Data Do We Create Every Day? The Mind-Blowing Stats Everyone Should Read. Available
online: https://www.forbes.com/sites/bernardmarr/2018/05/21/how-much-data-do-we-create-every-day-
the-mind-blowing-stats-everyone-should-read/#40f31fec60ba (accessed on 3 March 2020).
164.Gurman, T.A.; Ellenberger, N. Reaching the global community during disasters: Findings from a content
analysis of the organizational of Twitter after the 2010 Haiti earthquake.J. Health Commun.2015,20,
687–696. [CrossRef] [PubMed]
165.Tapia, A.H.; Moore, K.A.; Johnson, N.J. Beyond the trustworthy tweet: A deeper understanding of
microblogged data by disaster response and humanitarian relief organizations. In Proceedings of the
ISCRAM, Baden-Baden, Germany, 12–15 May 2013.
166.
Arslan, M.; Roxin, A.-M.; Cruz, C.; Ginhac, D. A review on applications of big data for disaster management.
In Proceedings of the 2017 13th International Conference on Signal-Image Technology & Internet-
Systems (SITIS), Jaipur, India, 4–7 December 2017.
167.Liu, C.; Shirowzhan, S.; Sepasgozar, S.M.; Kaboli, A. Evaluation of classical operators and fuzzy logic
algorithms for edge detection of panels at exterior cladding of buildings.Buildings2019,9, 40. [CrossRef]
168.Munawar, H.S.; Zhang, J.; Li, H.; Mo, D.; Chang, L. Mining multispectral aerial images for automatic
detection of strategic bridge locations for disaster relief missions. InPacific-Asia Conference on Knowledge
Discovery and Data Mining; Springer: Berlin, Germany, 2019.
169.Munawar, H.S.; Maqsood, A.; Mustansar, Z. Isotropic surround suppression and Hough transform target recognition from aerial images.Int. J. Adv. Appl. Sci.2017,4, 37–42. [CrossRef]
170.Ghurye, J.; Krings, G.; Frias-Martinez, V. A framework to model human behavior at large scale during natural
disasters. In Proceedings of the 2016 17th IEEE International Conference on Mobile Data Management
(MDM), Porto, Portugal, 13–16 June 2016.
171.Zhang, J.A.; Nolan, D.S.; Rogers, R.F.; Tallapragada, V. Evaluating the impact of improvements in the
boundary layer parameterization on hurricane intensity and structure forecasts in HWRF.Mon. Weather Rev.
2015,143, 3136–3155. [CrossRef]
172.Yablonsky, R.M.; Ginis, I.; Thomas, B. Ocean modeling with flexible initialization for improved coupled
tropical cyclone-ocean model prediction.Environ. Model. Softw.2015,67, 26–30. [CrossRef]
173.Zhang, J.A.; Marks, F.D.; Montgomery, M.T.; Lorsolo, S. An estimation of turbulent characteristics in the
low-level region of intense Hurricanes Allen (1980) and Hugo (1989).Mon. Weather Rev.2010,139, 1447–1462.
[CrossRef]
174.Bisson, M.; Spinetti, C.; Neri, M.; Bonforte, A. Mt. Etna volcano high-resolution topography: Airborne
LiDAR modelling validated by GPS data.Int. J. Digit. Earth2016,9, 710–732. [CrossRef]
175.Nomikou, P.; Parks, M.; Papanikolaou, D.; Pyle, D.; Mather, T.; Carey, S.; Watts, A.; Paulatto, M.; Kalnins, M.;
Livanos, I. The emergence and growth of a submarine volcano: The Kameni islands, Santorini (Greece).
GeoResJ2014,1, 8–18. [CrossRef]
176.Nonnecke, B.M.; Mohanty, S.; Lee, A.; Lee, J.; Beckman, S.; Mi, J.; Krishnan, S.; Roxas, R.E.; Oco, N.;
Crittenden, C. Malasakit 1.0: A participatory online platform for crowdsourcing disaster risk reduction
strategies in the philippines. In Proceedings of the 2017 IEEE Global Humanitarian Technology Conference
(GHTC), San Jose, CA, USA, 19–22 October 2017. Big Data Cogn. Comput.2020,4, 452 of 53
177.
Poslad, S.; Middleton, S.E.; Chaves, F.; Tao, R.; Necmioglu, O.; Bügel, U. A semantic IoT early warning system
for natural environment crisis management.IEEE Trans. Emerg. Top. Comput.2015,3, 246–257. [CrossRef]
178.Di Felice, M.; Trotta, A.; Bedogni, L.; Chowdhury, K.R.; Bononi, L. Self-organizing aerial mesh networks
for emergency communication. In Proceedings of the 2014 IEEE 25th Annual International Symposium on
Personal, Indoor, and Mobile Radio Communication (PIMRC), Washington, DC, USA, 2–5 September 2014.
179.Mosterman, P.J.; Sanabria, D.E.; Bilgin, E.; Zhang, K.; Zander, J. A heterogeneous fleet of vehicles for
automated humanitarian missions.Comput. Sci. Eng.2014,16, 90–95. [CrossRef]
180.Lu, Z.; Cao, G.; La Porta, T. Networking smartphones for disaster recovery. In Proceedings of the 2016 IEEE
International Conference on Pervasive Computing and Communications (PerCom), Sydney, Australia, 14–19
March 2016.
181.de Alwis Pitts, D.A.; So, E. Enhanced change detection index for disaster response, recovery assessment
and monitoring of accessibility and open spaces (camp sites).Int. J. Appl. Earth Obs. Geoinf.2017,57, 49–60.
[CrossRef]
182.Contreras, D.; Forino, G.; Blaschke, T. Measuring the progress of a recovery process after an earthquake: The
case of L’aquila, Italy.Int. J. Disaster Risk Reduct.2018,28, 450–464. [CrossRef]
183.Kahn, M.E. The death toll from natural disasters: The role of income, geography, and institutions.Rev. Econ.
Stat.2005,87, 271–284. [CrossRef]
184.Goh, T.T.; Sun, P.-C. Eaching social media analytics: An assessment on natural disaster postings.J. Inf.
Syst. Educ.2015,26, 27.
185.Grinberger, A.Y.; Lichter, M.; Felsenstein, D. Dynamic agent simulation of an urban disaster synthetic big data. InSeeing Cities Through Big Data; Springer: Berlin, Germany, 2017; . 349–382.
186.Lv, Z.; Li, X.; Choo, K.-K.R. E-government multimedia big data platform for disaster management.Multimed.
Tools Appl.2018,77, 10077–10089. [CrossRef]
187.Hashem, I.A.T.; Chang, V.; Anuar, N.B.; Adewole, K.; Yaqoob, I.; Gani, A.; Ahmed, E.; Chiroma, H. The role
of big data in smart city.Int. J. Inf. Manag.2016,36, 748–758. [CrossRef]
188.Deal, B.; Pan, H.; Pallathucheril, V.; Fulton, G. Urban resilience and planning support systems: The need for
sentience.J. Urban Technol.2017,24, 29–45. [CrossRef]
189.Kontokosta, C.E.; Malik, A. The Resilience to Emergencies and Disasters Index: Applying big data to
benchmark and validate neighborhood resilience capacity.Sustain. Cities Soc.2018,36, 272–285. [CrossRef]
190.Klein, B.; Koenig, R.; Schmitt, G. Managing urban resilience.Informatik-Spektrum2017,40, 35–45. [CrossRef]
191.Sepasgozar, S.M.; Forsythe, P.; Shirowzhan, S. Evaluation of terrestrial and mobile scanner technologies for
part-built information modeling.J. Constr. Eng. Manag.2018,144, 04018110. [CrossRef]
192.Sepasgozar, S.; Lim, S.; Shirowzhan, S.; Kim, Y.; Nadoushani, Z.M. Utilisation of a Terrestrial Scanner
for Reconstruction of As-built Models: A Comparative Study. In Proceedings of the ISARC, International
Symposium on Automation and Robotics in Construction, Oulu, Finland, 15–18 June 2015.
193.Sepasgozar, S.M.; Wang, C.; Shirowzhan, S. Challenges and opportunities for implementation of laser
scanners in building construction. In Proceedings of the 33rd International Symposium on Automation and
Robotics in Construction (ISARC 2016), Auburn, , USA, 18–21 July 2016.
194.Sepasgozar, S.M.; Forsythe, P.; Shirowzhan, S.; Norzahari, F. Scanners and photography: A combined
framework. In Proceedings of the 40th Australasian Universities Building Education Association (AUBEA)
2016 Conference, Cairns, Australia, 6–8 July 2016.
195.Li, H.; Chan, G.; Wong, J.K.W.; Skitmore, M. -time locating systems applications in construction.Autom.
Constr.2016,63, 37–47. [CrossRef]
196.Shirowzhan, S.; Sepasgozar, S.M.E.; Zaini, I.; Wang, C. An integrated GIS and Wi-Fi Locating system
for improving construction labor communications. In Proceedings of the 34th International Symposium on
Automation and Robotics in Construction and Mining, Taipei, Taiwan, 28 June–1 July 2017.
197.Shirowzhan, S.; Lim, S.; Trinder, J.; Li, H.; Sepasgozar, S.M.E. Data mining for recognition of spatial
distribution patterns of building heights airborne lidar data.Adv. Eng. Inform.2020,43, 101033.
[CrossRef]
198. Shirowzhan, S.; Trinder, J.; Osmond, P. metrics for spatial and temporal 3D Urban form sustainability
assessment time series lidar point clouds and advanced GIS techniques. InUrban Design; IntechOpen:
London, UK, 2019. Big Data Cogn. Comput.2020,4, 453 of 53
199.
Shirowzhan, S.; Trinder, J. Building classification from lidar data for spatio-temporal assessment of 3D urban
developments.Procedia Eng.2017,180, 1453–1461. [CrossRef]
©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/). TYPEReview
PUBLISHED19 August 2022
DOI10.3389/fdata.2022.945720
OPEN EDITED BY
Huan Liu,
Arizona State University, United States
REVIEWED BY
Christoph Quix,
Fraunhofer-Institut für Angewandte
Informationstechnik (FIT), Germany
Jakob Blomer,
European Organization for Nuclear
Research (CERN), Switzerland
Darja Solodovnikova,
University of Latvia, Latvia
*CORRESPONDENCE
Philipp Wieder
philipp.wieder@gwdg.de
SPECIALTY This article was submitted to
Data Mining and Management,
a of the journal
Frontiers in Big Data
RECEIVED16 May 2022
ACCEPTED29 July 2022
PUBLISHED19 August 2022
CITATION
Wieder P and Nolte H (2022) Toward
data lakes as central building blocks
for data management and analysis.
Front. Big Data5:945720.
doi: 10.3389/fdata.2022.945720
COPYRIGHT
©2022 Wieder and Nolte. This is an
open- article distributed under
the terms of the
Creative Commons
Attribution License (CC BY)
. The ,
distribution or reproduction in other
forums is permitted, provided the
original author(s) and the copyright
owner(s) are credited and that the
original publication in this journal is
cited, in accordance with accepted
academic practice. No , distribution
or reproduction is permitted which
does not comply with these terms.
Toward data lakes as central
building blocks for data
management and analysis
Philipp Wieder
*
and Hendrik Nolte
Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), Göttingen, Germany
Data lakes are a fundamental building block for many industrialdata analysis
solutions and becoming increasingly popular in research. Oftenassociated
with big data cases, data lakes are, for example, as centraldata
management systems of research institutions or as the core entity of machine
learning pipelines. The basic underlying idea of retaining data in its native
format within a data lake facilitates a large range of cases andimproves
data reusability, especially when compared to the schema-on-write applied in data warehouses, where data is transformed prior to the actual
storage to fit a predefined schema. Storing such massive amounts ofraw
data, however, has its very own challenges, spanning from the general data
modeling, and indexing for concise querying to the integration of suitable
and scalable compute capabilities. In this contribution, influential papers of
the last decade have been selected to provide a comprehensive overview of
developments and obtained results. The papers are analyzed withregard to the
applicability of their input to data lakes that serve as central data management
systems of research institutions. To achieve this, contributions to data lake
architectures, metadata models, data provenance, workflow support, and FAIR
principles are investigated. Last, but not least, these capabilities are mapped
onto the requirements of two common research personae to identifyopen
challenges. With that, potential research topics are determined, which have to
be tackled toward the applicability of data lakes as central building blocks for
research data management.
KEYWORDS
data lake, data analytics, research data management, provenance, big data, FAIR
1. Introduction
In recent years, data lakes have become increasingly popular in various industrial
and academic domains. In particular for academia, data lakes come with the promise
to provide solutions for several data management challenges at once. SimilartoData
Warehouses
(Devlin and Murphy, 1988; Inmon, 2005), data lakes aim at integrating
heterogeneous data from different sources into a single, homogeneous data management
system. This allows data holders to overcome the limits of disparate and isolated data
silos and enforce uniform data governance.
FrontiersinBigData01frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
Data Warehouses have a fixed schema, which implies a so-
calledschema-on-writeapproach to feed data into them.Extract-
Transform-Load(ETL) processes are therefore needed to extract
the raw data from its source, transform, e.g., to clean it or to
fit it into the predefined schema, and then load it into the Data
Warehouse
(El-Sappagh ., 2011). Although there are some
known challenges when these ETL processes
(Munappy ., 2020)
, the main drawback is the loss of information
during the transformation to fit data into the fixed schema.
To prevent this information loss, which limits the reuse of
the data e.g., for research questions outside the original scope,
James Dixon proposed the data lake concept in
Dixon (2010).
Here, in contrast to the schema-on-write of a Data
Warehouses, data is retained in its original format and a schema
is only inferred when a subsequent process reads the data, an which is termedschema-on-read.
The necessity for low cost and highly scalable mass storage
with the ability to be integrated into parallelised computations
was recognized as a key feature already at the advent of data
lakes, leading to a close connection between a data lakes and
Apache Hadoop
(Khine and Wang, 2018). This was
at some point challenged by large cloud providers Amazon
or Microsoft and their proprietary data lake solutions AWS Lake FormationorAzure Data Lake
(Hukkeri .,
2020; Aundhkar and Guja, 2021)
. These products introduced,
among other features, the separation of storage and compute
and offered customers the well-known cloud features the
pay-as-you-go payment model.
Although a data lake implements a schema-on-read
semantic, some modeling is mandatory to ensure proper data
integration, comprehensibility, and quality
(Hai ., 2018).
Such data modeling typically consists of a conceptual model,
which should facilitate frequent changes and therefore should
not enforce a fixed schema
(Mathis, 2017; Khine and Wang,
2018)
. The required metadata can be gathered by extracting
prescriptive information from the data itself, for instance by
reading out header information, or metadata can be additionally
extracted from the source along with the original raw data itself.
In addition, data can be continuously enriched with metadata
during its lifetime in the data lake, for instance by identifying
relationships among the different data sets
(Hai ., 2016;
Sawadogo ., 2019)
or by auditing provenance information.
Quite some literature exists related to the of data lakes
in different industries
(Terrizzano ., 2015; Golec, 2019;
Hukkeri ., 2020)
, particularly with the intent to manage big
amounts of data
(Miloslavskaya and Tolstoy, 2016). However,
there is also huge potential for the adoption of data lakes in
research institutions. One benefit is, for example, that data
silos, which quickly arise when different research teams work
independently, can be prevented or integrated. This also enables
novel analysis approaches across an homogeneous data set,
which are not possible dealing with distributed and isolated data
silos. Another advantage is that a common data governance
can be enforced on an overarching level, an institute or
a research project, to guarantee a predefined data quality level
while and to assist researchers to adhere to good scientific
practices.
When scaling out the usage of a data lake across an entire
research institution to be the central research data management
system, one is encountered with different cases and users
who have a diverse skill set. In this paper we want to explore
the current state of the art of data lakes and make on
this survey an applicability analysis of the presented works
in the context of a large research institution. For this, papers
were collected, which had unique contributions to at least one
of the topics presented below. We start in 2 with a
discussion about the existing Data Lake Architectures, which
offers an overview about the highest level of organization and
abstraction of a given implementation. In the following 3
different metadata models are presented, which lie conceptually
one layer below the general architecture, and ensure the correct
data organization in the data lake, which involves semantic
information about the data itself as well as metadata describing
the relationships among the data. One if not the most important
relationship which needs to be modeled in a data lake is the
data lineage, which is in detail discussed in 4. Closely
related to the topic of provenance auditing is the general ability
to perform automated data analytics workflows, ideally in a
scalable manner, on the data lake, which is discussed in the
following 5. In 6.2 two disparate data lake
users, i.e., a domain researcher and a data scientist, are to
perform an applicability analysis of the before presented works.
In addition a comparison on common but also topic-
specific criteria is done to extend on the generic applicability
analysis. on the general survey in each of these topics,
future challenges are identified.
2. Data lake architectures
As of today, a lot of development and analysis was conducted
in the area ofdata lake architectures, where the so-calledzone
architecture
(Patel ., 2017; Ravat and Zhao, 2019), including
thepond architecture
(Inmon, 2016), became the most cited
and . These architectures have already been surveyed by
Hai . (2021)andSawadogo and Darmont (2021), and a
functionalarchitecture was proposed by both of them, and
amaturityand ahybridarchitecture have been derived by
Sawadogo and Darmont (2021). These surveys, however, did
not include recent works the definition of a zone reference
model
(Giebler ., 2020)or a data lake architecture on
FAIR Digital Objects(FDOs)
(Nolte and Wieder, 2022).
FrontiersinBigData02frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
2.1. Definition of the term data lake
architecture
The term data lake architecture was defined by
Giebler . (2021)
to represent the comprehensive design of a
data lake, including the infrastructure, data storage, data flow,
data modeling, data organization, data processes, metadata
management, data security and privacy, and data quality. In
this data lake architecture framework, only the data security
and privacy and the data quality are considered to be purely
conceptual, whereas the other aspects include a conceptual and
a physical, i.e., system specific, dimension. As stated by
Madera
and Laurent (2016)
more generically, a data lake generally has a
logical and a physical organization. In this paper, we refer to the
termdata lake architectureonly with respect to the conceptual
organization of a data lake in the highest level of abstraction,
since this should make this work more comparable to the
existing literature, although there exists a strong dependency on
other aspects of a data lake, the metadata modeling.
2.2. Zone architecture
The general idea to divide a data lake into different zones
arises from the necessity to automatically run standardized
pre-processing pipelines, organize the resulting pre-processed
data, and make it available to subsequent processing steps, reporting,Online Analytical Processing(OLAP), and
particularly advanced analytics. This is achieved by assigning
data to different zones on the degree of processing, and
sometimes the intended future case. Therefore, it is common
to have araw data zone, where, according to the original idea
of a data lake, data is retained in its raw format to facilitate
repetition of processes or the application of methods on the original data. Pre-processed data is then usually collected
in a dedicated zone forpre-processed, orrefineddata, sometimes
calledstaging zone
(Zikopoulos, 2015)orprocessing zone(Ravat
and Zhao, 2019)
. Data that requires additional governance can
be collected in a dedicated zone of its own calledtrusted zone
(Zikopoulos, 2015), orsensitive zone(Gorelik, 2019).
The most extensive analysis of thezone architecturewas
conducted by
Giebler . (2020), where five different data lakes on thezone architecture
(Madsen, 2015; Zikopoulos, 2015;
Patel ., 2017; Sharma, 2018; Gorelik, 2019; Ravat and Zhao,
2019)
were analyzed with respect to their design differences,
specific features and their individual cases in order to derive
a generic meta-model for a zone, and to specify a zone reference
model on it. Giebler . identified that a zone is uniquely
defined by the characteristics of the data contained in that zone,
the intrinsic properties a zone enforces on the data, the groups which are intended to work in that zone, the modeling to organize the corresponding metadata, and the data
sources as well as destinations. In the presented zone reference
model, Giebler . propose to split the zones up in araw
zoneand aharmonized zone, which is case independent,
and a case-specificdistilled zone, which serves data to the
finaldelivery zone, to support reporting and OLAP tasks, and a
explorative zoneto support advanced analytics. Each zone hosts
aprotectedarea for data that requires special governance. The
actual implementation and the deployed systems can vary in
each zone, including the storage, the metadata model, and the
metadata management system itself. This entails synchronously,
that also the interface potentially changes with each zone.
2.3. Lambda architecture
TheLambda Architecturehas been proposed to enhance the
capability of a data lake to process data streams in near -
time instead of fully ingesting hot data into the data lake and
performing batch-processing with a certain time delay
(Mathis,
2017)
. However, retaining all raw data in its native format is the
core idea of a data lake. In order to resolve this contradiction, the
Lambda Architecture
(Warren and Marz, 2015)implements two
processing streams in parallel. Here, data is being processed in
near -time in thespeed layer, whereas thebatch layeringests
data into the data lake and performs some predefined processing
steps. There have been numerous implementations proposed for
a data lake utilizing the Lambda Architecture
(Hasani .,
2014; Villari ., 2014; Batyuk and Voityshyn, 2016)
. However,
the following two particular works are presented which are
building on top of public cloud offerings.
A Lambda Architecture was by
Munshi and Mohamed
(2018)
to build a data lake for smart grid data analytics Google’s cloud computing as Infrastructure as a Service (IaaS).
Here, the data is collected by a dedicatedData Collecting
Layer, in this particular case realized by ApacheFlume
1
. From
there, the data is sent to the core of this specific data lake
implementation, a Hadoop Cluster. The master node stores
the data on HDFS
(Borthakur, 2007), and computes arbitrary,
predefined functions MapReduce
(Dean and Ghemawat,
2008)
. The speed layer is implemented usingApache Spark
(Zaharia ., 2010). The serving layer combines the output of
the batch and the speed layer and provides a batch view of the
relevant data, e.g.,Hive
(Thusoo ., 2009),Impalaas
shown by
Li (2014), andSpark SQL(Armbrust ., 2015).
Similarly,
Pérez-Arteaga . (2018)compared three
different implementations the Software as a Service (SaaS)
offerings with a focus on serverless delivery of three different
public cloud providers, i.e., Google Cloud Platform, Microsoft
Azure, and Amazon Web Services Cloud. On AWS the speed
layer accepts dataviaKinesis Data streams and processes them Kinesis Analytics and AWS Lambda. The results are stored
1https://flume.apache.org/.
Frontiersin
BigData03frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
in a dedicatedS3-Speed-Bucket. Similarly, the batch layer uses
Kinesis Firehose to ingest data into AWS Lambda, from where
it is stored in anS3-Batch-Bucket. From here the data is read
by AWS Glue and stored in anS3-Result-Bucket. The serving
layer is realized by Athena which reads the data from both, the
S3-Result-Bucketand theS3-Speed-Bucket. In the Google Cloud
implementation, data is ingested by Pub/Sub to the speed and
the batch layer, which are both realized Dataflow. On the
batch layer, an additional Datastore is employed to retain the
raw incoming datasets. The serving layer uses BigQuery. On
Microsoft Azure, data is received and ingested by EventHub.
The speed layer uses Stream Analytics and forwards directly into
the Serving Layer which is Cosmos DB. The batch layer also
uses Stream Analytics to store the raw data into Data Lake Store
(Ramakrishnan ., 2017). From there it is read by Data lake
Analytics, which also stores its results in Cosmos DB.
2.4. Lakehouse
Lakehousesas described by
Armbrust . (2021)are a
consequence of the general observation that in some cases the
raw data from a data lake is as an input for an ETL process
to populate a data warehouse. The first step into a more unified
setup was provided byDelta lakes
(Armbrust ., 2020), which
provides ACID (atomicity, consistency, isolation, durability)
transactions on cloud object storage for tables stores. These
tables can be accessed from different systems, Spark, Hive,
Presto
(Sethi ., 2019), and others. This introduces,
among other things, the advantage of still separating storage
and compute. Lakehouses offer on top of ACID transactions
direct to the storage with traditional database semantics,
e.g., SQL, open file formats likeApache Parquet
(Vohra,
2016)
orORC
2
. Therefore, a metadata layer on top of the cloud
storage can provide convenient SQL- to tables, while
compute-intensive, non-SQL code, machine learning, can
directly the files on the storage devices and thereby get
higher performance.
2.5. FAIR digital object-
architecture aFAIR Digital Object- architecture, as proposed
by
Nolte and Wieder (2022), the data lake is not divided into
different zones but realizes a flat, homogeneous, and uniform
research data management from the ’s point of view. To
allow for segregation of data with a different pedigree of
subjected processing, theFAIR Digital Objectencapsulating
the corresponding data has a certain type with which the
delimitation between different data points is represented. This
2https://orc.apache.org/.
can mean in a simple example, that in practice there is a
Scanner−X
Raw
data type and aScanner−X
Preprocessed
data
type. This leads to a much more fine-grained partition of the data
lake as compared to thezone-architecture. This highly segregated
data lake, however, does not entail a correlated increase in
system complexity and administrative effort, since only one,
or a necessary subset of types of theFAIR Digital Objects
needs to be implemented and can then be further inherited
from, hereby reusing the existing implementation on the infrastructure. Since everything in this data lake is aFAIR Digital
Object, not only data but also including workflows and execution
environments, the interface is completely homogeneous,
since the interacts with these objects by calling predefined
functions. Each of these data objects is equivalently suited
as input for automated workflows or -defined advanced
analytics. The requirement for additional governance or security
measures can be defined on a per object basis and can be globally
enforced on the typed attributes describing the metadata
of theFAIR Digital Object.
2.6. Functional and maturity-
architectures
The classification intofunctionalandmaturity-oriented
data lake architectures do, unlike in the case of the zone,
lambda, lakehouse, and FAIR Digital Object- architectures,
not represent yet another design concept, but rather serve
as an improved way for classifying the different architectural
approaches. The goal is to allow for a more modular comparison
of existing data lake solutions and to better plan, the data life-
cycle as well as to help match the individual functionality of the
architectural pieces, which are building up the data lake, zones, or objects, on the required infrastructure.
Within afunctional- architecture classification, the
data lake is analyzed toward its operations which are performed
on the data while moving through the general data lake
workflow.
Hai . (2021)define three layers,ingestion,
maintenance, andexploration, where corresponding functions
are then sub-grouped. A similar definition is provided
by
Sawadogo and Darmont (2021), where the four main
components of a data lake are defined asingestion,storage,
processing,querying.
Following the maturity- architecture classification,
the degree of the processing of the data is the central
point of consideration. This classification is only helpful in
the discrimination and organization of different data sets,
however, it completely lacks consideration of workflows and
processing capabilities. However,
Sawadogo and Darmont
(2021)
highlight the advantage of the planning of the data
life-cycle. Therefore, ahybrid architecturewas proposed by
Sawadogo and Darmont (2021)alongside thefunctionaland
FrontiersinBigData04frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
maturitybased classifications. Within this architecture, the
individual components are uniquely identified by the data
refinement and the possible functionality, that can be performed
on the specific set of data.
3. Metadata models
Proper metadata management is key to prevent that a data
lake turns into a data swamp and thus is the most important
component to ensure a continuous operation and usability
(Walker and Alrehamy, 2015; Khine and Wang, 2018). Due to
the generally flat hierarchy and the requirement to store any data
in its native format, there is always the risk of losing the overall
comprehensibility of the data lake. This comprehensibility is
lost, if data cannot be found or the relationship to other data
sets cannot be retrieved. One of the most severe consequences
of this is the inability to define concise queries to select
the data one is looking for in a fine-grained manner. As a
consequence, numerous metadata models and systems tailor-
made for usage in data lakes have been proposed. These models
and systems originate from different cases and represent
various viewpoints, and therefore differ regarding their feature
sets. From this wide variety of available options, a few distinct
works have been selected and are discussed in the following
sections.
3.1. Data vault
Data modeling in adata vaultwas proposed by Linstedt
in the 1990s and published in the 2000s to allow for a more
agile metadata evolution, i.e., the continuous development
of the metadata schema, in data warehouses, compared to
star or a snowflake schemata
(Lindstedt and Graziano, 2011).
This ensemble modeling uses traditionally relational database
systems and combines the third normal form with the star
schema. All data is stored in three different types of tables.
Hubsdescribe a business concept and are implemented as lists
of unique keys, and can be populated by different data sources.
Linksdescribe relationships between the aforementioned hubs.
Satellitescontain all attributes which describe the properties of
a hub or a link. Evolving the data vault over time then mainly
implies adding additional satellite tables to links and hubs.
Therefore, there is no need to migrate existing tables, which
facilities the continuous addition of metadata over time.
Due to these characteristics of the data vault concept, it was
also applied in data lakes.
Nogueira . (2018)explained the
definition of a data vault for a specific case and discussed
the advantages of porting it to a NoSQL database by comparing
benchmark results compared to a SQL database. They also
exemplify how data sources can be added by defining hubs, links, and particularly satellites.
Giebler . (2019)
proposed to split the one central, data lake-wide data vault up
into three distinct sub-data vaults: theRaw Vault, theBusiness
Vault, and theData Mart, whereby the latter does not necessarily
need to be modeled in a data vault, but could also be a flat
table, or a star schema. The authors reported that the agile along with the ability to make incremental updates
serves well the needs for a data lake implementation. However,
they pointed out that it can be hard to enforce business rules
across the independent sub-data vaults, which they , that
managing ambiguous keys can not finally be solved, and that
high-frequency data can critically inflate satellites.
3.2. GEMMS
GEMMSis proposed by
Quix . (2016)as aGeneric and
Extensible Metadata Management Systemwith a particular focus
on scientific data management and, in this context, specifically
for the domain of live sciences. The key component of GEMMS
is an abstract entity calledData Unit, which consists of raw data
and its associated metadata. It is stated, that the main advantages
are, flexibility during ingestion and a interface that abstracts
singular files. TheseData Unitscan be annotated with semantic
metadata according to a suitable ontology. However, the core is
described withstructure metadata. Mappings are only discussed
for semi-structured files, CSV, XML, or spreadsheets,
however, it seems straightforward to extend this in other cases.
3.3. MEDAL and goldMEDAL
A graph- metadata model was presented by
Sawadogo . (2019)
, where a subset of data, called an object, is
represented as a hypernode that contains all information about
that particular object, the version, semantic information,
or something calledrepresentations.Representationspresent
the data in a specific way, for instance as a word cloud for
textual data. There is at least one representation required per
object, which is connected to this object by aTransformation.
These representations can betransformed, which is represented
as a directed edge in the hypergraph. This edge contains
information about thetransformation, i.e., a script description
or similar. Data versioning is performed at the level
of these hyperedges connecting two different representations.
Additionally, it is possible to define undetected hyperedges
representing the similarity of two objects, provided that the two
data sets are comparable.
This was revised by
Scholly . (2021). Here,
the concept was simplified to only usedata entities,processes,
links, andgroupings.Processesalso generate newdata entities,
dropping the rather complicated idea ofrepresentations. These
FrontiersinBigData05frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
concepts are again mapped on a hypergraph. Both models
require global metadata, such asontologiesorthesauri.
3.4. CODAL
The data lake, and in particular the utilized metadata
model calledCODAL
(Sawadogo ., 2019)was purpose-
built for textual data. It combines a graph model connecting
all ingested data sets with a data vault describing an individual
data set. One core component is thexml manifest, which is
divided into three parts: i)atomic metadata, ii)non-atomic
metadata, and iii) a division forphysical relational metadata.
Metadata of the first category can be described as key-value
pairs, whereasnon-atomic metadataonly contain references to a
specific entity on a different system, they are "stored in a specific
format in the filesystem"
(Sawadogo ., 2019). Additional
information about the link strength which is modeling the
relational metadatais stored in a dedicated graph database. Here,
each node represents one document with a reference to the
correspondingxml manifest.
3.5. Network- models
A network- model, which extends the simple
categorization by
Oram (2015)into three distinct types of
metadata, i.e.,Business Metadata,Operational Metadata, and
Technical Metadata, was proposed by
Diamantini . (2018)
to improve the data integration of different data sources
ingesting heterogeneous and unstructured data into the data
lake. Here, the notion of objects, or nodes in the resulting
graph, are as well, which are defined by the corresponding
source typology. on these objects, links are generated,
containing astructural, asimilarityor aLemma
(Navigli and
Ponzetto, 2012)
relationship. In this , a node is not
only created for each source but also for each tag in the
structural relationshipmodeling. Lexical similarities are derived
if two nodes have a common lemma in a thesaurus, while
string similarities are computed a suitable metric, in that
particular caseN-Grams
(Peterlongo ., 2005)was .
Similar nodes are merged. Due to this merge, synonyms in queries can be detected and appropriately handled.
3.6. CoreKG
CoreKG
(Beheshti ., 2018)contextualizes the metadata
in the data catalog. To this end, four features has been identified
to constitute this curation service
(Beheshti ., 2017b):
Extraction,Enrichment,LinkingandAnnotation. TheExtraction
functionality extracts information from the raw data containing
natural language, the names of persons, locations, or
organizations.Enrichmentfirst provides synonyms and stems
from the extracted features by lexical knowledge bases WordNet
(Miller, 1995). These extracted and enriched features
then need to be linked to external knowledge bases, likeWikidata
(Vrande
ˇ
ci ́
c, 2012). This enablesCoreKGto understand, if, for
instance, the name of a certain politician was extracted, to link
against the corresponding country that politician is active in, i.e.,
to set it into context. Additionally, users can also annotate the
data items.
3.7. GOODS
GOODSis the internal data lake of Google
(Halevy .,
2016a,b)
. It is unique compared to all other systems presented
since it gathers all its information in apost-hocmanner. This
means, that the individual teams continue working with their
specific tools within their established data silos, whileGOODS
extracts metadata about each dataset by crawling through the
corresponding processing logs or storage-system catalogs. The
central entity of this data lake is a data set, which can be
additionally annotated by users or a special data-stewardship
team. These datasets are then connected by aknowledge graph
(Singhal, 2012)to represent their relationships. Within these
relationships, thedataset containmentenables to split up data
sets, as it allows forbigtable column families
(Chang ., 2008)
to be a data lake entity themselves, along the entirebigtable. Due
to efficient naming conventions for file paths,GOODScan build
uplogical clusters, depending on whether they are regularly, e.g.,
daily, generated, if they are replicated across different compute
centers or if they sharded into smaller data sets. In addition,
the data sets are linked bycontent similarityas well. Since
the entire data lake contains more than 20 billion data sets
with the creation/deletion of 1 billion data sets per day, no
pairwise similarity can be performed. Instead, locality-sensitive
hash values are generated for individual fields of the data set are
generated and compared.
3.8. Constance
Constance
(Hai ., 2016)is a data lake service, which
extracts explicit and implicit metadata from the ingested data,
allows semantic annotations and provides derived metadata
matching and enrichment for a continuous improvement of
the available metadata, and enables inexperienced users to
work with simple keyword- queries by providing a query
rewriting engine
(Hai ., 2018). As it is typically done in data
lakes, data is ingested in raw format. The next step is to extract
as much metadata from it as possible, which is for structured
data likeXMLeasier since schema definitions can be directly
extracted. In the case of semi-structured data, likeJSONorCSV
files, a two step process called theStructural Metadata Discovery
FrontiersinBigData06frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
is necessary. First, it is checked, whether or not metadata is either
encoded in the raw file itself, a self-describing spreadsheet
or if metadata is encoded in the filename or file path. In a
second step, relationships are tried to be discovered during
the lifetime of the data lake between the different datasets, for
instance, on the frequencies of join operations.Semantic
Metadata Matchingis provided by a graph model and should a common ontology. In addition, schemata can be grouped on their similarity, which is useful in highly heterogeneous
data lakes.
4. Provenance
One of the generally most important metadata in
the context of linked data is provenance
(Hartig and Zhao,
2010)
. Data provenance or the data lineage hereby contains
information about the origin of a dataset, e.g., how it was created,
by whom, when it was created, etc. There has been an effort
by theW3Cto standardize the representation of provenance
information by the of an OWL2 ontology, as well as a
general data model, among other documents to complete their
specification calledPROV
(Belhajjame ., 2013; Missier .,
2013)
. Provenance is also becoming increasingly important in
science, as it is a natural way to make scientific work more
comprehensible and reproducible. This can be exemplified by
the adaption ofresearch objects
(Bechhofer ., 2010)and
reusable research objects
(Yuan ., 2018), focusing even
more on precise provenance information and repeatability of
computational experiments. Apart from this, provenance is
considered key in data lakes, to organize, track and link data
sets across different transformations and thereby ensure the
maintainability of a data lake.
4.1. CoreDB and CoreKG
CoreDB
(Beheshti ., 2017a)andCoreKG(Beheshti ., 2018)
are data lake services with a main emphasis on a
comprehensiveREST API, to organize, index and query data
across multiple databases. At the highest level, the main entities
of this data lake are data sets, which can be either of type
relationalor of typeNoSQL. In order to enable simultaneous
querying capabilities the CoreDB web service is itself in front
of all the other employed services. On this layer, queries are
translated betweenSQLandNoSQL. A particular focus is
lineage tracing of these entities. The recorded provenance is
hereby modeled by a directed acyclic graph, where /roles
and entities are nodes while connecting edges represent the
interaction. This employed definition is given by theTemporal
Provenance Model
(Beheshti ., 2012)and can answer, when,
from where, by whom, and how a data set was created, read,
updated, deleted, or queried.
4.2. GOODS
GOODSmetadata model has a particular focus on
provenance
(Halevy ., 2016a,b). In order to build up the
provenance graph, production logs are analyzed in apost-hoc
manner. Then the transitive closure is calculated to determine
the linkage between the data sets themselves. Since the data- events in those logs are extremely high, only a sample
is actually calculated and the transient closure is reduced to a
limited amount of hops.
4.3. Komadu- provenance auditing
Suriarachchi and Plale (2016a,b)proposed a data lake
reference architecture to track data lineage across the lake
by utilizing a central provenance collection subsystem. This
subsystem enables stream processing of provenance events
by providing a suitableIngest APIalong with aQuery
API. In order to centrally collect provenance and process it,
Komadu
(Suriarachchi ., 2015)is . Hereby, distributed
components can send provenance informationvia RabbitMQ
and web service channels. These single events are then
assembled into a global directed acyclic provenance graph,
which can be visualized as forward or backward provenance
graphs. this central subsystem, the need for provenance
stitching
(Missier ., 2010)is circumvented.
4.4. HPCSerA
A data lake case is described in the work of
Bingert .
(2021)
. Here, a specifies a so-calledjob manifest, which
unambiguously describes the job, which should be computed.
This includes the actual compute command, the compute
environments which are provided bySingularitycontainers
(Kurtzer ., 2017), git repositories which should be cloned
and potentially build at run-time, environment variables, annotations, and most importantly the input and expected
output data. This job manifest, written as ajsondocument,
is then sent to the data lake, which is here represented by
a dedicated web application, which is taking control of the
actual synchronization with the underlying services, the
high performance compute cluster or the databases. The data
lake generates all necessary scripts, which are divided into
three phases: i) pre-processing, run, and post-processing. These
scripts are submitted to the compute cluster, where within the
pre-processing step the compute environment is built on the
front-end, and data from a remote S3 storage is staged on a
fast parallel file system. Within this step, all possible degrees of
freedom, the input data, or the git commit, are recorded and
submitted to the data lake, where it is being indexed. Due to
this mechanism, jobs are later on searchable and a provenance
FrontiersinBigData07frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
graph is automatically created, which connects the artifactsvia
the job manifest as edges to their input or raw data. Due to
this recording, as well as the wrapping job manifest, each job is
precisely reproducible since one can submit the exact same job
without any unknowns.
4.5. JUNEAU
JUNEAUis build on top ofJupyter Notebooks, by replacing
its backend and customizing the interface
(Zhang and
Ives, 2019)
. It is therefore specifically targeted at data scientists
who are already familiar with Jupyter Notebooks. The main
constituents of the data lake are tables, or data frames, of which
transformations are tracked. Herefore, the notebook itself is
considered to be the workflow, and each executed cell within is
a task. The provenance information is captured, when the code
within a cell is transmitted to the usedkernel. on this, the
notebook is reformatted into a versioned data flow graph, where
procedural code is transformed into a declarative form
(Ives
and Zhang, 2019)
. a modified top-kthreshold algorithm
(Fagin ., 2003), similar data sets can be found with respect to
the individual provenance.
4.6. DCPAC
In order to manage automotive sensor data, Robert Bosch
GmbH has built a data lake
(Dibowski ., 2020). Although the
paper mainly focuses on their extensive DCPAC (Data Catalog,
Provenance, Control) ontology to build their semantic
layer, a dedicated data processing mechanism is provided. Data
processing is done containerized applications, which can data in the data lake, and either create a data resource
from it or curate existing data sets. The semantic data catalog is
updatedvia Apache Kafkamessages. Hereby, data items are
integrated and their provenance is automatically recorded.
4.7. DataHub
DataHub
(Bhardwaj ., 2014)combines a dataset
version control system, capable of tracking which operations
were performed on which dataset by whom as well as their
dependencies, with a hosted platform on top of it. Hereby
DataHubusestableswhich containrecordsas their primary
entities.Recordsconsist of a key, along with any of
typed, named attributes. In the case of completely unstructured
data, only the key could then refer to an entire file, in the
case of structured or semi-structured files likeXMLorJSON,
the schema can be (partially) modeled into thisrecord. These
individualtablescan then be linked to formdata setsunder
specification of the corresponding relationships. The version
information of atableordata setis managed aversion
graphi.e., a directed acyclic graph where the nodes are data sets
and the edges contain provenance information. In order to query
multiple versions at a time, a SQL- query language called
VQLis provided, which extends SQL about the knowledge that
there are different tables for the different versions of a data set.
Along with DataHub,ProvDB
(Miao ., 2017; Miao
and Deshpande, 2018)
is being developed. It incorporates a
provenance data model
(Chavan ., 2015)which consists
of aConceptual Data Modeland aPhysical Property Graph
Data Model. The first model considers a data science project as
working directory where all files are either of typeResultFile,
DataFile, orScriptFile. These files can be further annotated by
properties, i.e., JSON files. This model is then mapped onto
a property graph, where the edges represent the relationship,
e.g., parenthood. Provenance ingestion is possible threefold.
The first option is to prefix shell commands withprovdb ingest
which then forwards audited information to different specialized
collectors. Secondly, users can provide annotations. Lastly, there
are so-calledFile Views, which allow defining virtual files as a
transformation on an existing file. This can be the execution of a
script or of an SQL query.
5. Support for workflows and
automation
Although the first major challenge in building a data lake
is the aforementioned metadata management, scaling toward
big amounts of data (automated) operation and manageability
of the data lake become increasingly important. For example,
the extraction of metadata related to data, which is being
ingested in a data lake, requires a scalable solution and highly
automated processes that best can be integrated into work-
or data flows wherever necessary
(Mathis, 2017). As in the
case of metadata extraction, it is also here sometimes more
comfortable to split a complicated analysis up into a workflow
consisting of different steps. This has the additional advantage
that different parallelization techniques
(Pautasso and Alonso,
2006; de Oliveira ., 2012)
can then be applied to improve the
scalability of the implemented analysis.
5.1. KAYAK
KAYAK
(Maccioni and Torlone, 2017, 2018)offers so-called
primitivesto analyze newly inserted data in a data lake in anad-
hocmanner. KAYAK itself is a layer on top of the file system and
offers a interface for interactions. The respectiveprimitives
are defined by a workflow of atomic and consistent tasks and
can range from inserting or searching for a data set in the data
lake, to computing k-means, or performing an outlier analysis.
Tasks can be executed either by KAYAK itself, or a third party
FrontiersinBigData08frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
tool can be triggered, likeApache Spark(Zaharia ., 2016)or
Metanome
(Papenbrock ., 2015). Furthermore, tasks can be
sub-divided into individual steps. By defining a directed acyclic
graph, which consists of consecutive dependent primitives, so-
calledpipelinescan be constructed with KAYAK. Here, output
data is not immediately as input for a consecutive primitive,
but output data is first stored back into the data lake and
the corresponding metadata in the data catalog is updated.
Users can define atime-to-actionto specify the maximum
time they are willing to wait for a result or preview, or they
define atolerance, which specifies the minimal accuracy they
demand. A preview is a preliminary result of a step. In order
to enable these features, each step has to expose aconfidenceto
quantify the uncertainty of the correctness of a preview, and a
costfunction to provide information about the necessary run-
time to achieve certain confidence. KAYAK enables the parallel
execution of steps by managing dependencies between tasks.
These dependencies are modeled as a directed acyclic graph
for each primitive. By decomposing these dependency graphs
into singular steps, these can be scheduled by aqueue manager.
It enables the asynchronous execution of tasks by utilizing a
messaging system to schedule these tasks on atask executor,
which is typically provided multiple times on a cluster to allow
for parallel processing.
5.2. Klimatic
Klimatic
(Skluzacek ., 2016)integrates over 10,000
different geo-spatial data sets from numerous online
repositories. It accesses these data setsviaHTTP or Globus
GridFTP. This done in a manner that allows to capture
path- provenance information and can therein identify
relevant data sets on file extensions, likeNetCDForCSV.
It then pre-processes these heterogeneous data sets to integrate
them into this single, homogeneous data lake while ensuring
topological, geo-spatial, and -defined constraints
(Elmasri
and Navathe, 1994; Cockcroft, 1997; Borges ., 1999)
. The
pre-processing is done automatically within a three-phase
data ingestion pipeline. The first step consists of crawling and
scraping, whereDockercontainers are deployed in a scalable
pool. These crawlers retrieve aURLfrom acrawling queue
and then process any data found at thatURL, while adding
newly discoveredURLsback into thecrawling queue. this it is already enough to start with a limited amount
of initial repositories, those of theNational Oceanic and
Atmospheric Administrationor theUniversity Corporation
for Atmospheric Research. After these data sets have been
successfully discovered, these are submitted to aextraction
queue. Elements of this queue are then read byextractor
instances, and alsoDockercontainers which can be elastically
deployed. These extract metadata with suitable libraries/tools,
likeUK Gemini 2.2, and then load the extracted metadata into a
PostgreSQLdatabase. these automated processes, a is
for instance able to query for data in a certain range of latitudes
and longitudes, andKlimaticwill estimate the time needed to
extract all data from the different data sets within the specified
range, and will then provide the with an integrated data set focal operations
(Shashi and Sanjay, 2003).
6. Discussion: Selected cases
and resulting challenges
In this , two different cases or groups,
which can be considered to be representative of larger research
institutions, are presented. on these cases the
previously discussed data lake systems are being analyzed for
their applicability in these cases. In addition, for each ,
the presented systems are analyzed and compared to each other.
for this, 4 standard criteria are chosen. TheGeneralitymeasures
how easily the presented system can be to cover all kinds
of different (research) data. TheAdministrative Effortestimates,
how much work is needed to host the system and necessary
backend services without actually doing the domain research
itself. This is covered by theEase of , where the accessibility
from a pure ’s perspective is analyzed. Lastly, since data lakes
are also commonly in the context of big data, theScalability
is a crucial criterion to estimate the worth of deployment. In
addition to these four criteria, more topic-specific criteria might
be added with regard to the actual focus in the particular .
6.1. groups
In the following two disparate groups are presented,
which mainly differ in their technical proficiency.
6.1.1. Data scientists
In this case, mainly data scientists with an above-average
technology understanding are interacting with the data lake.
Their motivation to a data lake can come from a big data
background, where a massive amount of data should be stored
and processed, as well as standardizing processes and their
provenance auditing to enhance the reproducibility of their
experiments. Hereby, data scientist have the knowledge to work
withSQL,NoSQL, and graph databases, and to interact with
mass storage likeCEPH
(Weil ., 2006). In order to perform
their computations, they rely onad-hoc, local execution of code,
e.g., inJuypter Notebooksbut need to massively scale out their
computations in a later stage. Therefore, they need to be able
to either work in a cloud environment or on high-performance
compute clusters, which are much more cost efficient and are
purpose-built for large parallel applications.
FrontiersinBigData09frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
6.1.2. Domain scientist
In this case, mainly domain scientists are the data
lake. It can be assumed, that they are working in a laboratory,
or something similar in their respective field. Their motivation
to a data lake is driven by the general necessity of having
proper research data management. These users are generally
less experienced in dealing with more complicated applications databases and are generally not required to program large
applications or efficient, parallel code for high-performance
compute clusters. Data lineage does not only need to be recorded on digital transformations, i.e., monitoring which artifacts
were created by which processes on which input data, but
also along measurements and analysis steps that are happening
in laboratories, or comparable. Here, a data lake should be
able to manage all experiment data and associated data, e.g.,
an electronic notebook corresponding to an experiment, and
track for instance a sample across the creation and subsequent
measurement cycle.
6.2. Applicability analysis of the
presented data lakes
The following applicability analysis of the previously
presented data lakes will be done on the two provided cases as well as their perceived administrative effort.
6.3. Architecture
6.3.0.1. Zone architecture:
TheZone Architecturedivides a data lake into different zones
to allow for some organization of different data types, which
allows for easier automation of repetitive tasks. These zones can
be physically implemented on different machines. Users do not
necessarily have to all zones, which means, that they can
for instance not directly raw data, on their own. This
entails an administrative effort to serve all users. Additionally,
there is no assistance for domain scientists and built-in guidance
for reproducible analysis for data scientists by design.
6.3.0.2. Lambda architecture:
TheLambda Architecturehas some similarities with the
zone architecture but has generally a reduced complexity. The
rather rigidly coupled batch and speed layers prevent an agile
development by scientists but are ideally suited to provide
production systems of established workflows while maintaining
all raw data sets for later reuse.
6.3.0.3. Lakehouse:
TheLakehouseadds a valuable metadata layer on top of
an object store and facilitates the advantage of the separation
of storage and compute. This, however, entails a limited set of
supported file formats and therefore cases. The completely
flat hierarchy and homogeneous usage make this architecture
well suited for data scientists and domain scientists alike.
6.3.0.4. FAIR-DO architecture:
TheFAIR Digital Object- Architectureoffers a fine-
grained refinement on the types which also have a
clear abstraction, increasing the general comprehensibility. The
administrative effort is decreased, since data types are
derived from existing ones and general data lake functionalities
only need to be implemented once for the parent objects,
afterwards they can be reused in space. The flat architecture
does not intrinsically restrict and offers a homogeneous
interface across all stages. This allows to implement a customized
but homogeneous interfaces for domain researchers
covering the entire life cycle of a certain experiment. Meanwhile,
can data scientists work with the well-known abstraction
of objects they are familiar with from the object-oriented
programming paradigm. The possibility to globally enforce data
governance on the typed attributes of theseFAIR Digital
Objectsis well suited to integrate different data sources or silos
into a single research data management system.
6.3.0.5. Functional and maturity- architectures:
The classification in eitherfunctional-,maturity-,
or hybrid data lakes undermines the importance to sort data on their refinement, i.e., on the degree of processing
they were subjected to, while also stretching the importance
to formulate activities on the data lake as functions. This
has the advantage of a high standardization which eases
the administrative overhead while guaranteeing minimal data
quality and adherence to institutional policies. It is hard to
distinguish here between domain researchers and data scientists
since it is not clear how actual implementations of these concepts
would respect the different needs of those groups.
6.3.0.6. Qualitative comparison:
Looking at the four presented architectures one can do a
qualitative comparison as shown in
Table 1. As discussed, the
zone architecture has the highest complexity of all four of them,
therefore lacking in administrative effort and ease of , but
it has a high generality. The Lambda architecture reduces the
complexity compared to zone architecture, and is, therefore,
easier to maintain and , but is not as versatile applicable, since
it mainly serves production systems with established workflows.
Similar arguments can be found for the lakehouse, which can
only support limited file formats. The FAIR Digital Object- architecture has a high generality since it can wrap any
existing file. Always offering Digital Objects to interact with
is comfortable for the users but requires more administrative
work, particularly at the beginning. One can also see that all
architectures fulfill the general requirement for a data lake to be
scalable.
FrontiersinBigData10frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
TABLE 1 Comparing the four presented architectures.
Architecture Generality
Administrative
effort
Ease
of Scalability
Zone+––+
Lambda0+++
Lakehouse0+++
FAIR-DO+0++
Putting the presented architectures into the context of
the overall evolution of data lakes which were in their first
years mostly realized Hadoop clusters and the associated
software stack
(Khine and Wang, 2018), one can see a clear
development toward more abstracted systems. The first level
of abstraction was proposed by Functional- architectures,
which can also be mapped on Zone architectures by associating
a certain functionality with a certain zone. This idea was greatly
advanced by the FAIR-DO- Architecture where the users
don’t see the actual system they are working on, but only
trigger the execution of predefined functions a REST
API. This will lower the entry barrier, particularly
for domain researchers, while restricting the general risk of
a loss of consistency across the data lake. The general idea
to organize the data in the data lake regarding their pedigree
of subjected processing has clearly convinced, as it is also a
fundamental part of the newer architectures, i.e., the FAIR-DO Architecture, and the Maturity- architecture. Since
the lambda architecture only offers the serving layer, this is also
true here. Although there was the strive in data lakes to separate
storage and compute, the importance of storage performance
becomes more important in the recent developments around
lakehouses. Here, in future work, one should include active
storage into the entire concept. Promising ideas are shown by
Chakraborty . (2022), which extends the capabilities of a
Ceph cluster.
6.3.1. Metadata models
6.3.1.1. Data vault:
AlthoughData Vaultsmay seem old at first glance, they
actually offer a generic and flexible way of modeling diverse
data sets into a single model. However, designing a properData
Vaultis very challenging and requires deep knowledge about
data modeling in general as well as about the usage of databases.
Therefore, this model rather seems to be more suited for data
scientists than domain researchers, while the administrative
overhead depends on the system.
6.3.1.2. GEMMS:
TheGeneric and Extensible Metadata Management System
was particularly designed for research data. The concept of a data
unit seems straightforward, however, semantic annotations are
only possible with a suitable ontology. Although this increases
the quality of the resulting metadata catalog, resulting in
challenges ontology merging by administrators and the
effort of domain researchers to get their vocabulary into these
ontologies is a drawback. There was also no example provided,
of how this model can be in conjunction with unstructured
data.
6.3.1.3. MEDAL and goldMEDAL:
Also in these models, global metadata ontologies and
thesauri are necessary. The improved version ofgoldMEDAL
seems matured, as it only uses straight forward concepts as
data entities,processes,links, andgroupings. More unintelligible
concepts representations have been dropped. The presented
implementation usedNeo4J, which is within the defined realm of
data scientists. An open challenge seems to be the integration of
fully automated processes with adherence to global governance
to ensure data quality and to guide inexperienced users through
the data lake.
6.3.1.4. CODAL:
CODALwas purpose-built for textual data, it, therefore,
lacks the capacity to scale to generic cases. The combination
of aData Vaultwith a graph database along with the usage of
XMLdocuments seems only suited for experienced users, i.e.,
data scientists. Combining these two models seems powerful
for the specific case, however, entails a corresponding
administrative overhead.
6.3.1.5. Network- models:
This model is also a graph- model, which aims to
integrate heterogeneous and particularly unstructured data into
a single research data management system. The notion of
objects, represented as nodes, offers the necessary versatility,
to adapt to different cases. The required definition
of the corresponding source typology might not be easily
implementable for domain scientists, but the overhead of
including experienced administrators for the initial setup seems
reasonable. However, the powerful relationship modeling structural,similarityandLemmarelationships will introduce
quite some maintenance overhead. This model, therefore, seems
more appropriate for well-experienced data scientists who can
ensure correct implementation and operation.
6.3.1.6. CoreKG:
This data model was discussed in detail for the case of data
containing natural language. The proposed model and presented
workflow to implement this model are very convincing with the
big restriction, that it is only meaningful and implementable for documents containing natural language. Once the workflow
is set up, it seems useful for data scientists as well as domain
researchers.
FrontiersinBigData11frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
6.3.1.7. GOODS:
GOODSbuilds up a data catalog in apost-hocmanner. It
createsdata setswhich are enriched with information within logs
and annotations. Thesedata setsare then connected by a
knowledge graph to represent their relationships. Although the
idea to build up a data lake in apost-hocmanner seems very
promising for any larger institution, it is connected with large
challenges. Each log format and naming convention, also on the
file path level, needs to be understood by the data lake. This
requires for instance domain researchers to strictly follow global
data governance, which usually requires additional auditing.
Also on an administrative site, such a setup is as difficult to
implement as it is compelling to have. Accessing all systems from
one central data lake also comes with certain security risks which
need to be addressed, increasing the complexity even more.
Therefore, as impressive as this internal data lake of Google is,
it is most likely out of reach for other, much smaller research
institutions.
6.3.1.8. Constance:
The presented metadata model inConstancewas applied
to structured and semi-structured data, where metadata was
automatically extracted from the raw data itself or in the full
file path. This lacks the ability to upload an associated
metadata file. This could be done by a domain researcher who
uploads an electronic lab book containing all metadata of a
certain experiment. If this metadata needs to be indexed to
enable a semantic search over it, such a mechanism needs to be
provided. Furthermore, the usage of ontologies enables semantic
metadata matching on the one side, while on the other side
this might be hard to implement. Problems here are, that rather
data scientists are trained to them compared to domain
researchers and that the broader a data lake becomes, the more
likely the introduction of an additional ontology becomes, which
then might require complicated merges of ontologies
(Noy and
Musen, 2003; Hitzler ., 2005)
. Therefore, this seems more feasible for data scientists who are operating on a
restricted set of data sources.
6.3.1.9. Qualitative comparison:
In
Table 2a qualitative comparison of the eight discussed
models is provided. In addition to the previously criteria,
Similar Dataset ExplorationandSemantic Enrichmentis added.
The first one describes the possibility to find data in a
data lake that is similar to data a already has found.
This is for instance for statistical analysis machine learning
important, to be able to increase the input data set. Semantic
Enrichment describes the possibility to, ideally continuously,
add semantic information to data in the data lake to improve
the findability. Implementing and a data vault on top
of a SQL or NoSQL database requires a manageable amount
of time. In addition, it is scalable, one can describe generic
entities and their relations, and allows for evolution over time,
therefore enabling not a continuous but a discrete semantic
enrichment. GEMMS was not yet by default extended to support
any file type, and the of ontologies has certain disadvantages, ontology merging and consultation. It is also not
completely clear how similar data sets can be found and how
a continuous semantic enrichment can be performed. MEDAL
is a rather complicated model, with a steep learning curve.
Relying on Representation which is derived by transformation
will probably limit the usage scenarios. Similarity links allow
for easy dataset exploration and allow for semantic enrichment.
The revised version goldMEDAL improves, compared to
MEDAL, usability by dropping the complicated Representation
and Transformation relationship and reducing it to simpler
Processes. CODAL was purpose-built for textual data, and thus
lacks generality. In addition, relying on a filesystem limits
scalability. Updating semantic metadata in an xml file, however,
allows for continuous semantic enrichment, and connecting
all data entities with nodes representing relationships allows
for a good dataset exploration. The network- models
can describe generic data, but however, the more complicated
notion of source typologies decreases the ease of . N-Grams to compute similarities, similar data sets can be
detected. Structural metadata can be randomly added to allow
for semantic enrichment. CoreKG is again a purpose-built
metadata model for textual data, therefore it is not generalizable.
setting up the full curation service requires some administrative
effort, but offers afterwards an easy and powerful model. The
enrichment and linking service enable continuous data curation
and exploration. GOODS requires that necessary metadata is
encoded in log files or in storage-system catalogs, which limits
the generality. The administrative effort here is enormous,
however, the ease of for the users is great since no change
to their data silos and employed techniques is necessary. The
capability to scale across dozens or hundreds of data centers
is leading and the integration into an existing knowledge
graph enables similar dataset explorations. The evaluation of
semantic enrichment is difficult due to the high velocity of
the data. Constance offers a generic metadata model, however,
the structural metadata discovery did not explicitly include
unstructured data images. The query rewriting engine eases
the drastically and offers similar dataset exploration and
semantic enrichment.
To summarize, the discussed metadata models offer diverse
approaches for data modeling. However, there are common
patterns across these models. All of these models have an
atomic entity around which the entire modeling evolves. In
order to harmonize these different atomic entities, in some
models called objects, ontologies are commonly utilized. This
increases the entry barrier, particularly for domain researchers,
and can always lead to the necessity to perform ontology
merging. These models also always employed a method to model
relationships between their atomic entities, or aggregations of
them. As a general observation, one can state that a proper
FrontiersinBigData12frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
TABLE 2 Comparing the nine presented metadata models.
ModelGenerality
Administrative
effort
Ease
of Scalability
Similar dataset
Exploration
Semantic
enrichment
Data Vault+000+0
GEMMS00–0–0
MEDAL00–0++
goldMEDAL0000++
CODAL-00-++
Network Models+0–0++
CoreKG-–+0++
GOODS0–+++0
Constance00+0++
metadata model for data lakes has to offer some means to
describe, also semantically, its entities on its own, as well
as their relationship toward each other. This has led to the
simultaneous usage of different database systems, i.e., SQL,
NoSQL, and Graph databases, within a single data lake, which
introduced the challenge to query these different systems with
a single query. More powerful query-rewriting engines
and/or suitable meta-languages which support the integration of
semantic meaning within this process is one of the challenges
metadata modeling in data lakes is currently facing. In addition,
semantic metadata extraction from particularly unstructured
data, images, is also a key challenge to improve the usability
and adoption of data lakes.
6.3.2. Provenance
6.3.2.1. CoreDB(KG):
The employedTemporal Provenance Modelis suitable for
data scientists and domain researchers, although it is not a
widely standard. However, no details about the technical
implementation and the versatility are given. Therefore, no final
assessment of the actual applicability is possible.
6.3.2.2. GOODS:
Apart from the already discussed challenges of employing a
post-hocanalysis to populate a central data lake, an additional
challenge arises when this to gather provenance
information: The analysis software needs to write suitable
logs. Since this is not the case for all scientific software, this is hard to implement for domain researchers, while
data scientists might be able to cope with that issue when self-written code or wrappers around existing software suits.
Interestingly, onlyGOODScalculates the transitive closure of the
provenance graph, which seems very useful.
6.3.2.3. Komadu:
This data lake implementation offers a dedicatedIngest
APIwhich uses aRabbitMQmessaging queue, to retrieve
lineage information about the performed processing steps.
The transparent assembly of these singular tasks to a global
provenance graph is comfortable and useful. As in theGOODS
discussion, data scientists can custom build software or write
wrappers around existing ones to utilize the messaging system.
Domain researchers will probably have a hard time when their
scientific software suit does not support this kind of provenance
auditing.
6.3.2.4. HPCSerA:
Here, users are required to describe their analysis they want
to run in ajob manifest. This job is then executed on an system. By usingSingularitycontainers and enabling the
dynamic build and integration of arbitrary git commits, this
integrates well with a typical workflow. These systems
are often by data scientists, which benefit here from
transparent provenance auditing of completely generic jobs and
the ability to re-run a previous analysis to reproduce the results.
This mechanism can also be extended for better Cloud support,
however, there is a lack of anad-hocanalysis with a similar
provenance auditing, which might be important for domain
researchers.
6.3.2.5. JUNEAU:
This modification forJupyter Notebookoffers anad-hoc
experience for users, who are working withPythonand with
tabularized data. SinceJupyter Notebookis broadly utilized by
data scientists and domain researchers alike, it is generally
suited for both groups. However, this only works
for tabularized data and only supportsPythonwhich is
limiting the possible cases. In addition, this presented data
lake implementation fell short of detailed data and metadata
handling and mainly focused onad-hocprocessing. It remains
unclear, how well this implementation is able to serve as a central
research data management system for a variety of data sources
and groups.
FrontiersinBigData13frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
6.3.2.6. DCPAC:
Data lineage inDCPACis recorded by custom build
containers which send messages toApache Kafka. This requires users to containerize their applications and implement
a messaging mechanism. This is a suitable method for data
scientists but is challenging for domain researchers. particularly
for domain researchers, it would be necessary to check the
messages for quality and consistency.
6.3.2.7. DataHub:
This platform which offers a data set version control
system is a good solution for all researchers. The representation
of the provenance within aversion graphis interesting and
straightforward. In addition, the possibility to useProvDB
offers more detailed modeling capabilities on the basis of files.
The ingestion of the provenance data, however, is not generic
enough. the shell command, will not offer a suitable depth,
for instance, to offer full reproducibility, while on the other
hand, thefile viewsare only suitable for data scientists familiar
withSQL. The third provided option, i.e., annotations, is
very error prone and is therefore unsuited as a guideline for good
scientific practice.
6.3.2.8. Qualitative comparison:
In
Table 3a qualitative comparison of the seven discussed
models is provided. In addition to the previously criteria,
Reproducibilityis added. This specifies, whether on
the gathered provenance information, a result can always be
reproduced, which becomes increasingly relevant for scientific
publications. CoreDB/CoreKG offers a RestAPI and thereby an
easy-to- interface and the distinction between the types SQL
and NoSQL offers a great generality. However, the employed
Temporal Provenance Model is not an established standard
and is not aimed to guarantee reproducibility, but rather
comprehensibility. GOODS relies on production logs, on
which heuristics are to calculate the provenance graph. It
is aimed for scalability and efficiency, not for reproducibility.
Komadu relies on a RabbitMQ messaging, it is therefore not
generally applicable. The provided RestAPI is a useful interface. However, reproducibility relies on the quality of the
messages, thus it is depending on the analytics job running, and
is independent of the data lake itself. The data lake which uses
HPCSerA can execute arbitrary scripts and it offers a RestAPI to
work with. Its strength lies in its transparent lineage auditing on
an system by Job Manifests. By storing and linking
all entities, every artifact can be reproduced. The inclusion
of systems makes this setup very scalable. JUNEAU is
extremely -friendly by building directly on top of the well-
known Jupyter Notebooks. Therefore it lacks generality and
scalability since it depends on data frames and is limited to
the resources of a single notebook. The transparent lineage
recording during the submission of the code in a cell to
the kernel allows reproducibility. DCPAC works on arbitrary
data, however the usage of an extensive ontologies requires
users to familiarize with it. The usage of Docker containers
is scalable, and offers a fair reproducibility. DataHub can deal
with any kind of data and offers a comfortable graphical interface. The, although limited, provenance information
in conjunction with the version control of the data allows
for decent reproducibility. In conclusion, while all previously
discussed data lake implementations share the common idea
of building a global provenance graph, there exists a wealth
of different provenance models and auditing methods. In the
future, data lakes should focus more on established models
for provenance representation, to enhance interoperability.
Furthermore, from the fact that each data lake implementation
found a unique provenance auditing , it becomes
clear that in each case a specific processing mechanism was in
mind, an system in HPCSerA or a Jupyter Notebook
in JUNEAU. This means, that not a single data lake offered
provenance auditing capabilities over the entire life cycle of a
data-driven project for generic applications. A future challenge
here is, to support provenance auditing inad-hocanalytics, within a Jupyter Notebook, as well as in larger computations
that run in a Cloud or environment and integrate this
into a homogeneous, global provenance graph, ideally in a
reproducible manner. These single tasks need then to be linked
to workflows, with the same provenance granularity. A similar
challenge here is to support generic applications, without relying
on a built-in messaging or logging functionality.
6.3.3. Support for workflows and automation
6.3.3.1. KAYAK:
KAYAKoffers a very sophisticated mechanism to implement
parallelizable and automatable workflows. The decomposition of
an abstract workflow intotaskswhich can then be combined to
primitivesand finally can be chained to entirepipelines, requires
some prior knowledge. Although powerful, it is rather suited for
data scientists, and not so much for domain researchers, since
the above decomposition of established scientific software suits
into these entities is not straightforward. Furthermore, although
the idea of atoleranceand atime-to-actionis very useful on a data
lake, this is only suitable for a subset of methods that are iterative
by nature. From the viewpoint of a generic scientific application,
this might simply add additional overhead and increase the entry
barrier. Therefore, this well-designed data lake is mostly suitable
for experienced data scientists.
6.3.3.2. Klimatic:
The automated processing capabilities ofKlimaticare onDockercontainers. The generic to split up a
pipeline into distinct stages which are linked by dedicated
queues can be adopted to serve other cases as well. Although
it was only for data set exploration within this particular
implementation, also data analysis pipelines could be set up
FrontiersinBigData14frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
TABLE 3 Comparing the seven provenance models.
ImplementationGenerality
Administrative
effort
Ease
of ScalabilityReproducibility
CoreDB/CoreKG+0++0
GOODS0–++–
Komadu-0+00
HPCSerA+0+++
JUNEAU–0+–+
DCPAC+00+0
DataHub+0+0+ this . This would require building containers that
are pushing back their output into a certain queue, or the current
data lake implementation can be extended to offer a generic
wrapping method that accepts arbitrary containers and then
orchestrates the communication with the queuing system. One
can see, how this data lake can be extended to also serve domain
researchers from other sciences as well.
6.3.3.3. Qualitative comparison:
In
Table 4a qualitative comparison of the two discussed
processing concepts is provided. KAYAK has a scalable and
parallelizable to executing a workflow. Since -
defined Primitives are supported, it is also very generic. In
addition, the interface on top of the filesystem eases the
, however, the entire execution model with its depth of
options requires some time to familiarize with it. Klimatic
presents a specific case and it is not completely clear
how generalizable the general is. Setting all queues
and ingestion pipelines up for the first time requires some
administrative effort but is, therefore, more comfortable for
users to . The usage of remote Docker hosts to serve multiple
workers which get their jobs from a queue is also very scalable.
In conclusion, there is only a limited amount of work
focusing on workflows and automation for processes on data
lakes. The challenge here is to incorporate a scalable back-end
to support the compute-intensive operations associated with
big data. portable containers is a well-suited .
Future developments, however, would largely benefit from a
modularized allowing to integrate different back-ends
in a suitable manner, i.e., to have native support for individual
machines, as well as for Cloud and environments. This
extends explicitly to the employed workflow engines, which
should similarly prevent a lock-in effect, as envisioned byCWL
(Amstutz ., 2016).
7. Summary and outlook
This paper presents and summarizes the most relevant
papers connected to data lakes and analyzes the past
TABLE 4 Comparing the two workflow and automation tools.
Imple
mentation
Generality
Administrative
effort
Ease
of Scalability
KAYAK+00+
Klimatic0–++
developments to identify future challenges. This is done
with a particular focus on the applicability for larger research
institutions which are characterized by diverse groups,
which are for simplicity represented by domain researchers and
data scientists within this paper.
One can see in 2, that there is a trend toward
an abstraction of the underlying systems. This allows to
conceptually model the data life-cycle and increases the
usability by defining a certain functionality within this life-cycle.
Furthermore, by only exposing predefined functions to users the
consistency of the data lake can be ensured, even when by
inexperienced users. To increase the general group of a data
lake, it is important, that the metadata model is similarly easy to and yet generic enough to be suitable for diverse cases.
In 3 it was seen, that there is the general need to
model relationships between some atomic data lake entities.
In addition, these atomic entities also need to be described by
semantic metadata, which will be more intuitive, particularly
for domain researchers. The most important challenge here is,
to find a metadata model, which offers a low entry barrier for
domain researchers to fill in their data, but offers a enough depth
to for experienced users to utilize the sophisticated methods for
special cases, as they are presented.
By analyzing the papers which are presented in 4 it becomes clear, that there is the open quest to develop
a uniform provenance auditing mechanism which is able to
capture homogeneous lineage information along the entire
project life-cycle, reaching from firstad-hocscripts to large-scale
parallel applications.
Also, in 5 there is a clear trend toward containerized
applications to enable processing on data lakes. The advantages
FrontiersinBigData15frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
are many-fold, reaching from portability to an increased
reproducibility. The provided mechanisms to allow for parallel
and asynchronous executions are convincing. The next future
challenge can be identified to enable these methods to different back-ends, reaching from single systems, to public or
private clouds, and systems.
The concluding analysis in 6 took more different
criteria into account and compared the presented data lake
systems on the criteria. Here one could see, that there is no
single data lake system, which fulfills all requirements. Instead,
most of the systems are to some extend purpose built systems,
which are compromising in some aspects to better exceed at
other. In addition, two different groups from the view point
of a larger research institution were defined in 6.1. on these groups a more subjective analysis was done, with
the purpose to motivate the general accessibility of this group to certain data lake implementation. This might be useful
in order to improve systems to attract more , wich is the
largest challenge currently in data lake development. However,
it would have also the largest benefit, to get diverse groups
excited of the idea of data lakes. This will lead to an increased
influx of (meta-) data and methods, and the integration of
previously siloed data will enable novel analysis which have not
been possible before.
Author contributions
HN: writing original draft, analysis, and interpretation
of existing literature. PW: writing original draft,
writing review, and supervision. Both authors
reviewed the results and approved the final version of
the manuscript.
Funding
We gratefully acknowledge funding by the
Niedersächsisches Vorab funding line of the Volkswagen
FoundationandNationalesHochleistungsrechnen
(NHR), a network of eight computing centers in
Germany to provide computing capacity and promote
methodological skills.
Conflict of interest
The authors declare that the research was conducted in the
absence of any commercial or financial relationships that could
be construed as a potential conflict of interest.
Publisher’s note
All claims expressed in this article are solely those of the
authors and do not necessarily represent those of their affiliated
organizations, or those of the publisher, the editors and the
reviewers. Any product that may be evaluated in this article, or
claim that may be made by its manufacturer, is not guaranteed
or endorsed by the publisher.
References
Amstutz, P., Crusoe, M. R., and Tijaníc, N. (2016).Common Workflow
Language. v1. 0. Available online at:
https://www.commonwl.org/v1.0/Workflow.
html
(accessed August 07, 2022).
Armbrust, M., Das, T., Sun, L., Yavuz, B., Zhu, S., Murthy, M., . (2020). Delta
lake: high-performance acid table storage over cloud object stores.Proc. VLDB
Endowment13, 3411–3424. doi: 10.14778/3415478.3415560
Armbrust, M., Ghodsi, A., Xin, R., and Zaharia, M. (2021). “Lakehouse: a generation of open platforms that unify data warehousing and advanced analytics,”
inProceedings of CIDR.
Armbrust, M., Xin, R. S., Lian, C., Huai, Y., Liu, D., Bradley,J. K., . (2015).
“Spark sql: relational data processing in spark,” inProceedings of the 2015 ACM
SIGMOD International Conference on Management of Data(Melbourne, VIC),
1383–1394.
Aundhkar, A., and Guja, S. (2021). A review on enterprise data lake solutions.J.
Sci. Technol. 6, 11–14. doi: 10.46243/jst.2021.v6.i04.pp11-14
Batyuk, A., and Voityshyn, V. (2016). “Apache storm on topology for
-time processing of streaming data from social networks,” in2016 IEEE First
International Conference on Data Stream Mining and Processing (DSMP)(Lviv:
IEEE), 345–349.
Bechhofer, S., De Roure, D., Gamble, M., Goble, C., and Buchan, I. (2010).
Research objects: toward exchange and reuse of digital knowledge.Nat. Preced.
doi: 10.1038/npre.2010.4626.1
Beheshti, A., Benatallah, B., Nouri, R., Chhieng, V. M., Xiong, H., and
Zhao, X. (2017a). “Coredb: a data lake service,” inProceedings of the 2017
ACM on Conference on Information and Knowledge Management(Singapore),
2451–2454.
Beheshti, A., Benatallah, B., Nouri, R., and Tabebordbar, A. (2018).
Corekg: a knowledge lake service.Proc. VLDB Endowment11, 1942–1945.
doi: 10.14778/3229863.3236230
Beheshti, S.-M.-R., Motahari-Nezhad, H. R., and Benatallah,B. (2012).
Temporal provenance model (TPM): model and query language.arXiv preprint
arXiv:1211.5009. doi: 10.48550/arXiv.1211.5009
Beheshti, S.-M.-R., Tabebordbar, A., Benatallah, B., and Nouri, R. (2017b).
“On automating basic data curation tasks,” inProceedings of the 26th
International Conference on World Wide Web Companion(Perth, WA),
165–169.
Belhajjame, K., B’Far, R., Cheney, J., Coppens, S., Cresswell, S., Gil, Y., .
(2013).Prov-dm: The prov data model. Technical Report.
Bhardwaj, A., Bhattacherjee, S., Chavan, A., Deshpande, A.,Elmore,
A. J., Madden, S., . (2014). Datahub: collaborative data science and
dataset version management at scale.arXiv preprint arXiv:1409.0798.
doi: 10.48550/arXiv.1409.0798
Bingert, S., Köhler, C., Nolte, H., and Alamgir, W. (2021). “An API to
include resources in workflow systems,” inINFOCOMP 2021, The Eleventh
International Conference on Advanced Communications and Computation(Porto),
ed C. -P. Rückemann, 15–20.
Borges, K. A., Laender, A. H., and Davis Jr, C. A. (1999). “Spatial data integrity
constraints in object oriented geographic data modeling,” inProceedings of the 7th
FrontiersinBigData16frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
ACM International Symposium on Advances in Geographic Information Systems
(Kansas City, MO), 1–6.
Borthakur, D. (2007). The hadoop distributed file system: architecture and
design.Hadoop Project Website11, 21.
Chakraborty, J., Jimenez, I., Rodriguez, S. A., Uta, A., LeFevre, J., and Maltzahn,
C. (2022). Skyhook: towards an arrow-native storage system.arXiv preprint
arXiv:2204.06074. doi: 10.1109/CCGrid54584.2022.00017
Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A.,Burrows, M., . (2008). Bigtable: a distributed storage system for structured data.ACM Trans.
Comput. Syst. 26, 1–26. doi: 10.1145/1365815.1365816
Chavan, A., Huang, S., Deshpande, A., Elmore, A., Madden, S., and
Parameswaran, A. (2015). “Towards a unified query language forprovenance and
versioning,” in7th USENIX Workshop on the Theory and Practice of Provenance
(TaPP 15)(Edinburgh).
Cockcroft, S. (1997). A taxonomy of spatial data integrity constraints.
Geoinformatica1, 327–343. doi: 10.1023/A:1009754327059
de Oliveira, D., Ogasawara, E., Oca na, K., Bai ao, F., and Mattoso, M. (2012).
An adaptive parallel execution strategy for cloud- scientific workflows.
Concurrency Comput. 24, 1531–1550. doi: 10.1002/cpe.1880
Dean, J., and Ghemawat, S. (2008). Mapreduce: simplified data processing on
large clusters.Commun. ACM. 51, 107–113. doi: 10.1145/1327452.1327492
Devlin, B. A., and Murphy, P. T. (1988). An architecture for a business and
information system.IBM Syst. J. 27, 60–80. doi: 10.1147/sj.271.0060
Diamantini, C., Giudice, P. L., Musarella, L., Potena, D., Storti, E., and Ursino,
D. (2018). “A metadata model to uniformly handle heterogeneous data
lake sources,” inEuropean Conference on Advances in Databases and Information
Systems(Nicosia: Springer), 165–177.
Dibowski, H., Schmid, S., Svetashova, Y., Henson, C., and Tran, T. (2020).
“ semantic technologies to manage a data lake: data catalog, provenance and control,” inSSWS@ ISWC(Athen), 65–80.
Dixon, J. (2010).Pentaho, Hadoop, and Data Lakes. Available online at:
https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/
(accessed April 22, 2022).
Elmasri, R., and Navathe, S. (1994). Fundamentals of databasesystems.
El-Sappagh, S. H. A., Hendawi, A. M. A., and El Bastawissy, A. H. (2011). A
proposed model for data warehouse ETL processes.J. King Saud Univer. Comput.
Inf. Sci. 23, 91–104. doi: 10.1016/j.jksuci.2011.05.005
Fagin, R., Lotem, A., and Naor, M. (2003). Optimal aggregation algorithms for
middleware.J. Comput. Syst. Sci. 66, 614–656. doi: 10.1016/S0022-0000(03)00026-6
Giebler, C., Gröger, C., Hoos, E., Eichler, R., Schwarz, H., and Mitschang,
B. (2021). “The data lake architecture framework: a foundation for building
a comprehensive data lake architecture,” inProceedings der 19. Fachtagung für
Datenbanksysteme für Business, Technologie und Web (BTW 2021).
Giebler, C., Gröger, C., Hoos, E., Schwarz, H., and Mitschang, B. (2019).
“Modeling data lakes with data vault: practical experiences, assessment, and lessons
learned,” inInternational Conference on Conceptual Modeling(Salvador: Springer),
63–77.
Giebler, C., Gröger, C., Hoos, E., Schwarz, H., and Mitschang, B. (2020). “A zone
reference model for enterprise-grade data lake management,”inProceedings of the
24th IEEE Enterprise Computing Conference (EDOC 2020)(Eindhoven: IEEE).
Golec, D. (2019). “Data lake architecture for a banking data model,” in
ENTRENOVA-ENTerprise REsearch InNOVAtion, Vol. 5(Zagreb), 112–116.
Gorelik, A. (2019).The Enterprise Big Data Lake: Delivering the Promise of Big
Data and Data Science. Sebastopol, CA: O’Reilly Media.
Hai, R., Geisler, S., and Quix, C. (2016). “Constance: an intelligent data lake
system,” inProceedings of the 2016 International Conference on Management of
Data(San Francisco, CA), 2097–2100.
Hai, R., Quix, C., and Jarke, M. (2021). Data lake concept and systems: a survey.
arXiv preprint arXiv:2106.09592. doi: 10.48550/arXiv.2106.09592
Hai, R., Quix, C., and Zhou, C. (2018). “Query rewriting for heterogeneous data
lakes,” inEuropean Conference on Advances in Databases and Information Systems
(Budapest: Springer), 35–49.
Halevy, A., Korn, F., Noy, N. F., Olston, C., Polyzotis, N., Roy,S., . (2016a).
“Goods: organizing google’s datasets,” inProceedings of the 2016 International
Conference on Management of Data(San Francisco, CA), 795–806.
Halevy, A. Y., Korn, F., Noy, N. F., Olston, C., Polyzotis, N., Roy, S., .
(2016b). Managing google’s data lake: an overview of the goodssystem.IEEE Data
Eng. Bull. 39, 5–14. doi: 10.1145/2882903.2903730
Hartig, O., and Zhao, J. (2010). “Publishing and consuming provenance
metadata on the web of linked data,” inInternational Provenance and Annotation
Workshop(Troy: Springer), 78–90.
Hasani, Z., Kon-Popovska, M., and Velinov, G. (2014). “Lambda architecture for time big data analytic,” inICT Innovations(Ohrid), 133–143.
Hitzler, P., Krötzsch, M., Ehrig, M., and Sure, Y. (2005). “What is ontology
merging?” inAmerican Association for Artificial Intelligence(Palo Alto, CA: AAAI
Press), 4.
Hukkeri, T. S., Kanoria, V., and Shetty, J. (2020). “A study of enterprise data
lake solutions,” inInternational Research Journal of Engineering and Technology
(IRJET),Vol. 7(Tamilnadu).
Inmon, B. (2016).Data Lake Architecture: Designing the Data Lake and Avoiding
the Garbage Dump. Basking Ridge, NJ: Technics Publications.
Inmon, W. H. (2005).Building the Data Warehouse. Indianapolis, IN: John
Wiley & Sons.
Ives, Z. G., and Zhang, Y. (2019). “Dataset relationship management,” in
Proceedings of Conference on Innovative Database Systems Research (CIDR 19)
(Monterey, CA).
Khine, P. P., and Wang, Z. S. (2018). Data lake: a ideology inbig data era.
ITM Web Conf. 17, 03025. doi: 10.1051/itmconf/20181703025
Kurtzer, G. M., Sochat, V., and Bauer, M. W. (2017). Singularity:
Scientific containers for mobility of compute.PLoS ONE12, e0177459.
doi: 10.1371/journal.pone.0177459
Li, J. (2014). “Design of -time data analysis system basedon impala,” in2014
IEEE Workshop on Advanced Research and Technology in Industry Applications
(WARTIA)(Ottawa, ON: IEEE), 934–936.
Lindstedt, D., and Graziano, K. (2011).Super Charge Your Data Warehouse:
Invaluable Data Modeling Rules to Implement Your Data Vault. North Charleston,
CA: CreateSpace.
Maccioni, A., and Torlone, R. (2017). Crossing the finish line faster when
paddling the data lake with kayak.Proc. VLDB Endowment10, 1853–1856.
doi: 10.14778/3137765.3137792
Maccioni, A., and Torlone, R. (2018). “Kayak: a framework forjust-in-time data
preparation in a data lake,” inInternational Conference on Advanced Information
Systems Engineering(Tallinn: Springer), 474–489.
Madera, C., and Laurent, A. (2016). “The next information architecture
evolution: the data lake wave,” inProceedings of the 8th International Conference
on Management of Digital Ecosystems(Biarritz), 174–180.
Madsen, M. (2015).How to Build an Enterprise Data Lake: Important
Considerations Before Jumping in. San Mateo, CA: Third Nature Inc.
Mathis, C. (2017). Data lakes.Datenbank Spektrum17, 289–293.
doi: 10.1007/s13222-017-0272-7
Miao, H., Chavan, A., and Deshpande, A. (2017). “Provdb: Lifecycle
management of collaborative analysis workflows,” inProceedings of the 2nd
Workshop on Human-in-the-Loop Data Analytics(Chicago, IL), 1–6.
Miao, H., and Deshpande, A. (2018). Provdb: provenance-enabled lifecycle
management of collaborative data analysis workflows.IEEE Data Eng. Bull. 41,
26–38. doi: 10.1145/3077257.3077267
Miller, G. A. (1995). Wordnet: a lexical database for english.Commun. ACM. 38,
39–41. doi: 10.1145/219717.219748
Miloslavskaya, N., and Tolstoy, A. (2016). Big data, fast data and data lake
concepts.Procedia Comput. Sci. 88, 300–305. doi: 10.1016/j.procs.2016.07.439
Missier, P., Belhajjame, K., and Cheney, J. (2013). “The W3C PROV family
of specifications for modelling provenance metadata,” inProceedings of the 16th
International Conference on Extending Database Technology(Genoa), 773–776.
Missier, P., Ludäscher, B., Bowers, S., Dey, S., Sarkar, A.,Shrestha, B., . (2010). “Linking multiple workflow provenance traces for interoperable
collaborative science,” inThe 5th Workshop on Workflows in Support of Large-Scale
Science( Orleans, LA: IEEE), 1–8.
Munappy, A. R., Bosch, J., and Olsson, H. H. (2020). “Data pipeline management
in practice: challenges and opportunities,” inProduct-Focused Software Process
Improvement, ed M. Morisio, M. Torchiano, and A. Jedlitschka (Cham: Springer
International Publishing), 168–184.
Munshi, A. A., and Mohamed, Y. A.-R. I. (2018). Data lake lambda
architecture for smart grids big data analytics.IEEE Access6, 40463–40471.
doi: 10.1109/.2018.2858256
Navigli, R., and Ponzetto, S. P. (2012). Babelnet: The automatic construction,
evaluation and application of a wide-coverage multilingual semantic network.
Artif. Intell. 193, 217–250. doi: 10.1016/j.artint.2012.07.001
FrontiersinBigData17frontiersin.org Wieder and Nolte10.3389/fdata.2022.945720
Nogueira, I. D., Romdhane, M., and Darmont, J. (2018). “Modeling data lake
metadata with a data vault,” inProceedings of the 22nd International Database
Engineering and Applications Symposium(Villa San Giovanni), 253–261.
Nolte, H., and Wieder, P. (2022). Realising data-centric scientific
workflows with provenance-capturing on data lakes.Data Intell. 4, 426–438.
doi: 10.1162/dint_a_00141
Noy, N. F., and Musen, M. A. (2003). The prompt suite: interactive tools
for ontology merging and mapping.Int. J. Hum. Comput. Stud. 59, 983–1024.
doi: 10.1016/j.ijhcs.2003.08.002
Oram, A. (2015).Managing the Data Lake: Moving to Big Data Analysis.
Sebastopol, CA: O’Reilly Media.
Papenbrock, T., Bergmann, T., Finke, M., Zwiener, J., and Naumann, F.
(2015). Data profiling with metanome.Proc. VLDB Endowment8, 1860–1863.
doi: 10.14778/2824032.2824086
Patel, P., Wood, G., and Diaz, A. (2017). “Data lake governance best practices,”
inThe DZone Guide to Big Data-Data Science and Advanced Analytics, Vol.4
(Durham, NC), 6–7.
Pautasso, C., and Alonso, G. (2006). “Parallel computing patternsfor grid
workflows,” in2006 Workshop on Workflows in Support of Large-Scale Science
(Paris: IEEE), 1–10.
Pérez-Arteaga, P. F., Castellanos, C. C., Castro, H., Correal,D., Guzmán,
L. A., and Denneulin, Y. (2018). “Cost comparison of lambda architecture
implementations for transportation analytics public cloud software as a
service,” inSpecial Session on Software Engineering for Service and Cloud Computing
(Porto), 855–862.
Peterlongo, P., Pisanti, N., Boyer, F., and Sagot, M.-F. (2005). “Lossless filter
for finding long multiple approximate repetitions a data structure, the
bi-factor array,” inInternational Symposium on String Processing and Information
Retrieval(Buenos Aires: Springer), 179–190.
Quix, C., Hai, R., and Vatov, I. (2016). “Gemms: a generic and extensible
metadata management system for data lakes,” inCAiSE Forum, Vol. 129
(Ljubljana).
Ramakrishnan, R., Sridharan, B., Douceur, J. R., Kasturi, P., Krishnamachari-
Sampath, B., Krishnamoorthy, K., . (2017). “Azure data lake store: a hyperscale
distributed file service for big data analytics,” inProceedings of the 2017 ACM
International Conference on Management of Data(Chicago, IL), 51–63.
Ravat, F., and Zhao, Y. (2019). “Data lakes: trends and perspectives,” in
International Conference on Database and Expert Systems Applications(Linz:
Springer), 304–313.
Sawadogo, P., and Darmont, J. (2021). On data lake architectures and metadata
management.J. Intell. Inf. Syst. 56, 97–120. doi: 10.1007/s10844-020-00608-7
Sawadogo, P. N., Scholly, E., Favre, C., Ferey, E., Loudcher, S., and Darmont,
J. (2019). “Metadata systems for data lakes: models and features,” inEuropean
Conference on Advances in Databases and Information Systems(Bled: Springer),
440–451.
Scholly, E., Sawadogo, P., Liu, P., Espinosa-Oviedo, J. A., Favre, C., Loudcher, S., . (2021). Coining goldmedal: a contribution to data lakegeneric metadata
modeling.arXiv preprint arXiv:2103.13155. doi: 10.48550/arXiv.2103.13155
Sethi, R., Traverso, M., Sundstrom, D., Phillips, D., Xie, W., Sun, Y., . (2019).
“Presto: Sql on everything,” in2019 IEEE 35th International Conference on Data
Engineering (ICDE)(Macao: IEEE), 1802–1813.
Sharma, B. (2018).Architecting Data Lakes: Data Management Architectures for
Advanced Business Cases. Sebastopol, CA: O’Reilly Media.
Shashi, S., and Sanjay, C. (2003).Spatial databases: A Tour. Upper Saddle River,
NJ: Prentice Hall.
Singhal, A. (2012). Introducing the knowledge graph: things, not strings.Off.
Google Blog5, 16.
Skluzacek, T. J., Chard, K., and Foster, I. (2016). “Klimatic:a virtual data lake
for harvesting and distribution of geospatial data,” in2016 1st Joint International
Workshop on Parallel Data Storage and data Intensive Scalable Computing Systems
(PDSW-DISCS)(Salt Lake City, UT: IEEE), 31–36.
Suriarachchi, I., and Plale, B. (2016a). “Crossing analytics systems: a case for
integrated provenance in data lakes,” in2016 IEEE 12th International Conference
on e-Science (e-Science)(Baltimore, MD: IEEE), 349–354.
Suriarachchi, I., and Plale, B. (2016b). “Provenance as essential infrastructure for
data lakes,” inInternational Provenance and Annotation Workshop(McLean, VA:
Springer), 178–182.
Suriarachchi, I., Zhou, Q., and Plale, B. (2015). Komadu: a capture and
visualization system for scientific data provenance.J. Open Res. Software3, e4.
doi: 10.5334/jors.bq
Terrizzano, I. G., Schwarz, P. M., Roth, M., and Colino, J. E. (2015). “Data
wrangling: the challenging yourney from the wild to the lake,” inCIDR(Asilomar).
Thusoo, A., Sarma, J. S., Jain, N., Shao, Z., Chakka, P., Anthony,
S., . (2009). Hive: a warehousing solution over a map-reduce
framework.Proc. VLDB Endowment2, 1626–1629. doi: 10.14778/1687553.
1687609
Villari, M., Celesti, A., Fazio, M., and Puliafito, A. (2014). “Alljoyn lambda:
an architecture for the management of smart environments iniot,” in2014
International Conference on Smart Computing Workshops(Hong Kong: IEEE),
9–14.
Vohra, D. (2016).Apache parquet. In Practical Hadoop Ecosystem. York,
NY: Springer.
Vrande
ˇ
ci ́
c, D. (2012). “Wikidata: a platform for collaborative data
collection,” inProceedings of the 21st International Conference on World Wide Web
(Lyon), 1063–1064.
Walker, C., and Alrehamy, H. (2015). “Personal data lake with data gravity pull,”
in2015 IEEE Fifth International Conference on Big Data and Cloud Computing
(Dalian: IEEE), 160–167.
Warren, J., and Marz, N. (2015).BigData:Principles and Best Practices of
Scalable Realtime Data Systems. Shelter Island, NY: Simon and Schuster.
Weil, S. A., Brandt, S. A., Miller, E. L., Long, D. D., and Maltzahn, C. (2006).
“Ceph: a scalable, high-performance distributed file system,” inProceedings of the
7th Symposium on Operating Systems Design and Implementation(Seattle, WA),
307–320.
Yuan, Z., Ton That, D. H., Kothari, S., Fils, G., Malik, T., .
(2018). Utilizing provenance in reusable research objects.Informatics, 5, 14.
doi: 10.3390/informatics5010014
Zaharia, M., Chowdhury, M., Franklin, M. J., Shenker, S., andStoica, I. (2010).
“Spark: cluster computing with working sets,” in2nd USENIX Workshop on Hot
Topics in Cloud Computing (HotCloud 10)( York, NY).
Zaharia, M., Xin, R. S., Wendell, P., Das, T., Armbrust, M., Dave, A., . (2016).
Apache spark: a unified engine for big data processing.Commun. ACM. 59, 56–65.
doi: 10.1145/2934664
Zhang, Y., and Ives, Z. G. (2019). Juneau: data lake management
for jupyter.Proc. VLDB Endowment12, 3352095. doi: 10.14778/3352063.33
52095
Zikopoulos, P. (2015).Big Data Beyond the Hype: A Guide to Conversations
for Today’s Data Center. York, NY; Chicago, IL; San Francisco, CA; Athens;
Athens; London; Madrid; Mexico City; Milan; Delhi; Singapore; Sydney,
NSW; Toronto, ON: McGraw-Hill Education.
FrontiersinBigData18frontiersin.org Citation:Rafay, A.; Aziz, M.; Zia, A.;
Asif, A.R. Automated Retrieval of
Heterogeneous Proteomic Data for
Machine Learning.J. Pers. Med.2023,
13, 790. https://doi.org/10.3390/
jpm13050790
Academic Editor: Salvatore Scacco
Received: 11 April 2023
Revised: 28 April 2023
Accepted: 28 April 2023
Published: 2 May 2023
Copyright:© 2023 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
Journal of
Personalized Medicine
Article
Automated Retrieval of Heterogeneous Proteomic Data for
Machine Learning
Abdul Rafay
1,2
, Muzzamil Aziz
2,
*, Amjad Zia
1
and Abdul R. Asif
1,3,
*
1
Department for Clinical Chemistry/Interdisciplinary UMG Laboratories, University Medical Center,
37075 Göttingen, Germany
2
Future Networks, eScience Group, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH
Göttingen (GWDG), 37077 Göttingen, Germany
3
German Centre for Cardiovascular Research (DZHK), Partner Site Göttingen, 37075 Göttingen, Germany
*Correspondence: muzzamil.aziz@gwdg.de (M.A.); asif@med.uni-goettingen.de (A.R.A.)
Abstract:Proteomics instrumentation and the corresponding bioinformatics tools have evolved at a
rapid pace in the last 20 years, whereas the exploitation of deep learning techniques in proteomics is
on the horizon. The ability to revisit proteomics raw data, in particular, could be a valuable resource
for machine learning applications seeking insight into protein expression and functions of
previously acquired data from different instruments under various lab conditions. We map publicly
available proteomics repositories (such as ProteomeXchange) and relevant publications to extract
MS/MS data to form one large database that contains the patient history and mass spectrometric
data acquired for the patient sample. The extracted mapped dataset should enable the research to
overcome the issues attached to the dispersions of proteomics data on the internet, which makes it
difficult to apply emerging bioinformatics tools and deep learning algorithms. The workflow
proposed in this study enables a linked large dataset of heart-related proteomics data, which could
be easily and efficiently applied to machine learning and deep learning algorithms for futuristic
predictions of heart diseases and modeling. Data scraping and crawling offer a powerful tool to
harvest and prepare the training and test datasets; however, the authors advocate caution because of
ethical and legal issues, as well as the need to ensure the quality and accuracy of the data that are
being collected.
Keywords:mass spectrometry; proteomics; machine learning; data scraping; data harvesting
1. Introduction
The revolution in mass spectrometric technologies since the early 1990s allowed the
identification and quantification of a larger of proteins in a variety of sample types.
In the last 10 years, there have been significant advancements in high-resolution mass spec-
trometry (HRMS), which took the identification and quantification of proteins to a higher
level of accuracy and sensitivity [1]. However, the accumulation of MS data possesses
numerous challenges including vender-specific data formats and data heterogeneity with
regard to the quality and completeness of the data generated by different MS instruments
(Table 1). For instance, there might be events where the data generated by some of the
instruments is of low quality and with a high level of background noise, while others
may produce high-quality data with a low level of background noise. In addition, some
instruments may have different levels of sensitivity and specificity for proteomics analysis,
which ultimately impacts the accuracy and completeness of the data produced from these
instruments and from different labs [2].
Machine learning (ML) and deep learning (DL) models perform significantly better
when trained on large datasets to identify more complex patterns and relationships within
the data. The rapid growth in protein sequencing technology has led to enormous amounts
of proteome data, which present a valuable opportunity for machine learning applications
J. Pers. Med.2023,13, 790. https://doi.org/10.3390/jpm13050790https://www.mdpi.com/journal/jpm J. Pers. Med.2023,13, 7902 of 10
in the field of bioinformatics. Proteome data provide information on protein expression,
modification, and , and can be to predict the effects of genetic mutations
and drug treatments. However, the sheer volume of proteome data can pose a significant
challenge for analysis and interpretation. In the last two decades, the field of deep learning
has made remarkable strides, outperforming conventional machine learning algorithms in
a variety of fields, including computer vision, speech recognition, natural language process-
ing, bioinformatics, and medical image analysis. In an effort to mimic the functioning of
the human brain, deep learning makes of the capabilities of artificial neural networks
that assist representation learning. Deep learning can automatically discover and learn
complex patterns and features from data, unlike traditional machine learning approaches
such as support vector machines (SVM) and random forests (RF), which require manual
feature engineering. As a result, it works especially well for scientific fields that have to huge, complex datasets.
Table 1.Various MS output file formats encoding the mass spectra.
Files Formats
Vendor SpecificOpen Spectrum
d/(Waters and Agilent instruments)mzXML
raw/(Waters and Agilent instruments)mzML
RAW(Thermo Scientific instruments)mz5
yep(Agilent/Bruker instruments)imzML
wiff(AB SCIEX instruments)mzData
t2d(ABI/Sciex instruments)YAMSF
MGF
dta
pkl
ms2
ms1
With the advancement of MS instruments, proteomics data are increasing at a rapid
pace. One source of proteome data that has gained significant attention in recent years is
the Protein Data Bank (PDB), which contains over 175,326 entries only for protein structures
( date: 28 February 2023). Similarly, the current release of UniProtKB/Swiss-Prot
protein knowledgebase statistics (release 2023_01) showed a huge amount of additional
data from the previous year (2022). These sources provide a rich foundation of proteomics
data for machine learning algorithms, but the diversity of the data, as well as their size,
poses significant challenges for processing and analysis [3]. Therefore, it is crucial to
integrate the pile of scattered proteomics data into a single database—a rapidly developing
field called “proteomic data integration”—for improving our comprehension of complex
biological systems and for creating diagnostics and therapeutics approaches.
In liquid-chromatography mass spectrometry (LC-MS)- proteomics, discovery
experiments were often employed to investigate potential biomarker candidates. While
fold change and statistical significance are still regularly utilized, supervised machine
learning (ML) is increasingly becoming acknowledged as a potent method for identifying
potential biomarker candidates. ML methods logistic regression (LR), random forest
(RF), K-nearest neighbors (KNN), and support vector machine (SVM) are being by
proteomics researchers. These ML techniques have been successfully to identify
biomarkers for the development of cancer and forecast its subtypes. These ML techniques
can be in a variety of platforms, including Perseus, the R, or Python programming
languages and commercially available applications [4].
To address the above-mentioned challenges, AI tools and technologies are being
developed to enable the efficient processing and analysis of proteome data. For instance,
deep learning techniques such as convolutional neural networks and recurrent neural
networks have shown promise in analyzing large-scale proteome data [5]. Additionally, J. Pers. Med.2023,13, 7903 of 10
cloud- technologies such as Apache Spark and Hadoop can be to parallelize
data processing and facilitate data sharing among different researchers [6].
The of AI tools and technologies for the analysis of proteome data holds sig-
nificant promise for unlocking the full potential of these valuable resources. However,
it is important to ensure that the ethical and legal aspects are considered and that the
quality and accuracy of the data are rigorously evaluated. By leveraging the power of
AI and machine learning, researchers can make significant strides in understanding the
complexities of proteomics data and them to develop treatments and therapies.
In the present study, we collect the raw MS data from 126 published studies heart
tissue/cell material proteomics from the ProteomeXchange repository. The data were allied
with the patient/animal disease and treatment history. For MS data submitted without
history information, the corresponding authors were automatically sent email reminders to
update the missing information.
2. Methodology
The Python programming language was to automate the web crawling and
scraping processes for the ProteomeXchange website and publication web pages. To
help implement the web crawler spider and parse HTML data, the Scrapy framework
(v. 2.8.0) of Python (v. 3.10) was employed. The main page of the ProteomeXchange website
contains an interactive form to input the search criteria and narrows down the publications
matching the search criteria. The web source (ProteomeXchange) contained thousands of
proteome-related studies, and only those studies related to heart diseases were targeted.
The Selenium web driver (v. 4.8.3) and Python framework were to create a web bot
that can automatically fill in the form to make a search engine work. The web crawling
layout utilized in this project is depicted in 1. A web bot is to take the provided
keywords and crawl through the links that are downloadable from the main page during
the automatic crawling and scraping process. Each URL is then added to a queue as a result
of this action. The URLs are then taken out of the queue one at a time, and the web scraper
starts working on each URL individually to extract the necessary data. This procedure
makes it possible to efficiently and methodically extract data from a large of web
pages, allowing for the timely and efficient gathering of enormous amounts of information. 1.An illustration of data crawling. In the automatic crawling and scraping, the web bot takes
the provided keywords, and then, it crawls all the links fetched from the main web page and builds a
queue. Each URL is deque from the queue and the web scraper works on that URL. J. Pers. Med.2023,13, 7904 of 10
2.1. Data Request
With the keyword “heart”, a total of 280 publications were scraped ( date: 23
January 2023), and only eight datasets (publications) included patient medical histories.
Thus, an automated email remainder was created and sent to the corresponding authors
of the corresponding publications, requesting data on a patient’s medical history and
experimental design. The whole pipeline was automated Python programming and
consolidated to fetch the author, corresponding author’s email, and publication details.
The libraries, packages, and their versions that were are as follows: Scrapy (V. 2.8.0),
Selenium web driver (v. 4.8.3), Pandas (v. 1.5.3), Python (v. 3.10), and Dask (v. 2023.3.2).
2.2. Data Collection
Python, along with its powerful libraries such as Pandas and NumPy, is a popular
choice for processing, analyzing, and visualizing large datasets. The Pandas library pro-
vides a comprehensive set of data structures and functions for working with structured data,
while the NumPy library offers fast numerical operations on large arrays. To scrape data
from the website, Scrapy with Python was , which is considered a fast and powerful
Python- web crawling framework in comparison to other tools [7].
We first iterated through the table content present on the first page and harvested
the web links to their personalized publication metadata web page. The personalized
publication metadata web page contained the publication-related information in XML
format, which was also scraped, and also included the publication link and the dataset
PRIDE URI, as well as an FTP link to the RAW proteome spectra files. Furthermore, the
profiled page also contained the FTP link to the experimental design file, which was present
in the TSV file format. All the heterogeneous data were scraped from multiple sources
and combined into one big CSV file in a wide columnar way, which contains a of
columns with respective data in them. The scraping steps are demonstrated in 2.
The process of data collection can be illustrated in two steps ( 2). First, a master
table is made on the primary web page web scraping techniques on the
specified keyword filter. The publication data is then extracted from the online extensible
markup language (XML) page each uniform resource identifier (URI) present in the
master table, as shown in step 1. The web scraper iterates through the numerous hypertext
markup language (HTML) tables and nested XML documents that are present on the web
page in the second step. The master table contains all the data that the scraper gathers from
the HTML tables. By following these steps, the necessary data are gathered and stored in
an organized manner, making them ready for additional analysis and processing.
2.3. Data Preparation and Data Wrangling
The harvested data were checked for errors, and data wrangling and munging proce-
dures were applied, including deleting duplicate entries, resolving missing or inconsistent
values, translating the data into a standard format, and dealing with outliers or mistakes, to
ensure the data are organized, consistent, and acceptable for the analysis. The data crawled
and scraped from web pages were unstructured and in raw format. Python Numpy, Pandas
(v. 1.5.3), and Dask libraries (v. 2023.3.2) were applied for the wrangling and munging
of the data. Some fields on the mathematical formulas were also derived to obtain
the correct and meaningful attributes to make a machine learning model on it, e.g., age
column was derived from date-of-birth column. There was also a great noise in the data,
such as many attributes coming with different column names and containing uncertain
values, which were also managed through Python and cleaned with Dask and Pandas. J. Pers. Med.2023,13, 7905 of 10 2.An illustration of the data collection. First, we have a master table scraped from the main
web page on the given keyword filter. Each URI present in the master table is to extract
the publication information from the online XML page as shown in step 1. In step 2, the scraper
iterates through the nested XML documents and multiple HTML tables present on the web page. At
the end, it extracts all the necessary information and stores it in the master table.
2.4. Data Consolidation
To store, manage, and analyze the data as a single entity, it is necessary to consolidate
the data from several sources into a single database. We adopted data lake architecture and
summarized the main data sources to be three when attributed, and they are as follows:
•Publication metadata: it refers to the information related to the publication itself. For
example, authors’ names and emails, titles, instruments , lab heads, keywords, etc.
•Publication characteristics and relevant information: it refers to the published dataset
of the publication and information related to it. For example, the FTP file link and file
types and formats of users observed measurements of proteome spectra available in
RAW file formats. J. Pers. Med.2023,13, 7906 of 10
•Medical patient history: it refers to the information related to the observed patients.
For example, patient age, gender, cell types, observed values, protein sequences,
disease type, etc. 3 depicts the data collection and cleaning pipeline statistics. The X-axis rep-
resents the of records cleaned per publication, while the Y-axis represents the
percentage of query errors. The graph is divided into two sections: passive and active
cleaning. Passive cleaning is the process of removing unnecessary information or duplicates
from a dataset without requiring intervention, whereas active cleaning requires input to identify and remove irrelevant data. The graph demonstrates that for both passive
and active cleaning, the percentage of query errors tends to decline as the of records
cleaned rises. This suggests that thorough cleaning produces data of higher quality that
can be amassed. The graph also demonstrates that active cleaning, as opposed to passive
cleaning, tends to produce a lower percentage of query errors. This further suggests that
adding input can enhance the precision and standard of the data-cleaning process. 3.Illustration of average records cleaned per publication. The X-axis represents the of
records cleaned in contrast to percentage of query error onY-axis.
2.5. Proposed Architecture for Machine Learning Exploitation
Here, we propose exemplary architecture for machine learning and deep learning
models to exploit the consolidated cleaned data ( 4). In this proposed architecture,
AWS S3 buckets can be employed to store data and form a data lake on the cloud for fast
writing and data retrieval. The collected data are already more than one terabyte and with
several heart diseases; therefore, Apache Spark (v. 3.1) can be adopted to train the model
in a cluster by utilizing the Spark Gang Scheduler. Our fetched data are from multiple
publications; therefore, the data of each publication will be treated as one department to
train a separate model on the data and then push that trained model to the S3 object store.
In a similar way, a separate model of each publication will be available in the respective
folder on the cloud. Later on, all the trained models for a disease will be consolidated and
merged into one robust model as a federated machine learning model.
The architectural design of the project illustrated in 4 is the logical layout of the
system. AWS S3 buckets are to store the department-trained models, which makes it
easier to keep track of the time travel of each model. Each department has shared to
the GWDG high-performance computing () system, where Spark ML is performed on
the datasets, and the trained models are then pushed to the AWS S3 buckets. This enables federated machine learning, which is a distributed to machine learning.
All the buckets containing the trained models are then to create a voting classifier
model, which produces more robust results. This design allows for the efficient storage,
sharing, and processing of data and models across departments, facilitating collaboration
and improving the overall performance of the system. J. Pers. Med.2023,13, 7907 of 10 4.Architectural design of the project. The architecture represents the logical design of the
project. Each bucket on the AWS S3 buckets service depicts a department-trained model, stores trained models, and makes time travel to each model easier. Every department has shared to
the GWDG system, where spark machine learning conducted on datasets and trained models is
pushed to AWS S3 buckets. This serves the purpose of federated machine learning. All the buckets
with trained models are to make one voting classifier model to have more robust results.
2.6. Challenges
The challenges faced during scraping and crawling publicly available resources are
as follows:
•Website structure:different websites and web pages have different structures, which
make it challenging to extract data in a consistent and reliable manner. This results in
the need to write separate scripts for each website and for its nested web pages.
•Dynamic content:some web pages dynamic content, such as JavaScript, to load or
update their pages. This makes it difficult to scrape the content in its final form, as
the data are generated after the page has loaded. We tackle this problem of dynamic
content through a Selenium bot.
•Data quality:data obtained through scraping were incomplete and contained errors,
making it necessary to validate and clean the data before they could be . J. Pers. Med.2023,13, 7908 of 10
•Incompleteness:data were incomplete, which leads to missing values, inconsistent data,
and a lack of information for certain fields. This made it difficult to clean and prepare
the data for analysis. We resolved the challenge with the help of the rich functions
available for data manipulation in Dask and Pandas.
•Duplicates:data contained duplicates, which lead to errors in analysis and cause issues
when trying to merge data from multiple sources. We leveraged the rich functions in
Pandas and Dask to handle the duplicates.
•Incorrect data types:data can be stored in the wrong format, such as a string when it
should be a date, or a when it should be a categorical variable. This causes
issues when trying to perform data operations. We convert the data types with the
help of the Dask Dataframe functions.
3. Discussion
We scraped the data of 280 publications related to heart diseases from ProteomeX-
change and all its nested databases/webpages. The data come from multiple heterogeneous
sources and with a lot of noise. We Python programming language to crawl and scrape
data and make of Numpy, Pandas, and Dask frameworks to clean the data and for-
mulate them in a format that is ready to prepare a machine learning model. The pipelines
totally replace the huge manual effort needed by a human to make consolidated data. The
pipeline’s running time is in minutes to scrape any of publications, as it runs
in a multi-threaded environment. The importance and benefits of this large and single
repository for proteome data extend to the data that have been scraped from many websites
and research studies are increased data accessibility, improved data quality, enhanced data
analysis, reduced redundancy, and facilitated collaboration. By collecting and compiling
data from a multitude of sources, a comprehensive and diverse dataset is created that
can be utilized by researchers to train and develop machine learning models. The of machine learning models in proteomics has the potential to revolutionize the field by
providing more accurate and precise predictions and classifications of proteome data. The of such models can also lead to the discovery of novel biomarkers and therapeutic
targets for a wide range of diseases. Furthermore, the creation of a single repository for
proteome data scraped from various sources promotes collaboration and the sharing of
resources among researchers, which can lead to breakthroughs and advancements that may
not have been possible otherwise. Overall, a large and single repository for proteome data
scraped from many websites and research studies is an essential resource for researchers
who aim to utilize AI and machine learning in proteomics research.
To the best of our knowledge, this is the first time that a single source or file of pro-
teomics data has been compiled specifically for research on heart diseases. By scraping
data from a wide range of sources and consolidating them into a single repository, we
have created a unique and valuable resource for researchers in this field. This compre-
hensive dataset provides a significant advantage for researchers who previously had to
manually gather and curate proteomics data from multiple sources. With this resource,
researchers can quickly and easily a diverse range of proteomics data related to heart
diseases, which can lead to insights, discoveries, and advancements in the field.
Several proteomics data repositories and multi-omics integration platforms are pub-
licly available for researchers with various purposes and functionalities, e.g., ProteomX-
change [8], the Mass Spectrometry Interactive Virtual Environment (MassIVE, https://
massive.ucsd.edu/ProteoSAFe/static/massive.jsp accessed on 26 April 2023), jPOST [9],
PRoteomics IDEntifications (PRIDE) [10], PeptideAtlas SRM Experiment Library (Pep-
tideAtlas/PASSEL) [11], Panorama Public [12], and iProX [13]. The PRIDE database is
available with the aim to archive various types of proteomics mass spectrometry data for
reproducible research, facilitate protein-centric integration of MS- data for variant and
modification analysis, and furnish MS- expression data to the Expression Atlas [10].
An integrated proteomics data system (IPDS), a data integration platform, is developed to
collect the expanding heterogeneous proteomic data and its relevant information and to J. Pers. Med.2023,13, 7909 of 10
make this information easily accessible to the scientific community [6]. Despite all these and
other databases, to our best knowledge, there is no publicly available dataset/repository
dedicated to a biomedical data integration system that is curated especially for a machine
learning point of view, where all MS files are mapped to its respective patient medical
history without personal information disclosure.
Accessing multiple data sources has long been a study area in the realm of multi-
database and data integration systems. With the advent of cloud databases and large data
processing frameworks, however, the solution has expanded to multistore systems. By
utilizing a shared query engine and, in some circumstances, a data processing framework Spark, these systems provide consolidated to multiple data stores, such as the
Relational Database Management System (RDBMS), NoSQL, and Hadoop Distributed File
System (HDFS). In a nutshell, multistore systems enable seamless to many types of
data sources through the of a common querying and processing , making it
easier to analyze and extract insights from data stored in different forms and locations [6].
Big data technologies- integration systems are created to handle and combine
massive, complicated datasets from various sources, facilitating effective querying and
analysis of various data sources. These systems are vital for large data processing and offer
researchers effective tools for delving into intricate biological processes. Several types of
big data technology- integration systems are available, such as query-optimization- systems (QoX and Estocada), distributed-processing- systems (BigIntegrator
and MyriaDB), hybrid systems (HadoopDB), and schema-free query engines (Apache
Drill). Our system is Apache Spark instead of Hadoop or SQL. This is because
the emergence of big data frameworks has led to the development of several SQL-
query processing systems, including Apache Spark. An open-source engine called Spark is
particularly adept at handling and analyzing massive amounts of data, and it can information from many different sources, including HDFS, OpenStack Swift, Amazon S3,
and Cassandra. Both in memory and disk operations, Spark performs noticeably better
than Hadoop MapReduce [14,15].
In summary, the collection and analysis of proteome data machine learning
has the potential to improve the accuracy of diagnosis and prediction of heart diseases.
These studies demonstrate the potential of already acquired proteome data for the machine
learning purpose to improve our understanding of diseases without need to consume
further precious patient samples to improve the treatment and prevention strategies.
Author Contributions:Conceptualization, A.R., A.Z., M.A. and A.R.A.; writing—original draft
preparation, A.R. and A.Z.; writing—review and editing, A.R., A.Z., M.A. and A.R.A.; method
development—A.R., A.Z. and M.A.; All authors have read and agreed to the published version of
the manuscript.
Funding:This work was carried out at the University Medical Center Göttingen (UMG) and the
Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG) in the scope of
the Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung
BMBF)-funded AI-NET-PROTECT project.
Institutional Review Board Statement:Not applicable.
Informed Consent Statement:Not applicable.
Data Availability Statement:
https://github.com/FutureSceince/Automated-Retrieval-of-Heterogeneous-
Proteomic-Data-for-Machine-Learning (accessed on 26 April 2023).
Acknowledgments:
We acknowledge support by the Open Publication Funds of the Göttin-
gen University.
Conflicts of Interest:The authors declare no conflict of interest. J. Pers. Med.2023,13, 79010 of 10
References
1.Géhin, C.; Holman, S.W. Advances in high-resolution mass spectrometry applied to pharmaceuticals in 2020: A whole age of
information.Anal. Sci. Adv.2021,2, 142–156. [CrossRef]
2.
Zhang, Z.; Chan, P.K.; Richardson, J.; Shah, B. An evaluation of instrument types for mass spectrometry- multi-
analysis of biotherapeutics.mAbs2020,12, 1783062. [CrossRef] [PubMed]
3.
Chen, C.; Hou, J.; Tanner, J.J.; Cheng, J. Bioinformatics Methods for Mass Spectrometry- Proteomics Data Analysis.Int. J.
Mol. Sci.2020,21, 2873. [CrossRef] [PubMed]
4.
Carrillo-Rodriguez, P.; Selheim, F.; Hernandez-Valladares, M. Mass Spectrometry- Proteomics Work-flows in Cancer
Research: The Relevance of Choosing the Right Steps.Cancers2023,15, 555. [CrossRef] [PubMed]
5.Meyer, J.G. Deep learning neural network tools for proteomics.Cell Rep. Methods2021,1, 100003. [CrossRef] [PubMed]
6.
Messaoudi, C.; Fissoune, R.; Badir, H. IPDS: A semantic mediator- system Spark for the integration of heterogeneous
proteomics data sources.Concurr. Comput. Pract. Exper.2021,33, e5814. [CrossRef]
7.
Zia, A.; Aziz, M.; Popa, I.; Khan, S.A.; Hamedani, A.F.; Asif, A.R. Artificial Intelligence- Medical Data Mining.J. Pers. Med.
2022,12, 1359. [CrossRef] [PubMed]
8.
Vizcaíno, J.A.; Deutsch, E.W.; Wang, R.; Csordas, A.; Reisinger, F.; Ríos, D.; Dianes, J.A.; Sun, Z.; Farrah, T.; Bandeira, N.; .
ProteomeXchange provides globally coordinated proteomics data submission and dissemination.Nat. Biotechnol.2014,32,
223–226. [CrossRef] [PubMed]
9.
Moriya, Y.; Kawano, S.; Okuda, S.; Watanabe, Y.; Matsumoto, M.; Takami, T.; Kobayashi, D.; Yamanouchi, Y.; Araki, N.; Yoshizawa,
A.C.; . The jPOST environment: An integrated proteomics data repository and database.Nucleic Acids Res.2019,47,
D1218–D1224. [CrossRef] [PubMed]
10.Perez-Riverol, Y.; Bai, J.; Bandla, C.; García-Seisdedos, D.; Hewapathirana, S.; Kamatchinathan, S.; Kundu, D.J.; Prakash, A.;
Frericks-Zipper, A.; Eisenacher, M.; . The PRIDE database resources in 2022: A hub for mass spectrometry- proteomics
evidences.Nucleic Acids Res.2021,50, D543–D552. [CrossRef] [PubMed]
11.Farrah, T.; Deutsch, E.W.; Kreisberg, R.; Sun, Z.; Campbell, D.S.; Mendoza, L.; Kusebauch, U.; Brusniak, M.-Y.; Hüttenhain, R.;
Schiess, R.; . PASSEL: The PeptideAtlas SRMexperiment library.Proteomics2012,12, 1170–1175. [CrossRef] [PubMed]
12.Sharma, V.; Eckels, J.; Schilling, B.; Ludwig, C.; Jaffe, J.D.; MacCoss, M.J.; MacLean, B. Panorama Public: A Public Repository for
Quantitative Data Sets Processed in Skyline.Mol. Cell. Proteom.2018,17, 1239–1244. [CrossRef] [PubMed]
13.Ma, J.; Chen, T.; Wu, S.; Yang, C.; Bai, M.; Shu, K.; Li, K.; Zhang, G.; Jin, Z.; He, F.; . iProX: An integrated proteome resource.
Nucleic Acids Res.2019,47, D1211–D1217. [CrossRef] [PubMed]
14.Samadi, Y.; Zbakh, M.; Tadonki, C. Performance comparison between Hadoop and Spark frameworks HiBench benchmarks.
Concurr. Comput. Pract. Exper.2018,30, e4367. [CrossRef]
15.Zaharia, M.; Chowdhury, N.M.M.; Franklin, M.; Shenker, S.; Stoica, I.Spark: Cluster Computing with Working Sets; UCB/EECS-
2010-53; University of California, Berkeley: Berkeley, CA, USA, 2010. Available online: http://www2.eecs.berkeley.edu/Pubs/
TechRpts/2010/EECS-2010-53.html (accessed on 10 April 2023).
Disclaimer/Publisher’s Note:
The statements, opinions and data contained in all publications are solely those of the individual
author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
people or property resulting from any ideas, methods, instructions or products referred to in the content. Utilizing Systems to Serve Compute-Intensive
Tasks from a Data Lake Managing Sensitive Data
Dissertation
zur Erlangung des mathematisch-naturwissenschaftlichen Doktorgrades
"Doctor rerum naturalium"
der Georg-August-Universität Göttingen
im Promotionsprogramm Computer Science (PCS)
der Georg-August University School of Science (GAUSS)
vorgelegt von
Hendrik Nolte
aus Göttingen
Göttingen 2024 ii
Betreuungsausschuss
Prof. Dr. Ramin Yahyapour
Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG),
Institut für Informatik, Georg-August-Universität Göttingen
Prof. Dr. Dagmar Krefting
Institut für Medizinische Informatik, Georg-August-Universität Göttingen
Prof. Dr. Martin Uecker
Institut für Diagnostische und Interventionelle Radiologie, Georg-August-Universität
Göttingen
Mitglieder der Prüfungskommission
Referent: Prof. Dr. Ramin Yahyapour
Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG),
Institut für Informatik, Georg-August-Universität Göttingen
Korreferentin: Prof. Dr. Dagmar Krefting
Institut für Medizinische Informatik, Georg-August-Universität Göttingen
Weitere Mitglieder der Prüfungskommission:
Prof. Dr. Martin Uecker
Institut für Diagnostische und Interventionelle Radiologie, Georg-August-Universität
Göttingen
Prof. Dr. Julian Kunkel
Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG),
Institut für Informatik, Georg-August-Universität Göttingen
Prof. Dr. Anne-Christin Hauschild
Institut für Medizinische Informatik, Georg-August-Universität Göttingen
Prof. Dr. Christian Riedel
Institut für Diagnostische und Interventionelle Neuroradiologie, Georg-August-
Universität Göttingen
Tag der mündlichen Prüfung: 4. September 2024 iii
Abstract
Data lakes have been proposed by James Dixon in 2010 to prevent information loss
happening during the extract-transform-load process usually employed to ingest
data into a data warehouse. Instead, he proposed a schema-on-read semantics,
where the schema is only inferred on the data when reading. This simple, yet pow-
erful idea has been drastically extended to support multiple data sources and different ecosystems, Hadoop or public cloud offerings.
Within this thesis, a data lake is designed that integrates sensitive health data and
offloads compute-intensive tasks to a High-Performance Computing () system.
The general applicability is demonstrated through a case where Magnetic Reso-
nance Imaging (MRI) scans are stored, cataloged, and processed. For this, an anal-
ysis of the challenges of data-intensive projects on systems is done. Here, the
requirement of a secure communication layer between a remote data management
system, i.e., the data lake, and the system is identified, alongside the general
problem of data placement and lifecycle management.
Following this analysis the communication layer is implemented and provides a
Representational State Transfer (REST) interface. This system can be completely set
up in space allowing easy scalability and portability. In addition, it provides a
fine-granular, token- permission system. Among other things, this allows to
automatically start uncritical tasks, the execution of an already configured job,
while a highly critical code-ingestion is only possible via an out-of-band confirma-
tion of a second factor. Its generic design enables usage outside of the scope of the
envisioned data lake by basically any client system.
In order to implement the actual data lake, the abstraction of digital objects is
. It allows to organize an otherwise flat hierarchical namespace and abstracts
the underlying infrastructure. This data lake can offload compute-intensive tasks to
an system while providing transparent provenance auditing to ensure main-
tainability, comprehensibility, and support researchers in working reproducibly. It
further provides a modular design, which allows swapping out, or adapting the re-
quired backend services, storage, metadata indexing, and compute.
This modular design is leveraged to guarantee full data sovereignty to the users
by providing the capability to store, index, and process encrypted data. For this, a secure service is developed which provides a secured and isolated parti-
tion on a shared system. This secure partition is completely generic and fully
implemented in software, therefore it can be deployed on any typical system
and does not require any additional procurement. Different encryption techniques
are systematically benchmarked with synthetic and cases where up to 97%
of the baseline performance is achieved. In addition, it can also be as a stan-
dalone service, which is demonstrated by including it in a clinical workflow.
Lastly, scalable and secure metadata indexing is provided by on-demand Elas-
ticsearch clusters which are spawned within the context of a typical job. Two
different encryption techniques are discussed and benchmarked. One is on the iv
previous secure service, while the other is platform-independent by encrypt-
ing all key-value pairs of a document individually on the client side and perform-
ing all metadata operations, search or range queries, on encrypted data on the
server side. Therefore, the encryption keys never have to leave the client system at
any time. To assess the two different encryption techniques, a throughput-oriented
benchmark is developed and on a synthetic dataset and within the MRI case. v
Acknowledgments
I would to thank my supervisor Prof. Dr. Ramin Yahyapour who has guided and
supported me through the entire process of defining a suitable research topic to the
final submission of my thesis. I want to extend this appreciation to my entire Thesis
Advisory Committee, Prof. Dr. Dagmar Krefting and Prof. Dr. Martin Uecker, who
supported me and provided valuable ideas.
I owe my colleagues from GWDG my greatest gratitude for the supportive work-
ing environment without which this work could not have been successful. In par-
ticular, I want to recognize the profound influence of Prof. Dr. Julian Kunkel on
me and my work. I also want to acknowledge the outstanding roles of Prof. Dr.
Philipp Wieder, Dr. Christian Boehme, Dr. Christian Köhler, Dr. Tim Ehlers, and Dr.
Sebastian Krey during my studies.
Writing all the code required for this work, quickly iterating through different
design choices, and evaluating these would not have been possible without the great
help of all the many students I supervised for theses, projects or as student assistants
during this time. Their crucial help is highly acknowledged.
Last but not least I want to thank my family for their endless and empowering
encouragement throughout the entire time. vii
Contents
Abstractiii
Acknowledgementsv
List of Figuresx
List of Tablesxii
List of Abbreviationsxiii
1 Introduction1
1.1From Data Warehouses to Data Lakes . . . . . . . . . . . . . . . . . . .2
1.2Goal of this Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4
1.3Research Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . .5
1.4Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6
1.5Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7
2 Requirement Analysis and System Specification9
2.1Use Case Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9
2.2Requirement Analysis of the Case . . . . . . . . . . . . . . . . . . . 10
2.3Proposed Workflow and System Specification . . . . . . . . . . . . 13
2.4External Review of the Objectives . . . . . . . . . . . . . . . . . . . . . . 16
3 Background and Related Work17
3.1A Brief History of Data Lakes . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2Use Cases from Literature . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.3Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.4Metadata Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.5Provenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3.6Compute Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
3.7Analysis of Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . 30
4 Integrating Compute and Data Management35
4.1Challenges of Data-Intense Projects on Systems . . . . . . . . . . 35
4.2Background on Storage Tiering in . . . . . . . . . . . . . . . . . . 37
4.3Overarching Concept of Integrating Data Management Tools into Workflows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.4Novel Governance-Centric Architecture . . . . . . . . . . . . . . . . . . 44
5 Remote to an System49
5.1Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
5.2Analysis of Possible Designs . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.3General Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.4Advanced Execution Models . . . . . . . . . . . . . . . . . . . . . . . . 56 viii
5.5Security Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.6Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.7Demonstration with Different Cases . . . . . . . . . . . . . . . . . . 67
5.8Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
6 Data Lake Design71
6.1Choosing the Right Abstraction . . . . . . . . . . . . . . . . . . . . . . . 71
6.2Novel DO- Data Lake Design . . . . . . . . . . . . . . . . . . . . . 73
6.3Extending the Design FAIR Digital Objects . . . . . . . . . . . . 77
6.4Example Deployment for the MRI Case . . . . . . . . . . . . . . . . 79
6.5Evaluation of the Proposed Architecture . . . . . . . . . . . . . . . . . . 84
6.6Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
7 SecureHPC: A Workflow Providing a Secure Partition87
7.1Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
7.2General Usage of Systems . . . . . . . . . . . . . . . . . . . . . . . 89
7.3General Design of the Secure Workflow . . . . . . . . . . . . . . . . . . 93
7.4Implementation of the Secure Workflow . . . . . . . . . . . . . . . . . . 96
7.5Extension for Multi-Node Support . . . . . . . . . . . . . . . . . . . . . 99
7.6Security Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
7.7Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
7.8Use Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.9Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
8 Secure Metadata Management117
8.1Background and Related Work . . . . . . . . . . . . . . . . . . . . . . . 118
8.2Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.3Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.4Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
8.5Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
8.6Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
9 Conclusion and Future Work133
9.1Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
9.2Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
Bibliography143
Declaration on the of AI157 ix
List of Figures
1.1Star schema of a data warehouse representing a sale in a store. The
sale is the actual measured observable, whereas the four dimension
tables provide further information about the attributes of the sale. . . .2
1.2Used scientific process adapted from the Design Science Research Pro-
cess [156]. The red steps represent novel adaptions not part of the
original DSRP, while all blue shades represent the original DSRP. The
different blue shades map to the four different design science process
elements of Hevner, i.e. relevance, artifact, evaluation, and communi-
cation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5
2.1BART’s Shepp Logan phantom. . . . . . . . . . . . . . . . . . . . . . . .9
2.2Simplified view of the MRI processing workflow. . . . . . . . . . . . . . 10
2.3Proposed workflow on a data lake. . . . . . . . . . . . . . . . . . . 14
2.4Result of a poll taken during a data lake BoF at ISC22 with more than
30 participants contributing to the questionnaire. . . . . . . . . . . . . . 16
4.1Sketch of the compute-centric paradigm.. . . . . . . . . . . . . . . . . 40
4.2Sketch of the DMS-centric paradigm.. . . . . . . . . . . . . . . . . . . 40
4.3Layer diagram showing the control and data flow of the compute-
centric paradigm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.4Layer diagram showing the control and data flow of the DMS-centric
paradigm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.5Sketch of the governance-centric paradigm. Both groups (compute-
centric and DMS-centric) are orchestrating the data flow through the
additionalGovernance Layer. . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.6High-level view of an experimental description represented by a work-
flow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
5.1Components of the proposed architecture, consisting of clients, an
API server, an systems.. . . . . . . . . . . . . . . . . . . . . . . . 55
5.2Basic schema of the FaaS methodology. . . . . . . . . . . . . . . . . . . 56
5.3Sketch of a Job data structure in which four different functions are
organized. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.4Jobs with implicitly defined dependencies and a custom dependency
DAG. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.5A sketch of the proposed token- authorization flow.. . . . . . . 64
5.6Overview of the levels at which dependencies can be resolved. 69
6.1Sketch of a Digital Object. . . . . . . . . . . . . . . . . . . . . . . . . . . 73
6.2Sketch of the data lake frontend and backends. . . . . . . . . . . . . . . 74
6.3Sketch of an FAIR Digital Object. . . . . . . . . . . . . . . . . . . . . . . 78
6.4Short excerpt from themri_kspace_data.jsonused to define the k-space data type. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 x
6.5Image of the DOT code serialization of the PROV document of one of
BART k-space reconstruction jobs. . . . . . . . . . . . . . . . . . . . . . 83
6.6Example folder structure according to the BIDS standard. . . . . . . . . 84
7.1A simplified sketch of an system.. . . . . . . . . . . . . . . . . . 90
7.2A schematic sketch of the typical workflow for users to submit a job
on an system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
7.3A schematic sketch of the secure workflow on an system, which
is divided into eight distinct steps. . . . . . . . . . . . . . . . . . . . . . 93
7.4Schematic sketch of the parallel starter.. . . . . . . . . . . . . . . . . . 100
7.5Distribution of the individual static overhead measurements of the
secure workflow when compared to the same job executed with the
normal workflow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.6Resulting hypnodensity plot for a single night with values on the x-
axis representing 15s windows. Stages are ”wake” (white), ”Stage 1
sleep” (pink), ”Stage 2 sleep” (turquoise), ”Stage 3/4 sleep” (blue),
”REM sleep” (green). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7.7Distribution of the running times when processing single nights for
sleep stage classification the secure workflow compared to the
insecure workflow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
7.8Excerpt from the Singularity recipe to perform an inverse fast
Fourier transform.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.9Segmented and reconstructed brain scan after the FreeSurfer-
processing is done. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
7.10 Complete clinical workflow from the MRI scanner to the doctors. . . . 114
8.1Geometric mean of the ingestion times on a 3 node cluster via ethernet
without encryption. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
8.2Ingestion time for varying cluster sizes for Ethernet and Omnipath in-
terconnects. The scaling factor is calculated the ethernet-
baseline measurement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
8.3Histogram of the of requests with regard to the full round-
trip latency for the 7-node ethernet cluster the NYC dataset. . . . 126
8.4Timeline of the individual request latency for the seven-node NYC
baseline benchmark with one worker per node.. . . . . . . . . . . . . 127
8.5Timeline of the individual request latency for the seven-node NYC
baseline benchmark with 10k documents response size.. . . . . . . . 127
8.6Geometric means of different Match All query measurements. . . . . . 128 xi
List of Tables
3.1Comparing the presented metadata models. A + indicates full agree-
ment, a 0 shows only partial agreement, and a - indicates that the
corresponding feature is missing.. . . . . . . . . . . . . . . . . . . . . 31
3.2Comparing the presented lineage tracking systems. The auditing col-
umn contains the employed auditing tool, whereas a - indicates that
no built-in system is provided. The protocol provides information
about the communication protocol which transfers the records to the
data lake, and the format states how the information is stored. The
reproducibility column gives a qualitative assessment of the achieved
default reproducibility. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
3.3Comparison of the discussed systems with respect to the in 2.2
presented criteria. A + indicates that the system completely fulfills the
requirement by design, a - that the design contradicts the fulfillment
of the objective, a 0 that it is dependent on the explicit implementa-
tion, and empty spaces indicate that no meaningful assertion is possi-
ble. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.1Comparison of different interaction paradigms. . . . . . . . 43
5.1Qualitative comparison of the different evaluated communication lay-
ers. A + indicates that the requirement is always fulfilled, a 0 indicates
that it is not always fulfilled but might be, and a - states that this re-
quirement is never fulfilled. . . . . . . . . . . . . . . . . . . . . . . . . . 54
5.2Definition of the eight roles. Operations marked in red have to be
considered security critical from the admin point of view, whereas
the orange marked operations from a point of view. . . . . . . . . 65
7.1Partitions and their Members. . . . . . . . . . . . . . . . . . . . . . . . . 97
7.2Comparison of different storage strategies. A ++ indicates the best
solution within a category, a + a sufficient solution, a 0 indicates a
neutral evaluation, whereas a - indicates an explicit lack. . . . . . . . . 102
7.3IO500results onBeeGFS. . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
7.4Results of the IO500 benchmark on an encrypted LUKS container re-
siding in a tmpfs. The specification Encrypted and Unencrypted refers
to the Singularity container. . . . . . . . . . . . . . . . . . . . . . . . . . 106
7.5IO500results on anext4mounted loopback device residing in antmpfs. 106
7.6IO500results of an eCryptFS layer on top of a BeeGFS cluster com-
pared to a native BeeGFS mount. . . . . . . . . . . . . . . . . . . . . . . 107
8.1Geometric means and the first and 99th percentile of the latencies of a
histogram aggregation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
8.2Documents per second for the MRI case with the Ethernet inter-
face.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 xii
8.3Latencies for the MRI custom case with the Ethernet interface in
seconds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 xiii
List of Abbreviations
2FATwo-Factor Authentication
ABEAttribute- Encryption
ACIDAtomicity, Consistency, Isolation, Durability
ACLAccess Control List
APIApplication Programming Interface
BARTBerkeley Advanced Reconstruction Toolbox
BIDSBrain Imaging Data Structure
BIBusiness Intelligence
BLOBBinary Large Objects
BoFBirds-of-a-Feather
CACertificate Authority
CI/CDContinuous Integration and Integration Development
CLICommand Line Interface
CPUCentral Processing Unit
CSVComma Separated Values
CWFRCanonical Workflow Framework for Research
DAGDirected Acyclic Graph
DBDatabase
DCPACData Catalog, Provenance, Control
DICOMDigital Imaging and Communications in Medicine
DMPData Management Plan
DMSData Management System
DOIPDigital Object Interface Protocol
DODigital Object
DSLDomain Specific Language
DSRPDesign Science Research Process
ECGElectrocardiography
EDFEuropean Data Format
EEGElectroencephalography
EMGElectromyography
EOGElectrooculography
ESElasticsearch
ETLExtract-Transform-Load
FAIRFindability, Accessibility, Interoperability, and Resusability
FDOFAIR Digital Object
FMFabric Manager
FUSEFile System in Space
FaaSFunction as a Service
GDPRGeneral Data Protection Regulation
GPFSGeneral Parallel File System
GPUGraphics Processing Unit
GUIGraphical Interface xiv
HDFSHadoop Distributed File System
HIPPAHealth Insurance Portability and Accountability Act
HPCHigh-Performance Computing
HTTPHypertext Transfer Protocol
IPoIBInternet Protocol over Infiniband
IPInternet Protocol
ISCInternational Supercomputing
IaaSInfrastructure as a Service
JITJust-In-Time
JSONJavaScript Object Notation
JVMJava Virtual Machine
KMSKey Management System
LDAPLightweight Directory Protocol
LUKSLinux Unified Key Setup
MPIMessage Passing Interface
MRIMagnetic Resonance Imaging
MVPMinimum Viable Product
NDJSONNewline Delimited JSON
NFSNetwork File System
NICNetwork Interface Cards
NIfTINeuroimaging Informatics Technology Initiative
OAuthOpen-Authorization
OLAPOnline Analytical Processing
OPEOrder-Preserving Symmetric Encryption
OPMOpen Provenance Model
OREOrder Revealing Encryption
PACSPicture Archiving and Communication System
PCOCCPrivate Cloud on a Compute Cluster
PIDPersistent Identifier
PKeyPartition Keys
POSIXPortable Operating System Interface
PSGPolysomnography
PXEPreboot Execution Environment
QoSQuality of Service
RDMARemote Direct Memory REMRapid Eye Movement
RESTRepresentational State Transfer
S3Simple Storage Service
SCPSecure Copy
SDKSoftware Development Kit
SQLStructured Query Language
SSHSecure Shell
SSOSingle Sign-On
SaaSSoftware as a Service
TCPTransmission Control Protocol
TEETrusted Execution Environments
TLSTransport Layer Security
UIUser Interface
UMGUniversity Medical Center Göttingen
URIUnique Resource Identifier
URLUniform Resource Locator xv
VMVirtual Machine
VPNVirtual Private Network
W3CWorld Wide Web Consortium
XMLExtensible Markup Language
XNATExtensible Neuroimaging Archive Toolkit
vFabricVirtual Fabric 1
Chapter 1
Introduction
In the last decades and even centuries, most scientific progress has been made in
a modus operandi, where a basic observation is described by a researcher a
model or theory. Within this newly crafted theory, predictions are made to identify
possible experiments that can be to falsify this theory [160] or to detect anoma-
lies to either improve the existing theory or to provoke a paradigm shift [102]. These
experiments are then conducted to collect data, which is afterward analyzed to deter-
mine if it agrees with the theory. Historically, many experiments could be performed
in laboratory conditions to measure the functional dependency of the required ob-
servables in isolation. This motivates the scientific method, an ever-repeating pro-
cess of designing a hypothesis on an initial question, in which predictions
have to withstand an experimental verification/falsification, ending in further ques-
tions, triggering a execution of this cycle.
However, with the general progress in science, the models in question have be-
come increasingly complex which makes it impossible to isolate the different observ-
ables from each other, which are all influencing the system. This makes it increas-
ingly hard if not near impossible to design suitable experiments to test predictions
derived by the theory in question.
Meanwhile, there have been huge advances in data acquisition, data storage, and
data processing. Nowadays there exists a wealth of sensors that can continuously
monitor almost any observable, ranging from weather information tempera-
ture and pressure to personalized medical data the current heart rate or blood
sugar levels. The cost-effectiveness of most of these sensors has led to widespread
adoption, while the ability to gather data from almost anywhere is continuously in-
creasing along, which is even driven by other trends Internet of Things devices.
This capability to collect vast amounts of diverse data from heterogeneous sources
has coined the term big data, which is primarily defined by the inability to analyze
this data set with traditional methods.
Although working with big data has numerous challenges [80, 120], it has the
potential to revolutionize research, particularly in complex systems, where the his-
toric scientific methods reached their limits. Here, data science has emerged as a scientific field that aims to extract knowledge from these datasets and was
even titled as the fourth paradigm of science [17], next to theory, experiment, and
simulation. This also highlights the outstanding role of compute infrastructure in
modern research, considering the high compute demand from scientific simulations.
These compute-intensive tasks are commonly served with High-Performance Com-
puting () clusters. These supercomputers are to solve extremely compute-
intensive problems that are too large to compute on a typical personal computer. 2Chapter 1. Introduction
1.1 From Data Warehouses to Data Lakes
To enable knowledge extraction from large datasets, also called data mining, proper
data and process management is key. Data management systems are to store,
retrieve, process, and update data, which can then, among other things, be for
scientific hypothesis testing [33]. However, to fully exploit the available wealth of
information and to ease the of data mining techniques, data should be stored
cleaned, and integrated [89].
1.1.1 Data Warehouses
This has led to the widespread adoption of data warehouses, which provide to cleansed and integrated data from different, or ideally all available data sources at
a central location [53, 90]. To do so, a data warehouse requires a pre-defined schema
before any data can be ingested.
Therefore, a thorough process to design a model is necessary to reduce the chance
of misrepresentations later on. Although there was a dispute in science about the
different stages of this design process, it was established that first the conceptual
modeling is done, where entities, their attributes as well as their relationships are
defined. This is followed by logical modeling, and lastly, physical modeling where
the previously defined conceptual modeling is further concretized with respect to
database-specific datatypes [69, 210]. Lastly, the multidimensional modeling is done,
where the previously derived model is split into fact and dimension tables. A fact
is hereby the actual observable, a dimension is an additional information describing
the observable, i.e., the fact.
This introduces a mismatch between the format in which the data are
generated at the source, and the format, which is defined in the data warehouse.
FIGURE1.1: Star schema of a data warehouse repre-
senting a sale in a store. The sale is the actual measured
observable, whereas the four dimension tables provide
further information about the attributes of the sale.
Therefore, data extracted from
a source need to be trans-
formed into the destination
format specified by the data
warehouse.This problem
is generally solved by intro-
ducing a dedicated system to
perform the required Extract-
Transform-Load (ETL) process.
However, this leads to an infor-
mation loss that is happening
during the transformation step,
where any information that is
contained in the raw data but
which is not sufficiently mod-
eled in the data warehouse will
be discarded. One example of
an information loss is an aggre-
gation operation, which decreases the resolution of the raw data along one or multi-
ple dimensions.
This information loss has to be particularly considered in a scientific context,
where the developed model of the data warehouse will be highly influenced by the
specific research question that the researcher is currently investigating. Since it is
possible that the initial research question will be altered, or refined over time due 1.1. From Data Warehouses to Data Lakes3
to insights and a better understanding, this means that the model needs to be
adapted as well. However, if the already ingested data is not available in its raw
format, as it is common in data warehouses due to the ETL processes, this data
cannot be enriched to fit into the updated format. Therefore the reusability of the
already gathered data is lost.
1.1.2 Data Lakes
As a solution to this severe drawback, James Dixon proposed the concept of a data
lake [56]. Here, the schema-on-write of data warehouses and the necessary
ETL processes are substituted by a schema-on-read paradigm. This means that
all ingested data is retained in its native format and only when the data, or a subset
of it, is requested, the data will be transformed into the required format and is passed
to the following process.
Considering the definition of data science as an "open-ended analysis without
preconceived hypothesis" [35], a data lake seems the ideal platform to support
this kind of research since the data in it were not subjected to any cleansing, aggrega-
tion, or similar pre-processing steps which are usually derived from an underlying
scientific theory.
The fundamental idea of data lakes is to retain all raw data in its native format
immediately preventing the usage of any lossy compression techniques. This re-
quirement directly leads to a huge increase in data volume which needs to be stored.
On the other side, the data must not only be stored but also has to be readily avail-
able for subsequent analysis. This results in the necessity for low-cost and scalable
storage and tight integration with similarly scalable compute resources to support
highly parallelized compute tasks.
1.1.3 Integrating Health Data
The life science domain has the potential to benefit drastically from readily avail-
able and integrated health data [180]. For example, by extracting patterns from
different patient monitoring systems and these different input variables, blood pressure, heart rate, temperature, respiratory rate, peripheral capillary oxygen
(SpO2), age, and many more, an early sepsis prediction could be made, which have
proven effectiveness in a randomized clinical trial [181]. This example demonstrates
that by data mining techniques meaningful correlations could be identified,
or observations could be made, which could have not been obtained by doing an
isolated experiment on a living human, or by simulating the entire human body.
Therefore, it is not surprising that the large cloud providers are now increas-
ingly offering special data lake services for personal health record management, IBM’s efforts in a blockchain [151], or Amazon’sHealthLake. The reason, why
novel developments are required when integrating health data into a data lake is that
very high privacy requirements are mandatory by laws such as the Health Insurance
Portability and Accountability Act (HIPPA) in the USA or the General Data Protec-
tion Regulation (GDPR) by the EU. Therefore, platforms to store, index, and process
these sensitive data sets need to be extra hardened to ensure full data sovereignty to
the users.
There is an increasing interest by researchers in universities and other public re-
search institutions to process personalized health data, preferably within their local
data centers. This has led to a widespread quest by data center operators to provide
these highly secured platforms, which is perceived as both, challenging and urgent, 4Chapter 1. Introduction
as mentioned in [44]: “At UC Berkeley, this has become a pressing issue“ and it has
“affected the campus’ ability to recruit a faculty member“.
However, just providing a secure platform to do one isolated task, e.g., process-
ing this sensitive data, is not enough. Instead, an overarching and seamless security
architecture covering the entire data life cycle is mandatory.
1.2 Goal of this Thesis
The overarching goal of this thesis is to design a data lake that can serve as a central
data repository for diverse scientific domains. Its applicability shall be tested within
a case in the life sciences, where data privacy and compute capacity are the main
concerns.
How can a data lake be designed so that it can serve different scientific domains
as the central data repository and processing platform, with a specific focus on
sensitive data?
To answer this primary research question it was split up into different, more de-
tailed, and isolated research questions. These are discussed in the following.
1.2.1 Data Lake Architecture
In the first part, the focus lies on the general development of a generic data lake
architecture that can utilize scalable resources for data storage, data processing, and
metadata management in a modularized manner. The following research question
can be articulated:
What data lake architecture can be to organize a flat-hierarchical data lake
with respect to data maturity and offered functionality that is built in a modu-
larized manner scalable components?
1.2.2 Scientific Support
Since the intended group are scientists the developed architecture needs to be
focused on these specific needs and assist the researchers in good scientific practice.
How can this data lake architecture be made FAIR by design, allowing for the
integration of diverse scientific workflows and enforcing overarching data man-
agement plans across the lake?
1.2.3 Secure Processing of Sensitive Data
One scientific domain that is particularly targeted by this data lake are the life sci-
ences. This can specifically involve working with regulated health data. Therefore,
the highest security standards are required when processing these data sets.
What are the attack vectors to gain to a ’s data while processing them
and how can trusted and secure compute capacity be provided which ensures the
full data sovereignty to the users? 1.3. Research Methodology5
FIGURE1.2: scientific process adapted from the Design Science
Research Process [156]. The red steps represent novel adaptions not
part of the original DSRP, while all blue shades represent the original
DSRP. The different blue shades map to the four different design sci-
ence process elements of Hevner, i.e. relevance, artifact, evaluation,
and communication.
1.2.4 Secure Metadata Handling of Sensitive Data
Since a modularized data lake design is envisioned, one also needs to ensure secure
metadata handling separately.
What are the attack vectors to gain to data stored in a database or search
engine and how can a secure platform be provided that ensures the full data
sovereignty to the users?
1.3 Research Methodology
The general research methodology in this thesis is on theDesign Science
Research Process(DSRP) model, which is formalized by Peffers [156]. This process
consists of six steps: a) problem identification and motivation, b) objectives of a
solution, c) design and development, d) demonstration, e) evaluation, and f) com-
munication. It extends theDesign Science Research Cyclesdescribed by Hevner [86],
consisting of the relevance cycle where the problem or opportunity is described, fol-
lowed by the rigor cycle where existing knowledge is gathered, and is finished by the
design cycle where the actual artifact is designed and evaluated. The DSRP enables
researchers to start at any step before the evaluation, as opposed to the methodology
established by Hevner which can solely start at the relevance cycle.
Although these are well-established scientific methodologies, they only cover the
lifecycle of a single project with a single resulting research artifact. Thus, if the initial
problem identification is done too broadly, the strict application of these scientific
methods can lead to more integral instead of modular solutions. Therefore, there are
efficiency limits when applying these methods to complex problems.
This drawback is accounted for within this thesis by adapting the DSR process
as shown in 1.2. Here, an explicitDecomposition and Encapsulationstep is
included after the definition of theObjectives of a Solutionand again after theDe-
sign and Developmentcycle. The goal of these steps is to identify the possibility of
restricting further the problem and the derived objectives, i.e., to extract a more spe-
cific sub-problem from the original complex problem and solve this the same
adapted DSRP methodology. To be eligible, the expected research artifact of such a
sub-problem needs to fulfill two requirements. First, it can be encapsulated, i.e., the
complexity of it can be hidden behind a well-defined interface. Second, it needs to
solve a general problem, not just a very specific aspect only relevant to this particular 6Chapter 1. Introduction
work. To ensure the latter, an extraProblem Generalizationstep precedes theObjectives
of a Solutionstep, whereas Peffers [156] suggests starting at theObjectives of a Solution
or theDesign and Developmentstep. This deviation from the well-established DSRP
shall maximize the reusability and impact of the work.
This eight-step scientific process is applied in this thesis to design a generic data
lake to integrate sensitive health data. In the following, only the eight steps for the
main process are described, while the identified sub-problems are only mentioned,
but are not recursively listed in detail:
1.Problem Identification and Motivation:At the beginning, a -world case within the life sciences is described, the problem is identified, and the ad-
vantages of a generic data lake for this specific case are motivated. To
clearly identify and define the problem, this case analysis is accompanied
by an extensive literature survey to establish a solid knowledge base.
2.Objectives of a Solution:From the requirement engineering on the case, as well as from the knowledge about the state-of-the-art, a set of objec-
tives for a suitable solution is derived.
3.Decomposition and Encapsulation:A systematic analysis of the required com-
ponents, their hierarchy, and their interactions are done. If a general and iso-
lated component is found, which is not already available, this is to restart
the process at Step 1 with theProblem Generalization.
4.Design and Development:The data lake is designed and developed, on
the previously gathered knowledge and for the defined objectives.
5.Decomposition and Encapsulation:The design and implementation are
checked for isolated sub-components. These can be identified by a well-defined
interface towards other components that might also exhibit a hierarchical rela-
tionship.
6.Demonstration:The applicability of the data lake is demonstrated by it
within the original project.
7.Evaluation: on the demonstration it is evaluated if the data lake fulfills
the prior defined objectives.
8.Communication:The communication is done in the form of this thesis, papers,
posters, and talks.
1.4 Contributions
By applying this research methodology to the stated research questions the follow-
ing contributions are made in this thesis:
• Conducting a requirement analysis for the overarching project in which con- this thesis is written and proposing an envisioned workflow on
a condensed set of functional and non-functional requirements
• Identifying gaps in current data lake research on an extensive litera-
ture survey concerning the integration of ecosystem-agnostic compute infras-
tructure and implementing security measures to manage and process sensitive
health data 1.5. Structure7
• Proposing an overarching concept to integrate different interfaces into
data-intensive workflows to enforce a global data governance
• Developing a secure communication layer to orchestrate the control flow of jobs between systems and externally hosted frontends
• Designing a novel digital object- data lake architecture to provide the
required abstraction of the underlying modularized systems
• Providing a blueprint to integrate scientific workflows within this data
lake which rely on diverse compute infrastructure while still fulfilling the FAIR
principles
• Developing a secure partition on a shared system that enables the pro-
cessing of sensitive data on highly scalable infrastructure
• Proposing two different workflows for providing secure metadata manage-
ment where one uses client-side encryption while the other offers server-side
encryption
1.5 Structure
The remainder of this thesis is structured as follows:
•Chapter 2:First, a requirement analysis is done on an overarching case. on these requirements, a data lake is identified as a suitable data
management system, for which further requirements are defined, which are
tested with a questionnaire towards their generality.
•Chapter 3:General background information about data lakes and their related
work showing the current state of the art is discussed. The general applica-
bility of data lakes for the presented case is confirmed on cases
provided by the literature. Considering the previously defined set of require-
ments different shortcomings of the existing systems are identified. One large
gap in existing techniques is the integration of diverse compute infrastructure
into data lakes, with a complete lack of support.
•Chapter 4:This gap is in detail analyzed, investigating the challenges of data-
intense projects on systems and conceptually proposing a solution. One
specific component that is hereby discovered is a communication layer be-
tween a remote data management system and an system.
•Chapter 5:This specific component is generically designed, so that it can not
only provide the required interface for the envisioned data lake but is generic
enough to work with other data management systems as well.
•Chapter 6: on this work, a modular data lake design is developed ac-
cording to the requirements defined in Chapter 2. To ensure the data privacy
of sensitive data, the data lake design needs to provide the highest security.
To provide this, the modular design is to divide this into smaller sub-
problems.
•Chapter 7:Following this modular design, a secure partition is devel-
oped which allows the processing of sensitive data on shared systems. 8Chapter 1. Introduction
•Chapter 8:Two different methods are developed for secure metadata indexing
to provide a secure data catalog. Both of these systems can be within the
context of the proposed data lake, as well as standalone services. The applica-
bility within the project is verified with a novel benchmarking tool.
•Chapter 9: A conclusion is drawn and an outlook for further research is pro-
vided. 9
Chapter 2
Requirement Analysis and System
Specification
To derive the objectives of a solution, a requirement analysis is done. For this,
the case is described in 2.1. Then in 2.2 a requirement anal-
ysis is done specifically for this project with a focus on re-usability of the created
scientific artifacts beyond this project. on this, in 2.3 a require-
ment specification artifact is modeled as an envisioned workflow, and a
system specification is proposed. Afterward, the gathered objectives are tested
for their general importance by a poll in 2.4.
This thesis is written in the context of a joint project with researchers from the
University Medical Center Göttingen (UMG) within the overarching project "Tap-
ping Into a Resource Hidden Behind MR Images: Learning Quantitative Imaging
Biomarkers from Raw Big Data", which involves 4 departments. Within this project,
only a limited of data sets are recorded specifically for research purposes.
Therefore, these data sets do not have to be classified as health data corresponding
to GDPR. However, the overarching goal is to develop pipelines that can later scale
to very large data sets and can also be for clinical purposes, i.e., for highly
sensitive and regulated health data.
2.1 Case Description
Unlike other medical imaging techniques, x-rays, Magnetic Resonance Imaging
(MRI) scanners do not record images in the well-known image space, i.e., the space
in which the recorded data points directly yield the objects as humans can see them.
Instead, an image is recorded in the k-space or reciprocal space. Hereby, the k-space
andtheimagespaceareconnectedbyaFouriertransformation.
(A) K-Space(B) Image Space
FIGURE2.1: BART’s Shepp Logan phantom.
This relationship is exemplified in Fig-
ure 2.1.In 2.1a,Berke-
ley Advanced Reconstruction Toolbox
(BART)’s [198] Shepp Logan phantom
is shown in k-space.In 2.1b,
the same k-space image is shown re-
constructed by a uniform fast Fourier
transformation in image space.This
shows, that in order for humans to be
able to work with MR images, the ac-
tual recorded k-space image needs to be
processed, i.e., to be reconstructed into
image-space. Typically, not only humans work with these reconstructed images, but 10Chapter 2. Requirement Analysis and System Specification
also analysis tasks, machine learning methods, try to extract features from these
reconstructed images and correlate them to known indicators. However, the recon-
struction process is not exact and leads to information loss. Thus, all processing
done on reconstructed images is subjected to the already happened information loss
and the additional inaccuracies this entails.
Overcoming this information gap between the original k-space and the recon-
structed image is the main goal of this project. This is envisioned to be achieved
by directly extracting relevant imaging biomarkers from the actually recorded MR
signals and including this information back into the reconstruction process. The
expectation of these novel techniques are that they will either improve the image
quality, for instance when compared to the standard reconstructions an MRI scan-
ner produces for medical uses, or that the recording time of a scan can be reduced
while ensuring the same image quality as before. To confirm this expectation, these
FIGURE2.2: Simplified view of the MRI pro-
cessing workflow.
reconstructed images are then further
processed, for instance for multispectral
brain tissue segmentation, a volumetric
analysis of the brain, or a thickness anal-
ysis of the brain. These pipelines can
then both input data, the novel im-
age reconstruction, and the standard re-
construction from the scanners. A sim-
plified view of the processing workflow
is shown in 2.2. It is shown, that
from the MRI scanner, both the k-space
images as well as the reconstructed images are extracted. The k-space images need
one more reconstruction step before both reconstructions can be processed with the
same processing pipeline. In order to obtain meaningful statistics this depicted
workflow should be applied to large data sets. Although only a limited of MRI scans are planned within the scope of this project, the developed pipeline
should be scalable to also work with larger cohorts, the UK Bio Bank [188].
Within this project, the recorded MR images should be archived in a suitable data
repository, not only to enable data sharing between all involved researchers within
this project, but also to provide to this data to other scientists as well. This
should increase the reusability of the recorded data and ensure its impact beyond
the scope of this single project. Therefore, requirements can come up in the
future which are hard to foresee. Thus, both the data repository as well as the pro-
cessing infrastructure should support cases and should not be too narrowly
designed for these specific requirements at hand.
2.2 Requirement Analysis of the Case
From the original proposal of the described project, as well as many discussions
with the involved researchers about their current practice and the envisioned goal,
a set of functional and non-functional requirements could be identified. The main,
high-level goal is to provide a data and process management system for this project.
Data RepositoryDoing an MRI scan is very time-consuming and costly. Therefore,
the measured raw data has high value and should be stored in a data repository,
where the findability, accessibility, and integrity of the data can be ensured. This
means specifically that raw data needs to be retained and must not be deleted or 2.2. Requirement Analysis of the Case11
overwritten. In addition, it is important to collect and link the related metadata,
particularly for file formats that do not carry any metadata themselves but rely on
sidecar files. The metadata should be indexed to yield an overarching data catalog
to allow researchers to select the data of interest on domain-specific, semantic
queries. This data repository should not only support raw data but should also
integrate resulting artifacts. This is for instance required to be able to compare the
different results of the same scan at the end of the pipeline, i.e., the ones which got
reconstructed by BART versus those reconstructed by the scanner. This also allows
to track and compare differences due to different hyperparameters.
Another requirement for the data repository is that it can be built up over time,
without the need to have the full processing pipeline at the beginning. Since do-
ing MRI scans is very time-consuming, they are done over a longer period. In
the meantime, the involved researchers can continuously work on novel processing
techniques and improve existing ones.
PerformanceEach of the described processing steps is highly compute intensive
and therefore requires scalable and performant compute clusters. In addition, some
of the involved researchers already have extensive experience, and highly so-
phisticated pipelines implemented to efficiently process MR images on these sys-
tems. Thus, integration is a key requirement for this project, both from an
efficiency and a usability perspective. However, processing large data sets, particu-
larly with iterative methods, as they are often in machine learning, is not only
compute but also data-intensive. Therefore, the available storage tiers need to be
efficiently .
ScalabilityTo serve this big data scenario, it is not only enough to consider the
performance of a single node, or a single task, but one needs to ensure scalability as
well. This concerns all involved parts, i.e., the processing infrastructure, and the data
repository, which combines storage capacity and metadata management. However,
not only the capability to scale out is important, but also the cost-effectiveness when
doing so.
GeneralityEven from the very start of this project, four different departments are
involved, and further collaborations can be anticipated. Each of these groups, and
the individual researchers within, have different ways of working, different data,
and different tools, which entails different needs and foci. This diversity makes it
impossible to simply deploy one of the many data management systems available,
which are tailor-made with regard to a very specific target audience, and therefore
contain different preconceived assumptions about the later usage. Even if a system
can be found that fulfills the current objectives, it might not work for future collabo-
rations. Typically, this problem leads over time to a sprawl of isolated data silos, i.e.,
data management systems that are only accessible and usable for specific cases
and groups, thus limiting the reusability of their data and the collaborations
between different research teams. Overcoming this gap and providing a unified
platform to enable all researchers to collaboratively work on the overarching work-
flow is one main target of this project. Therefore, a generic system is required, which
makes no assumptions about the data, or the processes at the design level.
Data ModelingWhen integrating data into a data repository some metadata mod-
eling is required. Otherwise one can quickly lose the maintainability. One simple 12Chapter 2. Requirement Analysis and System Specification
example to highlight the importance of data modeling are the image-space MR im-
ages within this project. These can be an artifact created by BART or could be re-
constructions made by the MRI scanner, which can be interpreted as some kind of
raw data. It is of utmost importance to differentiate between these two data types,
although both will be for further processing in the same data format, will show the
same subject, and share the same semantic metadata. The only difference will be in
the data lineage.
The data modeling step should be quite generic, and should not be done with one
single scenario or data flow in mind. This can be further substantiated by the view,
that not only the k-space images can be considered as raw data, but also the scanner
reconstructions. Therefore, data modeling is required which is flexible enough to
map those scenarios into an intuitive schema.
Continuous Integration into Scientific WorkflowsAs already motivated, there
are a range of workflows that scientists execute as part of their research. In some
simple cases, raw data is extracted from a single source and is then automatically
processed by a few tasks. On a high level, this is the case in the motivating example
shown in 2.2. However, the above-stated goal is to be generic without any
preconceived assumptions. Therefore, the possibility for more difficult workflows,
where tasks are not executed in a strict linear order but can be modeled in a directed
acyclic graph to allow for task parallelism, should be accounted for from the begin-
ning. In other cases, not all steps are necessarily predefined automated processing
tasks but some manual interaction might be required at certain points. Lastly, there
might also be some further analysis tasks that are physically performed in a lab. All
of these diverse scientific scenarios are possible workflows that need to be able to be
integrated into a single data lake to serve as the central data repository. This means
a data lake needs to be continuously adaptable to integrate scientific workflows
in order to provide a central data repository with integrated data.
It is very common, but also specifically within this project, that not all steps of
a scientific workflow are done by a single researcher. Therefore, data sharing is
important to enable and support the envisioned collaborations.
Data Lifecycle ManagementThe successful integration of scientific workflows can
further be to support researchers to follow good scientific practice, which can
be done with comprehensive data lifecycle management. This is comprised of ac-
tionable information about the duration over which the data has to be stored, or
what a sufficient data and metadata quality is within these scientific workflows.
One particular focus within the metadata quality is the provenance information. A
proper data lineage will allow for retrospective comprehensibility and reproducibil-
ity, which is currently a major challenge in science and was even called a "Repro-
ducibility Crisis" [8]. Considering the processing of big data on systems also
proper storage allocation across the available storage tiers becomes more important.
Data should always be backed up on cheap and scalable mass storage with high
durability, while only hot data should reside on fast and expensive filesystems closer
to the compute clusters.
FAIR DataOne increasingly important aspect of data management is theFAIRness
[209], i.e., how much the practiced data management adheres to basic principles re-
garding the Findability, Accessibility, Interoperability, and Reusability (FAIR). These
key points are subdivided into 15 high-level criteria. The main goal is to enable 2.3. Proposed Workflow and System Specification13
machine-actionability, however, the FAIR principles also ensure sustainable data
management which enables the ability of third parties to find and reuse already
existing data. These aspects are an increasingly important factor for funding and
publishing research, therefore scientists want to be compliant, but also want and
need to focus on the actual research. Therefore, strictly following the FAIR princi-
ples in a project from the beginning can be perceived as a distraction from the actual
work. In addition, these principles are not limited to the data management itself but
also extend to the entire infrastructure. Therefore, to fulfill the FAIR principles an
entire ecosystem that supports and eases the burden of researchers to individually
come up with solutions to this generic problem is required.
ModularityFrom a very abstract view, the envisioned data repository, i.e., the data
and process management system has to integrate three basic services: storage, meta-
data management, and compute capabilities. on these services, others may
be necessary, e.g., for governance or version control, or to more domain-
specific services. Since all of these services are subject to change due to technological
progress or domain-specific requirements, a modular design is advantageous which
will not only allow for necessary substitutions but will also help to achieve the en-
visioned generality. This will in the end enable researchers to overcome data silos,
which are prohibiting cross-silo analysis. A modular design is also specifically re-
quired for this specific MRI case, since small data sets should be processed in a
cloud environment, whereas larger data sets should be processed on an system,
according to the original project proposal.
SecurityOne particular challenge is the management and the processing of sensi-
tive data. Specifically for MR images, it is not possible to anonymize the data. For
example, an MRI scan showing the head of a person shows her/his face and is there-
fore immediately personalized. Although there exists the possibility to artificially
cut out the face, so-called de-facing, this is not possible for k-space data, which also
contains this personalized information. For some other processing tasks, de-facing
might be unfeasible because it introduces artificial asymmetries that impact the ac-
curacy of the results. Therefore one needs to ensure for these data sets that the full
data sovereignty stays with the during the entire data lifecycle. This means,
that no illegitimate users, attackers, or even admins should get to this kind of
data at any time.
2.3 Proposed Workflow and System Specification on the previously made requirement analysis and the identified key aspects,
a suitable system needs to be found that fulfills these defined requirements. Due to
the rather diverse groups and the intent to be able to expand and include even
more researchers and cases, a highly specific, purpose-built data management
system can be excluded. Instead, the envisioned target system should be able to
integrate all kinds of data in a similar fashion. A data warehouse can be rejected
due to the high commitment of storing the raw data. Instead, a data management
system is required that can be to build up a research data repository over time,
without the need to have the full processing pipeline in place at the beginning. Thus,
a data lake is proposed as a suitable candidate for this intended case, since it is
generally generic and retains raw data in its original format. 14Chapter 2. Requirement Analysis and System Specification
FIGURE2.3: Proposed workflow on a data lake.
In order to further derive a suitable data lake design, which fulfills the in Sec-
tion 2.2 defined objectives, a generic and high-level workflow is described
which models the requirement specification artifact. From this description, further
technical and conceptual requirements and features can be derived. On a coarse-
grained view, there are three different stages, i.e.Ingestion,Analysis, andPublication.
Those three stages can contain a total of six distinct tasks, yielding the proposed,
overarching workflow depicted in 2.3.
The first stage is the data ingestion. Here, within the general philosophy of a
data lake, the raw data is uploaded to the data lake. Since a data lake is envisioned
that supports multiple data sources and serves as the central data repository, proper
metadata management and data cataloging are key to ensuring maintainability [57].
Therefore, at least a minimal set of metadata needs to be extracted from the raw
data. The integration of different data sources is required since even from the very
beginning the k-space data and the scanner reconstructions can already be consid-
ered as two data sources, and it is mentioned by the researchers that more could be
required in the future Electroencephalography (EEG) data. Once the raw data
from diverse sources are integrated into this single data lake, further analysis can be
done. This can contain a manual data exploration step, where users can explore the
available data sets within an overarching data catalog, and can refine their queries
to select the desired subset. Once a suitable subset of the available data is found,
the actual processing can start. These processing tasks then create again some data
products, also called artifacts, which should then be reingested into the data lake.
Particularly when ingesting artifacts into the data lake, it is key to properly record
the lineage to ensure retrospective comprehensibility and ideally even reproducibil-
ity. These newly ingested artifacts can now again be as input data for consec-
utive processing tasks, thereby enabling the execution of workflows. Once a final
product is derived, researchers usually want to publish their results. Assuming that
researchers sometimes have to run the same analysis multiple times with slight devi-
ations, the publication step requires the ability to concisely select the desired result,
and probably also the concise workflow to create this artifact.
In order to enable this envisioned workflow when working on a data lake,
a set of requirements can be derived:
Data Generation:Not only when ingesting data from different sources, but also
when ingesting data from a single source, the capability for provenance recording
might already be required to support an overarching scientific workflow. For in-
stance, when performing different measurements with the same instrument, it is
important to link the resulting raw data against the experimental protocol, e.g. in
the form of an externally managed electronic lab notebook, automatically written
metadata, or in the form of explicit annotations.
Data Upload:In order to provide easy to the data lake from any client, it
is important to support common communication protocols, ideally even multiples. 2.3. Proposed Workflow and System Specification15
This is necessary, since and the capability to install required software on a
workstation in a lab, or a hospital, can be very limited.
Metadata Indexing:Structured indexing of metadata generally requires dedicated
data modeling. The aim of data modeling in the concept of data lakes in general
is to ensure the findability, quality, and comprehensibility of the data, especially
for unstructured data, within the lake. Since no fixed schema should be enforced,
it needs to support continuous updates and a multitude of different data models.
It should support domain-specific, -defined attributes and should, if desired,
support the of an ontology, or thesaurus.
Data Exploration:This step can be by users to get a better understanding
of the available datasets. The particular challenge here is to provide users with a
good overview, even in the context of big data. This requires, that users are able to
inspect data and data sets in different granularities, or to be more colloquially: users
should be able to zoom in and out of the data. For this, views providing overarching
information about a data set, for instance by aggregations, can also be .
Data Processing:The envisioned data lake should not only provide large and in-
tegrated data sets but should also offer scalable compute capacity. These compute
resources should not be limited to a single ecosystem that can serve the previously
described case, but the integration should support different systems, cloud
infrastructure for smaller tasks, or systems to support even the most compute-
intensive tasks. This will also ensure the capability to support future cases with
different compute requirements. Input data should either be selected by users di-
rectly, or indirectly via domain-specific queries. Particularly for experienced users,
the system and the provided software should be usable with minor interac-
tions with the data lake, while inexperienced users should be offered a higher de-
gree of automation, particularly with respect to provenance auditing. Proper lineage
recording should be guided to enhance good scientific practice as well as the main-
tainability of the data lake.
Data Publication:Since reproducibility is an increasingly important topic during
the publication process at journals or conferences, open data including any code and
intermediate results, sometimes even along with a reproducible workflow, are either
on a mandatory or a voluntary basis asked for. Depending on the venue and the
domain, various specific data formats and accompanying documents are required.
Therefore, data, artifact, and workflow selections should provide these datasets in
an interoperable format, which can be easily ported to other formats, research
objects [36] orBagIt[104]. However, being able to extract data for publication isn’t
enough, since it is usually required to keep data for at least 10 years archived.
Data Lifecycle:All of the aforementioned steps are part of the overall data lifecycle
and therefore all of these steps combined yield the overarching data management.
That means, for instance, each of these steps needs to support the FAIR principles. 16Chapter 2. Requirement Analysis and System Specification
FIGURE2.4: Result of a poll taken during a data lake BoF at ISC22
with more than 30 participants contributing to the questionnaire.
2.4 External Review of the Objectives
Until this point, the entire analysis with regard to these requirements is driven by
this single "Big Data MRI"-project. However, one of the identified objectives is gener-
ality, meaning that the developed data lake should be able to extend its scope to cater
to scientific projects. In order to prevent a too narrow view on this topic, the
10 objectives described in 2.2 are simplified and concretized with regard to
their underlying feature. Additional possible attributes, the cost, are also added.
Then, these features were included in a questionnaire that was given to the audience
of the Birds-of-a-Feather (BoF) session "Large-Scale Data Management with Data
Lakes" at the International Supercomputing (ISC) in 2022 which counted about 50
participants among which more than 30 actively participated in the questionnaire.
The audience can be assumed to be very international and consists of mostly admins and only a few users. Among other things, the audience was asked to
rank the importance of the identified data lake features. Each participant had a max-
imum of 5 votes to distribute over 16 different features, see 2.4. In total 151
votes were counted, ranking data sharing and security as the most important fea-
tures, while private cloud support was ranked the least important, which was one
of the key requirements of the MRI project. Generally, the result is in good agree-
ment with the objectives defined in 2.2. Interestingly, the total cost, advanced
metadata handling, and cloud support have been deemed less important. In addi-
tion, the need for interactive data has only been identified for hot data within
the project but is felt generally more urgent by the audience of the BoF. In general,
one can also see, that the capability to model the available data was considered very
important, considering the of votes fromRich Description of Metadata,Search
Capabilities, andMonitoring of Reproducibility. From the top 10 voted features only the
Interactive Data Accesshas not been explicitly stated as an objective. Considering the
high participation and voting rate, one can generally recognize this as an agreement
of the participants with the suggested features. 17
Chapter 3
Background and Related Work
After a data lake is identified in the previous chapter as a suitable platform
for data and process management, further analysis of the applicability of these
systems for the particular case is necessary. For this, further background
information about data lakes is provided in 3.1, followed by an in-depth
literature review to determine the current state of the art and establish a solid
knowledge base. Different cases are analyzed in 3.2 to compare the
defined objectives and intended case, with established systems. Then, the
four identified key properties of data lakes are investigated, i.e., the available
architectures in 3.3, the different metadata modeling techniques in Sec-
tion 3.4, the provenance auditing mechanisms in 3.5, and lastly the
compute integration in 3.6. This chapter concludes with an analysis of
the different discussed systems in 3.7. Parts of this chapter are published
in [208].
Data lakes have already been researched for over a decade since their first formal
introduction by James Dixon in 2010 [56]. During this time, extensive work has been
done on top of the original idea to retain all raw data in its native format. This has
led to a vivid evolution of data lakes, including their definition and requirements.
For instance, in 2013Pivotal[32] presented a business data lake aimed to substitute
the commonly utilized enterprise data warehouses. They proposed to a data
lake that is divided into multiple tiers and receives data from multiple sources. It
was also recognized, that in order to provide a unified view and unified data man-
agement capabilities, that metadata has to be maintained in a data catalog. This is an
extension of the original proposal from Dixon [56] who has originally only argued
with a single data source. The importance of proper semantic metadata management
was also highlighted a year later by Gartner, saying that "Without descriptive meta-
data and a mechanism to maintain it, the data lake risks turning into a data swamp"
[64]. They also recognized the particular importance of lineage recording when in-
gesting artifacts. In 2014 this criticism caused James Dixon to revisit his original idea
and address these concerns [57]. He defended the single data source, stating that one
should have multiple data lakes for multiple data sources. Integrating data from dif-
ferent sources would yield a "Water Garden", not a data lake. However, he agreed
that extending the concept to multiple data sources is easily possible, but would
require proper metadata management. Dixon also highlights that no assumptions
during the ingest about the data must be made.
3.1 A Brief History of Data Lakes
At the beginning, there was a high association between Hadoop and data lakes and
was at times even interchangeably [98]. At first, this connection was probably 18Chapter 3. Background and Related Work
formed by the timing of the original announcement after the Hadoop world in 2010
[56]. But Hadoop also offered horizontal scalability not only for storage but also for
compute. It also provided an integrated way to define parallel tasks, e.g., to infer
a schema on raw data, or for further analytical processing. to a Structured
Query Language (SQL) interface on a Hadoop cluster was even more stream-
lined with the evolution of the entire ecosystem around Hadoop, the integration
of Spark [215], or Hive [195].
Although it was not explicitly stated, this implemented a flat names-
pace, as it did not infer a conceptual hierarchical order on the stored data. This
ensured a high degree of data integration since they were homogeneously acces-
sible from a unified interface. The envisioned practice of performing all process-
ing steps on-demand, including some commonly shared preprocessing, has led to a
state where some processing tasks were repeated multiple times. Christian Mathis
challenged this consent, pointing out the wastefulness of recomputing particularly
intermediate results [121]. He suggested that users of data lakes should carefully
consider whether recomputing a result or storing an already transformed data set in
a more structured format, a parquet file, a connector, Hive, or pushing
it into an external database altogether, is more efficient.
This idea was further developed into theZone Architecture. Here, the data lake as
the central data repository, is split up into different, separated zones. The basic idea
is that data is sorted into these distinct zones on the pedigree of processing
they were subjected to, e.g., if the data is raw, cleansed, or aggregated. Although
a multitude of different systems are proposed [70, 118, 153, 165, 178, 219], they all
share a similar . They all have aRaw Data Zone, where unprocessed data
is retained in its original format. Pre-processed data is collected in a separateprepro-
cessed,refined, orstagingdata zone. on the intended purpose of the data lake
the exact and usage of zones differs from case to case to adequately accom-
modate potentially diversely processed data. This means, that each zone might have
an independent and unique data governance, general rights, or quality
controls. Depending on the individual implementation of a data lake, which is built the zone architecture, these zones can be realized different systems, both
in hardware and software. Therefore, the implementation and maintenance expense
for system administrators is increased, and similarly, also the complexity for data
scientists in their daily usage since data is scattered across different systems, poten-
tially requiring different methods due to different software stacks and pro-
viding different capacities, both in storage and compute. Thus, this evolution of
data lake architectures resulted in more heterogeneous data lakes compared to the
simpler Hadoop before.
The following stage of the architectural evolution of data lakes was shaped by
large, public cloud providers who offered proprietary but integrated solutions. Ba-
sically, every Cloud provider had their own, locked-in offering, including IBM [41],
Microsoft Azure [163], AWS or Google. Although each offering consists of its own
proprietary software stacks, the general is still similar. The central con-
tact point is a scalable, and cost-effective cloud storage, Amazon Simple Storage
Service (S3), which is an object store, offering via a Representational State
Transfer (REST) Application Programming Interface (API). This cloud storage ser-
vice is then extended by a database, where all objects, i.e., data files, are cataloged.
In order to ease the data ingest from diverse sources and data staging into differ-
ent compute services to perform advanced analytics on the data sets, dedicated data
transfer tools are . Last but not least, a governance service can be to en-
force different policies, control lists, homogeneously across all involved 3.2. Cases from Literature19
services. Since these different vendor-specific tools offer a high and seamless inter-
operability within each ecosystem, this has homogenized again the resulting data
lake architectures. Additionally, it brought down the cost and led to an increasing
adoption, particularly by large companies. on this recap one can see that the architecture, i.e., how a data lake is con-
ceptually built and organized is key in determining its features and interactions.
In addition, metadata modeling was also very early recognized as a mandatory re-
quirement when integrating data from multiple, possibly diverse data sources. One
particularly important aspect of metadata modeling is the provenance, which is also
related to the overall integration of computing capacity. As motivated by the history
of data lakes, the main differences in the development of data lakes were done in
these four areas. The last three of these topics also match well with the objectives
described in 2.2 and the results of the questionnaire discussed in 2.4.
Therefore these four aspects are in detail examined in the following literature review.
Afterward, the results are analyzed towards their applicability of the in 2.2
defined objectives.
3.2 Cases from Literature
Data lakes were early on adapted by the industry to support their Business Intelli-
gence (BI). An exploratory study among 12 experts on how to implement data lakes
in various enterprises found that data lakes are primarily as a staging area for a
data warehouse, as an archive for a data warehouse, as a sandbox for data scientists
doing advanced analytics on a local workstation, and as a data source for direct re-
porting [112]. The main benefits are hereby supposedly the speed at which one can
ingest raw data since many people just "dump all the data in there" and the to raw, unaltered data [112]. In these cases, common challenges were a lack of
data stewardship, data governance particularly for confidential data, data quality,
and the expertise of the users since a data lake is "complex to understand". Similarly
in [189] a Hadoop- data lake is presented for BI, which is as a staging area
for a data warehouse. In addition, it is also for unstructured data which will
not be loaded into the data warehouse. Instead, data analysts the data lake
directly for advanced analytics.
In contrast to these BI cases, Rihan Hai describes a scientific case from the
Internet of Production, which serves as a foundation for research and development,
instead of supporting decision-making [74]. Here, three different milling scientists
are working in different labs with different milling machines which have different
sensors and therefore create different output data. Although they all have differ-
ent research questions and therefore different analysis tasks, they want to be able
to compare their data. Therefore they need to be able to find related data sets to
the ones they have individually ingested and which they are working on. In addi-
tion, they all create per-milling machine diverse data, pressure and temperature
data, or video streams, which need to be integrated to allow for easy ad-hoc .
Retaining raw data is key to enabling continuous support amid changing research
questions.
This last example clearly motivates how a data lake can be to provide a
platform for researchers to collaborate, but also shows that it can be by each
scientist as a data repository. The described scenario in [74] can be transposed from
milling machines to MRI scanners since there are certain parallels to the case
described in Chapter 2. For instance, there will be also slightly different data which 20Chapter 3. Background and Related Work
shall be analyzed and compared to each other, e.g., the novel reconstructions to the
standard scanner reconstruction, similar to the different data sets created by the dif-
ferent milling machines. In addition, the inclusion of different MRI scanners in the
future might also be possible. The successful of a data lake in the case de-
scribed in [74] can be considered as a confirmation that a data lake is a suitable plat-
form for scientists who want to collaboratively work on joint workflows and data
sets.
3.3 Architectures
As of today, a lot of development and analysis was conducted in the area ofdata
lake architectures, where the so-calledzone architecture[153, 165], including thepond
architecture[88], became the most cited and . These architectures have already
been surveyed [74, 167], and afunctionalarchitecture is proposed by both of them,
while amaturityand ahybridarchitecture are only discussed in [167]. These surveys,
however, did not include recent works the definition of a zone reference model
[66].
3.3.1 Definition of the Term Data Lake Architecture
The term data lake architecture is defined to represent the comprehensive design of
a data lake, including the infrastructure, data storage, data flow, data modeling, data
organization, data processes, metadata management, data security and privacy, and
data quality [65]. In this data lake architecture framework, only the data security
and privacy, and the data quality are considered to be purely conceptual, whereas
the other aspects include a conceptual and a physical, i.e., system specific, dimen-
sion. As stated in [117] more generically, a data lake generally has a logical and
a physical organization. In this work, the termdata lake architectureis referred to
only with respect to the conceptual organization of a data lake in the highest level of
abstraction, since this should make this work more comparable to the existing liter-
ature, although there exists a strong dependency on other aspects of a data lake, the metadata modeling.
3.3.2 Zone Architecture
The general idea to divide a data lake into different zones arises from the necessity to
automatically run standardized pre-processing pipelines and avoid to re-run these
tasks on every data [121]. Therefore, one keeps and organizes the resulting
pre-processed data, and makes it available to subsequent processing steps, re-
porting,Online Analytical Processing(OLAP), and particularly advanced analytics.
This is achieved by assigning data to different zones on the degree of process-
ing, and sometimes the intended future case. Therefore, it is common to have
araw data zone, where, according to the original idea of a data lake, data is retained
in its native format. Pre-processed data is then usually collected in a dedicated zone
forpre-processed, orrefineddata, sometimes calledstaging zone[219] orprocessing zone
[165]. Data that requires additional governance can be collected in a dedicated zone,
calledtrusted zone[219], orsensitive zone[70].
The most extensive analysis of thezone architectureanalyzes five different data
lakes [70, 118, 153, 165, 178, 219] with respect to their design differences, specific
features, and their individual cases in order to derive a generic meta-model for
a zone, and to specify a zone reference model on it [66]. It is identified that a 3.3. Architectures21
zone is uniquely defined by the characteristics of the data contained in that zone, the
intrinsic properties a zone enforces on the data, the groups that are intended to
work in that zone, the modeling to organize the corresponding metadata,
and the data sources as well as destinations. In the presented zone reference model,
Giebler . propose to split the zones up in araw zoneand aharmonized zone,
which is case independent, and a case-specificdistilled zone, which serves
data to the finaldelivery zone, to support reporting and OLAP tasks, and aexplorative
zoneto support advanced analytics. Each zone hosts aprotectedarea for data that
requires special governance. The actual implementation and the deployed systems
can vary in each zone, including the storage, the metadata model, and the metadata
management system itself. This entails synchronous, that also the interface
potentially changes with each zone.
3.3.3 Lambda Architecture
TheLambda Architectureis proposed to enhance the capability of a data lake to pro-
cess data streams in near -time instead of fully ingesting hot data into the data
lake and performing batch-processing with a certain time delay [121]. However, re-
taining all raw data in its native format is the core idea of a data lake. In order to
resolve this contradiction, the Lambda Architecture [206] implements two process-
ing streams in parallel. Here, data is being processed in near -time in thespeed
layer, whereas thebatch layeringests data into the data lake and performs some pre-
defined processing steps. There have been numerous implementations proposed for
a data lake utilizing the Lambda Architecture, particularly at the early stages of the
data lake research [9, 82, 200]. However, the following two particular works are
presented which are building on top of public cloud offerings and represent more
state-of-the-art systems.
A lambda architecture is by [135] to build a data lake for smart grid data
analytics Google’s cloud computing as Infrastructure as a Service (IaaS). Here,
the data is collected by a dedicatedData Collecting Layer, in this particular case re-
alized by ApacheFlume
1
. From there, the data is sent to the core of this specific
data lake implementation, a Hadoop Cluster. The master node stores the data on the
Hadoop Distributed File System (HDFS) [28], and computes arbitrary, predefined
functions MapReduce [52]. The speed layer is implemented usingApache Spark
[216]. The serving layer combines the output of the batch and the speed layer and
provides a batch view of the relevant data, e.g.,Hive[196],Impalaas shown by
[110], andSpark SQL[6].
Another work compares three different implementations the Software as a
Service (SaaS) offerings with a focus on serverless delivery of three different public
cloud providers, i.e., Google Cloud Platform, Microsoft Azure, and Amazon Web
Services Cloud [157]. On AWS the speed layer accepts data via Kinesis Data Streams
and processes them Kinesis Analytics and AWS Lambda. The results are
stored in a dedicatedS3-Speed-Bucket. Similarly, the batch layer uses Kinesis Fire-
hose to ingest data into AWS Lambda, from where it is stored in anS3-Batch-Bucket.
From here the data is read by AWS Glue and stored in anS3-Result-Bucket. The serv-
ing layer is realized by Athena which reads the data from both, theS3-Result-Bucket
and theS3-Speed-Bucket. In the Google Cloud implementation, data is ingested by
Pub/Sub to the speed and the batch layer, which are both realized Dataflow.
On the batch layer, an additional Datastore is employed to retain the raw incoming
1
https://flume.apache.org/ 22Chapter 3. Background and Related Work
datasets. The serving layer uses BigQuery. On Microsoft Azure, data is received and
ingested by EventHub. The speed layer uses Stream Analytics and forwards directly
into the Serving Layer which is Cosmos DB. The batch layer also uses Stream Ana-
lytics to store the raw data into Data Lake Store [163]. From there it is read by Data
lake Analytics, which also stores its results in Cosmos DB.
3.3.4 Lakehouse
Lakehousesare a consequence of the general observation that in some cases the raw
data from a data lake is as an input for an ETL process to populate a data ware-
house [5]. The first step into a more unified setup is proposed byDelta lakes[4],
which provides ACID (Atomicity, Consistency, Isolation, Durability) transactions on
cloud object storage for table stores. These tables can be accessed from different
systems, Spark, Hive, or Presto [177]. This introduces, among other
things, the advantage of still separating storage and compute. Lakehouses offer on
top of ACID transactions direct to the storage with traditional database se-
mantics, e.g., SQL, open file formats likeApache Parquet[201] orORC
2
. There-
fore, a metadata layer on top of the cloud storage can provide convenient SQL- to tables, while compute-intensive, non-SQL code, machine learning, can
directly the files on the storage devices and thereby get higher performance.
3.3.5 Functional and Maturity- Architectures
The classification intofunctionalandmaturity-oriented data lake architectures, unlike
in the case of the zone, lambda, and lakehouse, do not represent yet another design
concept, but rather serve as an improved way for classifying the different architec-
tural approaches. The goal is to allow for a more modular comparison of existing
data lake solutions and to better plan the data life-cycle as well as to help match the
individual functionality of the architectural pieces, which are building up the data
lake, zones, on the required infrastructure.
Within afunctional- architecture classification, the data lake is analyzed to-
wards its operations which are performed on the data while moving through the
general data lake workflow. These architectures are represented in layers, where a
three-layer design consisting ofingestion,maintenance, andexplorationis proposed in
[74]. The corresponding functions are then sub-grouped within these layers. A sim-
ilar definition is provided in [167], where the four main components of a data lake
are defined asingestion,storage,processing,querying.
Following the maturity- architecture classification, the degree of the pro-
cessing of the data is the central point of consideration. This classification is only
helpful in the discrimination and organization of different data sets, however, it com-
pletely lacks the consideration of workflows and processing capabilities. Highlight-
ing the advantages of careful data life-cycle planning ahybrid architectureis proposed
alongside thefunctionalandmaturitybased classifications [167]. Within this architec-
ture, the individual components are uniquely identified by the data refinement and
the possible functionality, that can be performed on the specific data set.
2
https://orc.apache.org/ 3.4. Metadata Modeling23
3.4 Metadata Modeling
Proper metadata management is key for preventing a data lake from turning into a
data swamp and thus is the most important component to ensure continuous opera-
tion and usability [98, 203]. Due to the generally flat hierarchy and the requirement
to store any data in its native format, there is always the risk of losing the overall
comprehensibility of the data lake. This comprehensibility is lost, if data cannot be
found or the relationship to other data sets cannot be retrieved. One of the most se-
vere consequences of this is the inability to define concise queries to select the data
one is looking for in a fine-grained manner. As a consequence, numerous metadata
models and systems tailor-made for usage in data lakes are proposed. These mod-
els and systems originate from different cases and represent various viewpoints,
and therefore differ regarding their feature sets. From this wide variety of available
options, a few distinct works have been selected and are discussed in the following
sections.
3.4.1 Data Vault
Data modeling in adata vaultwas proposed by Linstedt in the 1990s and published in
the 2000s to allow for a more agile metadata evolution, i.e., the continuous develop-
ment of the metadata schema, in data warehouses compared to star or a snowflake
schemata [111]. This ensemble modeling uses traditionally relational database sys-
tems and combines the third normal form with the star schema. All data is stored
in three different types of tables.Hubsdescribe a business concept and are imple-
mented as lists of unique keys, and can be populated by different data sources.Links
describe relationships between the aforementioned hubs.Satellitescontain all at-
tributes which describe the properties of a hub or a link. Evolving the data vault
over time mainly implies adding additional satellite tables to links and hubs. There-
fore, there is no need to migrate existing tables, which facilitates the continuous
addition of metadata over time.
Due to these characteristics of the data vault concept, it is also applied in data
lakes. For example, in [139] the definition of a data vault for a specific case is
exemplified and the advantages of porting it to a NoSQL database by comparing
benchmark results to a SQL database are discussed. They also demonstrate how data sources can be added by defining hubs, links, and particularly satel-
lites. Another work proposes to split the one central, data lake-wide data vault up
into three distinct sub-data vaults: theRaw Vault, theBusiness Vault, and theData
Mart, whereby the latter does not necessarily need to be modeled in a data vault,
but could also be a flat table, or a star schema [67]. The authors reported that the
agile along with the ability to make incremental updates serves well the
needs for a data lake implementation. However, they pointed out that it can be hard
to enforce business rules across the independent sub-data vaults, that they , that
managing ambiguous keys cannot finally be solved, and that high-frequency data
can critically inflate satellites.
3.4.2GEMMS
GEMMSis proposed as aGeneric and Extensible Metadata Management Systemwith a
particular focus on scientific data management and, in this context, specifically for
the domain of live sciences [162]. The key component of GEMMS is an abstract en-
tity calledData Unit, which consists of raw data and its associated metadata. It is 24Chapter 3. Background and Related Work
stated, that the main advantages are, flexibility during ingestion and a interface
that abstracts singular files. TheseData Unitscan be annotated with semantic meta-
data according to a suitable ontology. However, the core is described withstructure
metadata. Mappings are only discussed for semi-structured files, Comma Sepa-
rated Values (CSV), Extensible Markup Language (XML), or spreadsheets, however,
it seems straightforward to extend this in other cases.
3.4.3MEDALandgoldMEDAL
A graph- metadata model is presented in [168], where a subset of data, called
an object, is represented as a hypernode that contains all information about that
particular object, the version, semantic information, or something calledrepre-
sentations.Representationspresent the data in a specific way, for instance as a word
cloud for textual data. There is at least one representation required per object, which
is connected to this object by aTransformation. These representations can betrans-
formed, which is represented as a directed edge in the hypergraph. This edge con-
tains information about thetransformation, i.e., a script description or similar. Data
versioning is performed at the level of these hyperedges connecting two
different representations. Additionally, it is possible to define undetected hyper-
edges representing the similarity of two objects, provided that the two data sets are
comparable.
This is revised in [172], where the concept is simplified to only data entities,processes,links, andgroupings.Processesalso generate newdata entities,
dropping the rather complicated idea ofrepresentations. These concepts are again
mapped on a hypergraph. Both models require global metadata, such asontologies
orthesauri.
3.4.4CODAL
This data lake, and in particular the utilized metadata model calledCODAL[168] is
purpose-built for textual data. It combines a graph model connecting all ingested
data sets with a data vault describing an individual data set. One core component
is thexml manifest, which is divided into three parts: i)atomic metadata, ii)non-atomic
metadata, and iii) a division forphysical relational metadata. Metadata of the first cate-
gory can be described as key-value pairs, whereasnon-atomic metadataonly contain
references to a specific entity on a different system, they are "stored in a specific for-
mat in the filesystem" [168]. Additional information about the link strength which is
modeling therelational metadatais stored in a dedicated graph database. Here, each
node represents one document with a reference to the correspondingxml manifest.
3.4.5 Network- Models
A network- model, which extends a simple categorization [150] into three dis-
tinct types of metadata, i.e.,Business Metadata,Operational Metadata, andTechnical
Metadata, is proposed to improve the data integration of different data sources in-
gesting heterogeneous and unstructured data into a data lake [54]. Here, the notion
of objects, or nodes in the resulting graph, is as well, which is defined by the
corresponding source typology. on these objects, links are generated, contain-
ing astructural, asimilarityor aLemma[138] relationship. In this , a node is
not only created for each source but also for each tag in thestructural relation-
shipmodeling. Lexical similarities are derived if two nodes have a common lemma 3.4. Metadata Modeling25
in a thesaurus, while string similarities are computed a suitable metric, which
is aN-Gramsin this particular case [158]. Similar nodes are merged. Due to this
merge, synonyms in queries can be detected and appropriately handled.
3.4.6CoreKG
CoreKG[13] contextualizes the metadata in the data catalog. To this end, four fea-
tures has been identified to constitute this curation service [15]:Extraction,Enrich-
ment,LinkingandAnnotation. TheExtractionfunctionality extracts information from
the raw data containing natural language, the names of persons, locations, or
organizations.Enrichmentfirst provides synonyms and stems from the extracted
features by lexical knowledge bases likeWordNet[128]. These extracted and
enriched features then need to be linked to external knowledge bases, likeWikidata
[202]. This enablesCoreKGto understand, if, for instance, the name of a certain politi-
cian is extracted, to link against the corresponding country that politician is active
in, i.e., to set it into context. Additionally, users can also annotate the data items.
3.4.7GOODS
GOODSis the internal data lake of Google [77, 78]. It is unique compared to all other
systems presented since it gathers all its information in a post-hoc manner. This
means, that the individual teams continue working with their specific tools within
their established data silos, whileGOODSextracts metadata about each dataset by
crawling through the corresponding processing logs or storage-system catalogs. The
central entity of this data lake is a data set, which users or a special data stewardship
team can additionally annotate. These datasets are then connected by aknowledge
graph[183] to represent their relationships. Within these relationships, thedataset
containmentenables to split up data sets, as it allows forbigtable column families[34]
to be a data lake entity themselves, along the entirebigtable. Due to efficient nam-
ing conventions for file paths,GOODScan build uplogical clusters, depending on
whether they are regularly, e.g., daily, generated, if they are replicated across differ-
ent compute centers or if they are sharded into smaller data sets. In addition, the
data sets are linked bycontent similarityas well. Since the entire data lake contains
more than 20 billion data sets with the creation/deletion of 1 billion data sets per
day, no pairwise similarity can be performed. Instead, locality-sensitive hash values
for individual fields of the data set are generated and compared.
3.4.8Constance
Constance[73] is a data lake service, that extracts explicit and implicit metadata
from the ingested data, allows semantic annotations and provides derived metadata
matching and enrichment for a continuous improvement of the available metadata,
and enables inexperienced users to work with simple keyword- queries by
providing a query rewriting engine [75]. As it is typically done in data lakes, data
is ingested in raw format. Then, as much metadata as possible is extracted from
it, which makes structured data likeXMLeasier to ingest since schema definitions
can be directly extracted. In the case of semi-structured data, JavaScript Object
Notation (JSON) or CSV files, a two-step process called theStructural Metadata Dis-
coveryis necessary. First, it is checked, whether or not metadata is either encoded in
the raw file itself, a self-describing spreadsheet, or if metadata is encoded in the
filename or file path. In a second step, relationships are tried to be discovered during 26Chapter 3. Background and Related Work
the lifetime of the data lake between the different datasets, for instance, on the
frequencies of join operations.Semantic Metadata Matchingis provided by a graph
model and should a common ontology. In addition, schemata can be grouped on their similarity, which is useful in highly heterogeneous data lakes.
3.5 Provenance
One of the generally most important metadata in the context of linked data
is provenance [81]. Data provenance or the data lineage hereby contains information
about the origin of a dataset, e.g., how, by whom, and when it was created. There
has been an effort by the World Wide Web Consortium (W3C) to standardize the rep-
resentation of provenance information by the of anOWL2ontology, as well as a
general data model, among other documents, to complete their specification called
PROV[16, 130]. Provenance is also becoming increasingly important in science, as
it is a natural way to make scientific work more comprehensible and reproducible.
This can be exemplified by the adaption ofresearch objects[11] andreusable research
objects[213], focusing even more on precise provenance information and repeatabil-
ity of computational experiments. Apart from this, provenance is considered key in
data lakes, to organize, track, and link data sets across different transformations and
thereby ensure the maintainability of it.
3.5.1CoreDBandCoreKG
CoreDB[12] andCoreKG[13] are data lake services with a main emphasis on a com-
prehensive REST API, to organize, index, and query data across multiple databases.
At the highest level, the main entities of this data lake are data sets, which can be
either of typerelationalor of typeNoSQL. In order to enable simultaneous query-
ing capabilities theCoreDBweb service is itself in front of all the other employed
services. On this layer, queries are translated betweenSQLandNoSQL. A particu-
lar focus is the lineage tracing of these entities. The recorded provenance is hereby
modeled by a directed acyclic graph, where /roles and entities are nodes while
connecting edges represent the interaction. This employed definition is given by the
Temporal Provenance Model[14] and can answer, when, from where, by whom, and
how a data set was created, read, updated, deleted, or queried.
3.5.2GOODS
GOODSmetadata model has a particular focus on provenance [77, 78]. In order to
build up the provenance graph, production logs are analyzed in apost-hocmanner.
Then the transitive closure is calculated to determine the linkage between the data
sets themselves. Since the data- events in those logs are extremely high, only a
sample is actually calculated and the transient closure is reduced to a limited amount
of hops.
3.5.3Komadu- Provenance Auditing
In [190, 191] a data lake reference architecture is proposed to track data lineage across
the lake by utilizing a central provenance collection subsystem. This subsystem en-
ables stream processing of provenance events by providing a suitableIngest API 3.5. Provenance27
along with aQuery API. In order to centrally collect provenance and process it,Ko-
madu[192] is . Hereby, distributed components can send provenance informa-
tion viaRabbitMQand web service channels. These single events are then assem-
bled into a globally directed acyclic provenance graph, which can be visualized as
forward or backward provenance graphs. this central subsystem, the need for
provenance stitching [131] is circumvented.
3.5.4JUNEAU
JUNEAUis build on top ofJupyter Notebooks, by replacing its backend and customiz-
ing the interface [218]. It is therefore specifically targeted at data scientists who
are already familiar with Jupyter Notebooks. The main constituents of the data lake
are tables, or data frames, of which transformations are tracked. Therefore, the note-
book itself is considered to be the workflow, and each executed cell within is a task.
The provenance information is captured when the code within a cell is transmitted
to the usedkernel. on this, the notebook is reformatted into a versioned data
flow graph, where procedural code is transformed into a declarative form [91]. Us-
ing a modified top-kthreshold algorithm [61], similar data sets can be found with
respect to the individual provenance.
3.5.5DCPAC
In order to manage automotive sensor data, Robert Bosch GmbH has built a data
lake [55]. Although the paper mainly focuses on their extensive Data Catalog, Prove-
nance, Control (DCPAC) ontology to build their semantic layer, a dedicated
data processing mechanism is provided. Data processing is done container-
ized applications, which can data in the data lake, and either create a data resource from it or curate existing data sets. The semantic data catalog is up-
dated viaApache Kafkamessages. Hereby, data items are integrated and their
provenance is automatically recorded.
3.5.6DataHub
DataHub[19] combines a dataset version control system, capable of tracking which
operations were performed on which dataset and by whom, as well as their depen-
dencies, with a hosted platform on top of it. Hereby,DataHubusestableswhich con-
tainrecordsas their primary entities.Recordsconsist of a key, along with any of typed, named attributes. In the case of completely unstructured data, the key can
refer to an entire file, in the case of structured or semi-structured files XML or
JSON, the schema can be (partially) modeled into thisrecord. These individualta-
blescan then be linked to formdata setsunder the specification of the corresponding
relationships. The version information of atableordata setis managed aver-
sion graph, i.e., a directed acyclic graph where the nodes are data sets and the edges
contain provenance information. In order to query multiple versions at a time, a
SQL- query language calledVQLis provided, which extends SQL about the
knowledge that there are different tables for the different versions of a data set.
Along with DataHub,ProvDB[126, 127] is developed. It incorporates a prove-
nance data model [37] which consists of aConceptual Data Modeland aPhysical Prop-
erty Graph Data Model. The first model considers a data science project as a working
directory where all files are either of typeResultFile,DataFile, orScriptFile. These files
can be further annotated byproperties, i.e., JSON files. This model is then mapped 28Chapter 3. Background and Related Work
onto a property graph, where the edges represent the relationship, a parent-
hood. Provenance ingestion is possible in three different ways. The first option is to
prefix shell commands withprovdb ingestwhich then forwards audited information
to different specializedcollectors. Secondly, users can provide annotations. Lastly,
there are so-calledFile Views, which allow defining virtual files as a transformation
on an existing file. This can be the execution of a script or of an SQL query.
3.6 Compute Integration
Although the first major challenge in building a data lake is the aforementioned
metadata management, when scaling towards big amounts of data, (automated) op-
eration and manageability of the data lake become increasingly important. For ex-
ample, the extraction of metadata upon the ingestion into the data lake requires a
scalable and highly automated process, that ideally can be integrated into work- or
data flows wherever necessary [121]. As in the case of metadata extraction, it is also
sometimes more comfortable to split a complicated analysis into a workflow consist-
ing of different steps. This has the advantage that different parallelization techniques
[147, 154] can then be applied to improve the scalability of the implemented analy-
sis. Apart from these challenges, having a data lake is often driven by the wish to
perform advanced analytics on big data sets. These tasks are usually quite compute
intensive, which makes a tight integration of scalable compute resources necessary.
Therefore, integration of compute is one key challenge in the area of data lakes. In
the following three different systems are discussed.
3.6.1KAYAK
KAYAK[115, 116] offers so-calledprimitivesto analyze newly inserted data in a data
lake in an ad-hoc manner. KAYAK itself is a layer on top of a file system and offers a interface for interactions. The respectiveprimitivesare defined by a workflow of
atomic and consistent tasks and can range from inserting or searching for a data set
in the data lake to computing k-means or performing an outlier analysis. Tasks can
be executed either by KAYAK itself, or a third-party tool can be triggered, likeApache
Spark[215] orMetanome[152]. Furthermore, tasks can be subdivided into individual
steps. By defining a directed acyclic graph, which consists of consecutive dependent
primitives, so-calledpipelinescan be constructed. Here, output data is not immedi-
ately as input for a consecutive primitive, but output data is first stored back
into the data lake and the corresponding metadata in the data catalog is updated.
Users can define atime-to-actionto specify the maximum time they are willing to
wait for a result or preview, or they define atolerance, which specifies the minimal
accuracy they demand. A preview is a preliminary result of a step. In order to en-
able these features, each step has to expose aconfidenceto quantify the uncertainty
of the correctness of a preview, and acostfunction to provide information about
the necessary run-time to achieve certain confidence. KAYAK enables the parallel
execution of steps by managing dependencies between tasks. These dependencies
are modeled as a directed acyclic graph for each primitive. By decomposing these
dependency graphs into singular steps, these can be scheduled by aqueue manager.
It enables the asynchronous execution of tasks by utilizing a messaging system to
schedule these tasks on atask executor, which is typically provided multiple times on
a cluster to allow for parallel processing. 3.6. Compute Integration29
3.6.2Klimatic
Klimatic[184] integrates over 10,000 different geospatial data sets from numerous on-
line repositories. It accesses these data sets via Hypertext Transfer Protocol (HTTP)
or Globus GridFTP. This is done in a manner that allows capturing of path-
provenance information and can therein identify relevant data sets on file ex-
tensions, likeNetCDFor CSV. It then pre-processes these heterogeneous data sets
to integrate them into this single, homogeneous data lake while ensuring topologi-
cal, geospatial, and -defined constraints [27, 45, 60]. The pre-processing is done
automatically within a three-phase data ingestion pipeline. The first step consists
of crawling and scraping, whereDockercontainers are deployed in a scalable pool.
These crawlers retrieve a Uniform Resource Locator (URL) from acrawling queueand
then process any data found at that URL while adding newly discovered URLs back
into thecrawling queue. this it is already enough to start with a lim-
ited amount of initial repositories, those of theNational Oceanic and Atmospheric
Administrationor theUniversity Corporation for Atmospheric Research. After these data
sets have been successfully discovered, these are submitted to aextraction queue. El-
ements of this queue are then read byextractor instances, and alsoDockercontainers
which can be elastically deployed. These extract metadata with suitable libraries/-
tools, likeUK Gemini 2.2, and then load the extracted metadata into aPostgreSQL
database. these automated processes, a is for instance able to query for
data in a certain range of latitudes and longitudes, andKlimaticwill estimate the time
needed to extract all data from the different data sets within the specified range, and
will then provide the with an integrated data set focal operations [179].
3.6.3 Hadoop- Data Lakes
As motivated in the background provided in 3.1, data lakes were much
driven by the Hadoop ecosystem. Two of these Hadoop- data lakes are de-
scribed in the following [96, 211]. In [96] a data lake system, calledData Café, for
biomedical data is described. Here, diverse data sources are integrated by the cen-
tral control layer, which queries diverse data sources, including existing data silos
and data warehouses, and stores the results on HDFS, or alternatively on S3 or SQL
databases. Data providers can provide additional metadata about the data sources
which is stored in Apache Hive. Data lake users can then Apache Drill [83] to
query the data lake.Data Cafécan then the information about the data sources
to find suitable data sets to perform join operations on.
This basic setup is extended to include advanced analytics for electricity usage
data [211]. Here, existing data sets are moved via Apache Sqoop to Hive, whereas
live data is streamed via Kafka [101] into HBase. Users can search the data lake with
Apache Phoenix and Apache Impala [24]. The retrieved data, which is stored in
HDFS, can then be processed with SparkMLlib [124].
These two systems are typical representations for Hadoop- data lakes,
which work solely on a flat namespace, which unlike in the zone architecture is not
further organized into different subsystems. 30Chapter 3. Background and Related Work
3.7 Analysis of Related Work
The related work is discussed with a particular focus on the architecture, metadata
modeling, provenance, and compute integration since these are the key aspects iden-
tified in Chapter 2. Before the discussed data lakes are mapped on the defined ob-
jectives, compare 2.2, the related work is compared with regard to common
design patterns, and common or individual shortcomings for each category. It can
be assumed, that common design patterns can be found in different data lake imple-
mentations because they provide convenient solutions to common problems in data
lakes. Similarly, there might also be a correlation between common shortcomings
and common design patterns.
3.7.1 Analysis of Data Lake Architectures
Most of the presented data lake architectures share the common idea of dividing
the overarching data lake into sub-systems, e.g., zones, layers, or classifications, i.e.,
maturity or functionality. This is done in order to structure the otherwise flat names-
pace. Data is either put into a sub-system on the degree of processing it was
subjected to, or by the specific functionality a sub-system offers, e.g., offering sup-
port for specific workloads.
The advantage of dividing a data lake into sub-systems is that it organizes the
data and the interactions with it within the lake. However, this argument holds only
true under the assumption of limited and well-defined data sources and workflows,
since these sub-systems are purpose-built. Another disadvantage is the technical
complexity of these systems. In the specific case of the zone architecture each zone
is usually implemented by an individual set of (physical) systems, according to a
meta-analysis [66]. This does not only increase the administrative complexity in
general but particularly in managing rights. In addition, these data lakes do
not enforce an overarching data catalog by design, limiting the findability and the
provenance tracking within the lake for the end users.
Another common feature across all presented architectures compared to the early
Hadoop- data lakes is, that modern data lakes emphasize a separation of stor-
age and compute.
3.7.2 Analysis of Metadata Modeling
There are certain properties that most of the discussed metadata models share, as
can be seen in Table 3.1. Almost all of the presented models have a high degree
of generality, i.e., they can be to represent any kind of data. The only excep-
tions are CODAL and CoreKG which focus on lexical data. Generally one can divide
the metadata models into two groups, the entity- systems, which model each
datum, dataset, or data source as an individual entity, and the knowledge-graph,
or information-centric systems. One can observe, that all general applicable meta-
data models are entity-. All of the systems support continuously evolving
metadata, while only two-thirds offer similarity links to support related dataset ex-
ploration. InteRESTingly, only one-third of all presented models offer data quality
assurance, although the importance is generally recognized.
3.7.3 Analysis of Provenance Auditing
The evaluation of the discussed provenance auditing is done on four different
aspects, i.e., the employed auditing technique, the communication protocol to 3.7. Analysis of Related Work31
ModelGenerality
Entity Similarity
Links
Quality
Control
Data Vault++-0
GEMMS++-+
MEDAL+-+-
goldMEDAL+++-
CODAL--+-
Network- Models++++
CoreKG----
GOODS+-+0
Constance+-++
TABLE3.1: Comparing the presented metadata models. A + indicates
full agreement, a 0 shows only partial agreement, and a - indicates
that the corresponding feature is missing.
ingest the lineage information into the data lake, the format, in which it is stored,
and the achieved reproducibility. These four aspects are key when an existing tech-
nique should be re- for general-purpose cases and pave the way for a more
generalized system that can incorporate different compute paradigms, specifically
for scientists. For provenance auditing a tool is required, which can track all in-
teractions done with/on a compute cluster, e.g., an system or a cloud. This
specifically includes multi-step jobs on systems which might include prepro-
cessing on a frontend node, processing done non-interactively on the batch system,
and possible postprocessing on the frontend nodes, once the batch job has finished.
Similarly, a communication protocol needs to be , which can be easily from these different platforms. Here one needs to consider the different network
accessibilities, e.g., compute nodes can only communicate within a dedicated network, or cloud platforms might not enable a task to a network at all. In ad-
dition, it is preferred to store provenance information in an established format to
ensure interoperability.
In Table 3.2 a summary of the seven discussed data lake implementations is
shown. One can see, that most systems do not automatically provide an auditing
system and rely on the or the ’s tasks to pro-actively send the lineage infor-
mation. All systems a REST API for communication, although Komadu, which is on RabbitMQ for provenance auditing, uses the Advanced Message Queuing
Protocol by default but can be extended to a REST API with an additional gate-
way. All systems some kind of graph to store the provenance information. Usu-
ally, this is done via a Directed Acyclic Graph (DAG), however, only DCPAC follows
the PROV-O convention, whereas all others implement a custom graph representa-
tion. One can also observe, that without an integrated auditing tool, reproducibility
can not be achieved by design. Particularly JUNEAU provides a strong capability
for reproducibility. That is because it introduces an extra auditing layer between the
Jupyter Notebook, where users write their code, and the kernel, where the code is
actually being executed. This enables JUNEAU to precisely record what transfor-
mations have been done on which input data and store this information along with
resulting artifacts.
3.7.4 Analysis of Compute Integration
There are different approaches to providing tight integration of scalable compute
resources into the fundamental design of a data lake. 32Chapter 3. Background and Related Work
SystemAuditingProtocolFormatReproducibility
CoreDB-REST API
Temporal Provenance
Model
-
CoreKG-REST API
Temporal Provenance
Model
-
GOODS
post-hoc,
log-
-Knowledge Graph-
Komadu-RabbitMQDAG-
JUNEAUJupyter Notebooks-versioned flow graph+
DCPAC-Apache KafkaPROV-O-
DataHub
provdb ingest,
annotations,
file views
-property graph+
TABLE3.2: Comparing the presented lineage tracking systems. The
auditing column contains the employed auditing tool, whereas a -
indicates that no built-in system is provided. The protocol provides
information about the communication protocol which transfers the
records to the data lake, and the format states how the information is
stored. The reproducibility column gives a qualitative assessment of
the achieved default reproducibility.
The discussed JUNEAU system uses Jupyter Notebooks as its primary interface
and therefore offers integrated compute capability per default. However, the general
applicability is also limited since only tables, so-called data frames, are supported as
input and output data.
KAYAK offers a very sophisticated mechanism to implement parallelizable and
automatable workflows. However, the decomposition of an abstract workflow into
taskswhich can then be combined toprimitivesand finally can be chained to entire
pipelines, requires quite some prior knowledge. Although powerful, the decomposi-
tion of established scientific software suits, as they are commonly on sys-
tems, into these entities, i.e., tasks, primitives, and pipelines, is not straightforward
and would require a substantial amount of work for each job. A similar argument
holds true for the usage in cloud environments. Furthermore, although the idea of
atoleranceand atime-to-actionis very useful on a data lake, this is only suitable for
a subset of methods that are iterative by nature. From the viewpoint of a generic
scientific application, this simply adds additional overhead and increases the entry
barrier.
Klimatic on the other hand uses Docker containers to perform processing tasks.
Although the generic to split up a pipeline into distinct stages which are
linked by dedicated queues can be adopted to serve other cases as well, it does
not describe how the actual compute infrastructure is integrated. One can therefore
assume that this solely works by accessing a local, or remote Docker engine.
Lastly, the Hadoop- systems are discussed. Considering the capabilities of
the Hadoop ecosystem, this achieves a high integration of compute into the data lake
design. map-reduce methods, one can directly work on the data on HDFS for-
mat, but one can also Spark for advanced analytics machine learning. How-
ever, one is always bound to the Hadoop ecosystem and it is hard if not impossible
to integrate other computing paradigms, or cloud, later if a project needs
to extend its reach. 3.7. Analysis of Related Work33
SystemGenerality
Performance
Scalability
FAIR
Scientific
Workflows
Data
Lifecycle
SecurityModularity
Zone00---0+
Lambda00---0+
Lakehouse-0--000
Functional/Maturity00-0000
Data Vault+0-+--
GEMMS+0-+--
MEDAL+0-+--
goldMEDAL+0-+--
CODAL-0----
Network- Models+0-+--
CoreKG-0---+0
GOODS+0-0--0
Constance+0-+--0
CoreDB-0-----
Komadu00-+0--
JUNEAU----0--
DCPAC++-0---
DataHub+0++0--
KAYAK0--++--
Klimatic-0-----
Hadoop-++0000-
TABLE3.3: Comparison of the discussed systems with respect to the
in 2.2 presented criteria. A + indicates that the system com-
pletely fulfills the requirement by design, a - that the design contra-
dicts the fulfillment of the objective, a 0 that it is dependent on the
explicit implementation, and empty spaces indicate that no meaning-
ful assertion is possible.
3.7.5 Gap Analysis
Table 3.3 compares the discussed systems to the identified requirements of the envi-
sioned system, as discussed in 2.2. One can see that there is currently not a
single system that is capable of fulfilling all the objectives. Although most of them
are generally applicable, fewer are able to integrate scientific workflows. However,
beyond these two characteristics, there is a large decline in the of systems
addressing the depicted challenges. There is only a single system that explicitly sup-
ports the FAIR principles, data lifecycle management, and security by design. Only
a few others could be extended in specific circumstances, to support these criteria.
Additionally, most systems are designed in a monolithic way, which makes the inte-
gration of backend services difficult, which might be required to integrate scientific workflows.
One remarkable gap is the lack of diverse compute integration into data lakes.
The discussed systems in 3.6 are all tailormade to a single specific ecosystem,
i.e., Jupyter Notebooks for JUNEAU, Docker containers for Klimatic, a purpose-built
workflow engine for KAYAK, and the Hadoop ecosystem for all the Hadoop-
data lakes. These purpose-built systems do not provide a generalized compute in-
terface and are solely limited to the individual compute framework, limiting the
performance, scalability, and support for scientific workflows which rely on
different systems. This is a particularly outstanding observation compared to the
other insufficiently covered requirements of the envisioned data lake, since provid-
ing scalable compute resources is generally recognized as a key characteristic of a
data lake. There are a of survey papers published, investigating the cur-
rent state of the art [40, 48, 74, 136, 137, 167], where none of these survey papers
treat computation at all, although [167] explicitly lists "scalability in terms of storage 34Chapter 3. Background and Related Work
and processing" as one of six key characteristics of a data lake. However, during
their review process, they solely discuss the possibility of doing batch processing
with Apache Hadoop and, preferably, Apache Spark, while pointing out that some
implementations Apache Flink or Apache Storm for interactive -time pro-
cessing, which is in good agreement with the findings of [48]. In [136] it is claimed
that the provisioning of highly scalable computing capacity is one of the key chal-
lenges data lakes currently face. However, the presented systems in 3.6 do
provide scalable compute capacity, however, each one does so in only one fixed and
very specific way. Therefore, the actual challenge for data lakes is to integrate dif-
ferent compute infrastructures, cloud and systems, into one homogenous
data lake. 35
Chapter 4
Integrating Compute and Data
Management
Within this chapter, the challenges of data-intense projects on systems are
identified in 4.1, and a deeper background on storage tiering on systems is provided in 4.2, from which an overarching concept for the in-
tegration of data management systems into workflows is discussed in Sec-
tion 4.3. From this analysis, a novel governance-centric interaction paradigm is
proposed in 4.4. Parts of this chapter are published in [140].
In Chapter 2 the general idea of utilizing a data lake for the presented MRI case
is proposed to fulfill the identified requirements. This is further underpinned by
an extensive literature review in Chapter 3. Within a gap analysis on this re-
view, a general lack of data lakes utilizing systems for compute-intensive tasks
is identified. This lack is caused by the general to integrate compute re-
sources in a tailormade way to support only a single platform or ecosystem, instead
of providing a general compute interface where different platforms can be plugged
in. Although it seems generally possible to combine the different presented tech-
niques to design a data lake, that can fulfill the requirements, for instance by the very generic zone architecture and providing compute capabilities within dif-
ferent dedicated zones, this distributed does prevent an overarching data
management and governance.
Therefore, this is an ideal point to perform Step 3, i.e.,Decomposition and Encap-
sulation, of the presented scientific methodology, see 1.3, to identify indepen-
dent components which can be encapsulated. Since there is no extensive knowledge
base that could be established during the literature review, a very basic analysis is
done with regard to the different ways users can interact with an system, and
what specific challenges are there for data-intensive projects on systems. on this analysis, a hierarchical ordering of the identified, involved components is
done.
4.1 Challenges of Data-Intense Projects on Systems
When conducting data-driven research on systems, four challenges can be
identified:
•Performance:One can often see, that data-driven projects work with iterative
procedures on small files, which lead to heavy loads on the storage system,
particularly on the metadata servers, and can lead to large performance degra-
dation due to storage bottlenecks. 36Chapter 4. Integrating Compute and Data Management
•Data Management:Fulfilling the FAIR principles requires the inclusion of ex-
ternal services, a handle system for PIDs, which are usually not hosted
on an system. Therefore, researchers tend to make their data FAIR at the
end of a project, which is time-consuming and requires strict bookkeeping. For
instance, to make data findable a naming scheme for created files, objects, and
paths is mandatory. Then one also has to actually follow it. Sharing data with
other researchers often comes as an afterthought. It is a reasonable assump-
tion, that most projects do neither strictly follow the FAIR principles nor their
Data Management Plan (DMP) if there was one defined at the beginning of a
project. It can be expected that this issue will only be exacerbated by the in-
creasing complexity and heterogeneity of the employed storage systems in the
compute continuum.
•Integration of Compute and Data Handling:Computing on an system
feels a bit archaic. Users have to manually define many system settings, for
instance, filenames and paths to define what storage to . Meanwhile, the
complexity of the tiered storage systems in modern systems has drasti-
cally increased. There exists no way to define and enforce a data governance,
which is homogeneously applicable across all of the disparate storage tiers
since they all have their own set of policies. For instance, node-local storage is
not backed up, and is only available during the resource allocation, but is very
fast, whereas the globally availableHOMEfilesystem is backed up but offers only
very limited space and performance.
•Reproducibility:Being able to understand the lineage of data and how to re-
produce certain outputs is important for trust in the scientific results. How-
ever, as executions on systems are usually scripts that are invoked man-
ually, on binaries created specifically for the given supercomputer, it is tricky
to reproduce results.
For all of these challenges, but particularly for the second one, utilizing a suitable
Data Management System (DMS), a data lake, seems a good solution. How-
ever, since there will always be a gap between a remote DMS and an system,
ensuring reliable information within the DMS that originates from an system
is an unsolved problem. This lack of reliable information is generally also true for
provenance information. This might seem surprising since it is a general require-
ment for scientific work and there are provenance auditing tools available.
One often is to monitor the system calls on the compute nodes
during job execution, likePASS[134] does, to create audit trails. Following a similar
,LPS[50] has drastically reduced the runtime overhead, but is not com-
pletely transparent due to the of a dedicatedLibrary Wrapper.ReproZip[42] also
continues the idea of audit trails of system calls to automatically build packages to
re-run an experiment.
A different way for lineage recording is provided byData Pallets[113]. Here, all
processes run within containers where all write to the storage devices is inter-
cepted and transparently redirected to data containers. Hereby, all data containers
are automatically annotated with reliable provenance recordings.
Although there exist these different solutions, provenance auditing tools are not
in widespread on systems. Considering the general idea of these tools, one
simple explanation could be, that users who are already overwhelmed with their
data are confronted with even more data, i.e., the recorded provenance. To circum-
vent this problem, resulting from a too-fine granular solution seems to abstract the 4.2. Background on Storage Tiering in HPC37
data management problem more coarse-grained to overcome the gap between these
node-local and hardware-close tools and the higher level interaction a wants to
have with an system.
4.2 Background on Storage Tiering in Usually, clusters provide at least two parallel file systems, oneHOMEand one
WORKfilesystem, which provide file via Portable Operating System Interface
I/O (POSIX-IO) semantics, the relaxed Message Passing Interface I/O (MPI-IO) [47]
semantics or the close-to-open semantics by the Network File System (NFS)
[155], to name just a few. All of these semantics require dedicated metadata servers
to empower parallel file systems, Lustre [174], or the General Parallel File Sys-
tem (GPFS) [171]. These metadata servers handle all metadata operations spe-
cial data structures called inodes to handle these metadata operations. If an inode
represents a folder on such a filesystem, it contains a list of all inodes located in
this folder. Depending on the actual operation, which should be done, it might be
necessary to also read additional information from each inode within a folder, for
example, the file permissions, or the ownership. The cost for these metadata opera-
tions scales linearly with the of files stored within a single folder. However,
if the list of inodes stored in a single inode becomes too long, indirect inodes have
to be . This behavior can be triggered if those inode lists are inlined within
a small data block within the inode itself. This is typically done to avoid lookups
on the storage servers holding the actual data of a file, which would otherwise in-
crease the latency of such a metadata operation drastically. These indirect inodes,
potentially even consisting of multiple layers, lead to an even worse performance
degradation. Therefore, having too many files within a single folder has a huge per-
formance penalty. However, this can often be observed in machine learning projects,
e.g., if there a tens of thousands of small images in a single folder whose name en-
codes the particular target, a folder calledcatscontaining many small images of
cats.
Although there is a varying amount of overhead necessary in the different se-
mantics and filesystems, they all share the problem of bottlenecking when exposed
to this described small file IO. Current mitigation strategies consist of providing a
tiered storage system, where each tier is optimized to handle certain workloads, or
meeting a specific cost-to-capacity ratio. This option leads to increased complex-
ity and requires the users to manually move and stage data to the correct tiers to
achieve optimal performance while ensuring that cold data is not piling up on fast
and expensive storage, which is not backed up.
In addition, novel storage concepts, object stores, are being integrated into cluster[72], which supports flat namespaces by design. These are already com-
mon in cloud environments, with prominent standards Amazon’s S3, or Open-
stack Swift. However, their REST- interfaces entail additional overhead, both,
on the communication layer, and also on the application layer, since the file han-
dling drastically differs from the well-established POSIX-IO compatible file systems.
However, these designs require a lot of knowledge by the users and will increase the
overall complexity of the data management in the sense, of what data is stored where
and how can it be efficiently accessed.
In addition, the exact composition of these storage tiers is unique for each system and usually evolves over time. This makes it nearly impossible to utilize data
management systems, the previously proposed data lake, to efficiently utilize an 38Chapter 4. Integrating Compute and Data Management system, when users want to remotely execute an job. That is because the
developers, or admins of the data lake would need to continuously adapt the data
placement for each for the connected systems.
Thus, the complex storage architectures of systems are a burden for users
working directly on systems, and those only it via a dedicated data man-
agement system. on this observation, the need for a generic solution becomes
apparent, to further integrate storage and compute without relying on manual con-
figurations by users.
4.3 Overarching Concept of Integrating Data Management
Tools into Workflows
There are several challenges that a typically faces when working on an system due to the complex storage structure. Considering the different namespaces
offered by the different storage systems, it is challenging to get a concise overview
of all the data that is currently stored. Here, the current strategy is to systems to
provide a unified namespace to users. However, these systems completely lack any
awareness of the overarching workflow a is doing and are therefore struggling
to stage data meaningful and transparently record their lineage. That increases the
barrier to adhering to the FAIR principles and performing transparent provenance
auditing to ensure reproducibility. In addition, the DMS should optimize the usage
of a tiered storage system to provide maximal performance during compute and durable and low-cost storage for cold data. Here, a unified namespace is also not
useful, because the data is already cataloged by the DMS.
First, the abstract components and their features are identified and discussed
when interacting with a storage and compute infrastructure, such as an system.
Then the status quo as an archetype for the standard interaction paradigm and the
envisioned -friendly and data-centric flow are described.
4.3.1 Components
The necessary components when handling data and compute are as follows:
•Resources:These are raw storage, compute, and network infrastructures such
as compute nodes and object/file systems and their interconnect. They come
with their own specification, i.e., what resources they actually provide and
their characteristics.
•Resource Management (Compute):This layer manages the usage of the re-
sources by assigning compute jobs to available compute resources satisfying
the requirements for the respective (parallel) jobs.
•Resource Management (Storage):This abstract concept defines where to store
certain data and provides the respective space on a storage system.
•Job Specification:Defines the scope of a compute job together with its require-
ments, dependencies, and specification such that it can be executed.
•Program:A code that can be executed on the compute infrastructure, e.g., a
binary program or script. 4.3. Overarching Concept of Integrating Data Management Tools into Workflows
39
•Software Landscape:The ecosystem and environment provided by the plat-
form that allows the preparation of programs on the system. This can also
include software dependencies.
•Workflow Specification:Defines how to execute jobs and their dependencies
to achieve the overall data-processing goal.
•Data Management Plan:Defines for any data inputs and data products the
policies, data handling, and such to enable the FAIR principles while the data
sovereignty of the is preserved.
•Data Management System:A generic or domain-specific system organizing
and indexing all data in well-defined schemata. Usually, these systems are interactively and are hosted in a cloud environment. They may also pro-
vide process management capabilities.
• Interface:Allows the to interact with the system, e.g., to manage and
interact with some or all of the above components and to upload/download
data.
•Client:The computer system of the , from which the interface(s) are
accessed.
The way, one can resolve the previously specified challenges and implement the
components depends on the way a wants to interact with it, the system pro-
viding these capabilities, and the data flow involved. For example, either, a connects to the system, and uses it as the central contact point, or the interface
of the DMS is to manage the data processing on the system remotely.
4.3.2 Interaction Paradigms
On the most extreme scale, one can argue that there are three different kinds of users.
For instance, tech-affine people who want to natively work on the system in
a traditional command-line , users who utilize state-of-the-art compute-
centric tools, or those, who ideally only want to work with the interface of their
domain-specific DMS. These three interaction paradigms are analyzed in the follow-
ing.
Traditional Paradigm
In the traditional , the interface is a shell (such as bash) on a login node
of the cluster and the client is a Secure Shell (SSH) [212] enabled program that the runs on their Desktop/Laptop. There exists no data management plan, the thinks about how to manage data and, therewith, manually performs the resource
management for storage, identifies how to map output data to files (influenced by
the applications) and directory structures, and utilizes the available parallel file sys-
tems. Also, the manually prepares programs s/he wants to by downloading
the necessary codes on the machine and ensuring it works with the system architec-
ture and software environment that is deployed on the system. The wider
software landscape on the system is prepared by data center staff but libraries
can be extended by the users in order to create meaningful programs. In most cases,
workflows are not explicitly specified but manually invoked. The resource manage-
ment of the compute resources is provided by tools such as Slurm. The job specifi-
cations are (bash) scripts that are invoked - they define the compute requirements. 40Chapter 4. Integrating Compute and Data Management
Such scripts are submitted to Slurm which decides how to map and schedule them
on the available compute resources. These steps are basically manually set up, re-
quiring a scientist to think about how the experiment should be conducted and then
documented (if at all) in a lab notebook or scripts that do some of the work. Po-
tentially, workflow tools such as Snakemake are utilized to specify and automate
dependencies between tasks. This is not only error-prone but any change to the en-
vironment requires the to modify the experimental setup and perform the steps
again. This very common interaction with the system can be considered a bit
archaic.
Compute-Centric Paradigm
In a Compute-centric , a would connect to the frontend as usual.
In the simplest form, a would delegate the job of maintaining a data catalog
and staging the selected input data to a DMS tool. This workflow is depicted in
FIGURE4.1: Sketch of the compute-centric
paradigm. 4.1. Here, to get to the
requested input data, a would
formulate a domain-specific, semantic
search query and send this request to
the DMS. Usually, a DMS would a dedicated database or search engine
to filter the requested data. The data
is loaded into the running code of the
. This could either require a dedi-
cated data transfer to a pre-configured storage target, or the system and the
DMS are already connected to the same storage system. When looking at different
established systems, iRODS
1
or Delta Lake [4], the is typically responsible
for lineage recording and enforcing reproducibility. Therefore, these solutions typi-
cally only assist users to manage and organize their data but do not free them from
the burden of efficient IO and working in agreement with good scientific practice.
-Case-Centric Paradigm
The opposite way to integrate a DMS into an workflow, is to the DMS as the
FIGURE4.2:Sketch of the DMS-centric
paradigm. frontend, see 4.2. This DMS-
provided interface can be to
query and select input data, define a
compute job, and submit this job to an system, without the need to ex-
tra log in to the system or trans-
fer data explicitly. One example of such
a DMS isViking[100], which offers a interface on a web application from
which users can trigger the execution of
different molecular dynamics simulations without logging into the system via
SSH. This functionality requires a communication channel, between a remote DMS
and an system. Additionally, the DMS needs to be able to work with the indi-
vidual resource manager, of each system. The advantage of this lies
in the capability to perform transparent lineage recording and guaranteed repro-
ducibility. That is, because the DMS has complete control over the input data and
1
https://irods.org/ 4.3. Overarching Concept of Integrating Data Management Tools into Workflows
41
the processes that run on them, thorough logging methods is enough to en-
sure reproducibility. This is similar to the idea of JUNEAU presented in 3.5.4.
Similar to the -centric case, storage tiering is hard to support.
4.3.3 Data Flow
These two scenarios also differ in their data path, i.e., in the storage systems involved
in the data management and data processing, and the data transfers between these
systems.
Compute-Centric Paradigm
In the Compute-centric case, a accesses data through their respective, na-
tive interface, e.g. through the library functions of their respective programming
language, or as an input parameter of their program that they want to run. This
means, that only data is available which is directly accessible from the sys-
tem, and data transfers, e.g., for better performance, have to be done manually.
FIGURE4.3: Layer diagram showing the con-
trol and data flow of the compute-centric
paradigm.
In 4.3 a layered diagram is
shown which shows the possible data
staging strategies. Within the in-
terface, e.g., ssh or a jupyter note-
book, a can explicitly copy/stage
data on non-node-local storage. Within
this layer, this has to be done deliber-
ately. Assuming that a workflow en-
gine, snakemake, is , data can
be staged on non-node-local storage in
a more automated and transparent way
in the form of a dedicated workflow
step. These two options can be consid-
ered asynchronous data staging since
this will not lead to stalling times on the
compute infrastructure.Synchronous
data staging happens on theJob Layer,
where a process first has to data
on a slow storage tier and then stage it. In this case, even a very fast node-local stor-
age tier can be . Therefore, theWorkflow Engine Layeris on top of the resource
management layer, to enable asynchronous data staging. Afterward, the actual job
can continue to process the data. Since the node-local storage is typically only avail-
able during the resource reservation, which is managed by the resource manager,
e.g., Slurm, a has to ensure that the overall process running on that node, does
not only stage data, but also archives it once it is done. The entire data staging and
IO optimization is therefore solely the ’s responsibility.
-Case-Centric Paradigm
Within the DMS-centric , users are generally not interested in accessing the
data directly, e.g., with a suitable library within self-written code. Instead, they are
rather interested in having the complexity of running their job abstracted away. For
this strategy, a layered diagram is shown 4.4, depicting the data flow. Here,
a accesses the DMS via a browser, or a DMS-specific Graphical Interface 42Chapter 4. Integrating Compute and Data Management
(GUI). Within this interface, a triggers the execution of a workflow, or a single
analysis step on the selected input data. Often, these DMS are deployed in a cloud
environment and have their own storage layer. Depending on whether this storage
tier can be integrated into the system, there are different strategies for data ac-
cess.
FIGURE4.4:Layer diagram showing the
control and data flow of the DMS-centric
paradigm.
Either one can asynchronously or syn-
chronously fetch data from the DMS
within a dedicated data mover process
and stage it either node-local, in the
case of a synchronous data transfer, or
non-node-local in the case of an asyn-
chronous data transfer. For this pur-
pose, the data mover process would ei-
ther be granted to the DMS stor-
age concerning the ’s permissions,
or the DMS can provide an endpoint,
for instance, a REST endpoint, from
which the process can fetch the required
data. Since generally there is a com-
munication layer required to the
resources of an system from the
outside, this can also be to asyn-
chronously fetch data from the DMS.
Lastly, a dedicated data mover process
can also fetch the data synchronously
from the DMS on the compute node it-
self. This means, that the entire data
movement and staging strategy is solely
in the hands of the admins of the DMS, where the corresponding functionality is im-
plemented and configured.
4.3.4 Control Flow
Similar to the aforementioned data flow, there also exists a control flow, as can be
seen by the left arrow in 4.3 and 4.4. The control flow is initially trig-
gered by the within the interface and is from there passed down to the
final task running on a node. From this upmost layer, the control path goes down
via an optional workflow layer to the resource manager where the tasks get mapped
on the actual hardware, in case of the -centric view. In the DMS-centric view,
the control flow gets even more abstracted, since the input recorded by the interface has to first pass through the DMS layer, where the request gets ini-
tially processed and mapped on the DMS infrastructure. Since this DMS system is
completely disjunct from the system, a dedicated communication layer is re-
quired to bridge those two systems. On the system, the individual tasks are
again mapped to the nodes in the infrastructure layer via the resource manager.
4.3.5 Analysis of the Interaction Paradigms
To summarize the previous discussion about the different interaction paradigms,
Table 4.1 compares the characteristics of the individual components – ignoring the 4.3. Overarching Concept of Integrating Data Management Tools into Workflows
43
CharacteristicsTraditionalCompute-CentricGovernance-CentricUse Case-Centric
Resources (Compute)AutoAutoAutoAuto
Resources (Storage)ManualManualAutoAuto
Res. Mgmt (Compute)Semi-AutoSemi-AutoAutoAuto
Res. Mgmt (Storage)
ManualManualAutoAuto
Job specManualSemi-AutoSemi-AutoAuto
ProgramManualManualSemi-AutoAuto
Software landProvidedProvided/-ContainerProvided/-ContainerProvided
Workflow specManualSemi-AutoSemi-AutoAuto
DMP
ManualManualSemi-AutoTool-specific interfaceSSHSSH+WebWeb+SSHWeb interface (Data)SSHSSHWeb+SSHWeb
ClientSSHSSH+BrowserBrowser+SSHBrowser
PerformanceUser-specificUser-specific+++
Data management--++Tool-specific
Integration–0++++
Reproducibility–+++Tool-specific
Flexibility+++++–
TABLE4.1: Comparison of different interaction paradigms.
column Governance-Centric for now. The responsibility for one of the defined com-
ponents and features are either the , i.e., a manual process, semi-automatic, thus
aiding the (potentially following a specification), or fully automated. -
specific means it depends on the skill of the . The resulting differences in the
degree of automation can be illustrated best when looking at the interfaces a can to interact with the processes and data. Traditionally, only ssh connections
are supported, whereas in the compute-centric paradigm, at least some interactions
may take place via a web interface. In the case-specific paradigm typically only
a web interface is available that hides the system and all internal processes.
The resource handling requires a lot of manual interaction of the users in the
traditional and the compute-centric concept, while it is completely automated in the case-centric on configurations provided by the admins of the
specific DMS, not by the admins of the system.
A similar pattern can be seen in the characteristics of the task-related compo-
nents. Here, the experience with the system evolves from a completely
manual interaction to a partially automated or guided system in the compute-centric
context, where already some low-level programming tools for workflow orchestra-
tion, data selection, and containers for dependency management are . However,
the program and data management rely still on manual work and are therefore po-
tentially error-prone. On the other side, the case-centric system fully automates
these steps. Again, all task-related interactions are fully automated by the case-
specific system.
These intrinsic characteristics have different advantages and disadvantages. The
traditional usage paradigm relies heavily on manual work by the to achieve
a reasonable performance. Also, the data management, the integration of storage
and compute, and therefore the overall reproducibility is very much exposed to errors. However, this enables the highest level of flexibility. The compute-centric
paradigm improves this by utilizing the discussed semi-automated components and
hereby improves the integration and reproducibility. The case-specific systems
will most likely have reasonable, but not custom-made, configurations to achieve
good performance. Here the interaction with data is challenging as the upload/-
download via web frontend limits the performance. 44Chapter 4. Integrating Compute and Data Management
4.3.6 Discussion of Isolated Components
It is shown that in both cases, i.e., the compute-centric and the -case-centric
paradigm, some form of central contact point, i.e., frontend is required, be it a graph-
ical interface of a DMS as in the case of the -case-centric paradigm, or a system
that provides an API that can be in programs or with a command line
interface. on the layer diagram of the two paradigms showing the involved
components for the control and data flow, see 4.3 and 4.4, different
independent layers, or components are identified. One important component in the
-case-centric paradigm that is identified is the communication layer. This layer
needs to provide an interface to the DMS as well as to the system. However,
comparing the paradigms, as done in Table 4.1 demonstrates a significant current
challenge. Since one of the requirements is to be as generic as possible, it seems
reasonable to support both paradigms. Furthermore, when unifying the concepts of
the compute-centric and the case-centric paradigms, the individual advantages
of these two worlds should be combined to offer the best possible solution.
4.4 Novel Governance-Centric Architecture
The goal is to expand upon the existing concepts to provide a novel, unified view
of processes and data in order to improve the experience on systems. To
this end, a novelGovernance-Centricinteraction paradigm is proposed on the
previous analysis of the current interaction paradigms where different gaps are iden-
tified. This unified view can then serve as the foundation to not only integrate FIGURE4.5: Sketch of the governance-centric
paradigm. Both groups (compute-centric
and DMS-centric) are orchestrating the data
flow through the additionalGovernance Layer.
systems in a generic data lake architec-
ture but might serve as a blueprint and
a deeper understanding of how to in-
tegrate diverse computing ecosystems
into a data lake, and how to support
different interaction paradigms with it.
Similarly, this also allows to increase
the efficiency of data-intensive projects
running on systems that are com-
pletely unrelated to the data lake in
question. The metrics for the expe-
rience, i.e., performance, data manage-
ment, integration, reproducibility, and
flexibility, all mostly boil down to the
question of where the data is located
and how they are linked. This has to
be tackled simultaneously in two di-
rections: first, an additional integration
layer above the resource manager (com-
pare 4.3 and 4.4) is re-
quired, as shown in 4.5. This
layer has to provide an integrated and
unified namespace to the users. Sec-
ondly, an information flow, which is di-
rected in the opposite direction as the
control flow, is required. Although this
can be achieved with available auditing 4.4. Novel Governance-Centric Architecture45
tools, there is no concept for a tool that processes the incoming information and
hereby makes it actionable. The advantage of an actionable information flow com-
pared to the existing systems is that the information is utilized to create a desired,
predefined state, and not just create yet another piece of data a has to manage
manually.
Therefore, the governance-centric interaction paradigm is proposed that aids the
users and automatizes the integration of data and compute. In Table 4.1, the re-
quired degree of automation to bridge the gap between the compute-centric and
the -case-centric paradigms is identified. Resource management should be fully
automated to achieve the highest degree of integration of data and compute. The
task-specific components should be semi-automated to guide the in manag-
ing the data, working reproducibly, and ensuring that a predefined, ideal state is
reached while allowing as much flexibility as possible. Similarly flexible should be
the interface, to allow interaction from both groups.
A professional and proper data management requires users to define anexperi-
mental descriptionat the beginning of a project. This experimental description should
consist of a workflow that links data sets and compute tasks as well as a data man-
agement plan for the respective input/output data. Then the has to initially
modify their tasks, e.g., job scripts, to allow linking of the tasks and their data prod-
ucts to the workflow and also to generate descriptive metadata for the data sets.
Building upon the previous discussion, the goal is to not only the workflow as
an abstract concept that users may informally follow but rather enforce its usage.
The implications of the design are that the system can exploit the information
to perform many, of the previously manual tasks, automatically. Examples of these
tasks that should be automated are, receiving and processing information about in-
put data and artifacts created during the task execution, enforcing archival/deletion
policies defined in the DMP, or ingesting results into a DMS along with all required
metadata including lineage information. The latter is the required feature that will
unify both groups, i.e., those working in a -case-centric DMS paradigm and
those working in a compute-centric paradigm.
To summarize, the experimental description shall be a -defined and machine-
readable workflow description that contains information about the data flow, the
tasks that process these data sets and create artifacts, and further optional informa-
tion policies or the IO intensity of each task. This means that for every
task a wants to schedule via the resource manager, this task has to be linked to
a specific workflow step within the experimental description at job submission time.
Thus, each and every submission of a job on an system becomes one concrete
invocation of the abstract task description within the experimental workflow linked
to data in the DMP.
4.4.1 Experimental Description
Specifically in data-driven projects, it is common that there is not a single task, but
that the entire processing consists of multiple steps which are concatenated into a
workflow. Therefore, a has to provide a simple graph, compare 4.6, con-
necting input and output data via tasks as a workflow description. This workflow
could represent an MRI pipeline, where Dataset 1 could be a k-space recording and
Dataset 2 is the model that is within Task 1 to reconstruct Dataset 1 into im-
age space. The resulting image can then be manually inspected by a doctor. The
reconstruction can also be for further processing, for instance for a quantized,
volumetric analysis of the different brain areas. This process can be repeated with 46Chapter 4. Integrating Compute and Data Management
FIGURE4.6: High-level view of an experimental description repre-
sented by a workflow.
different parameters to find the optimal configuration. Manual steps in the work-
flow are explicitly annotated as they require data to be accessible.
Within this workflow definition, general policies can be defined, e.g. where and
when data should be archived, how long artifacts should be kept on hot storage if a
manual inspection is required, what accompanying metadata are required, or if in-
put data can be altered. The archiving of data should explicitly support remote DMS
as a target, to integrate systems with remote DMS, and similarly, data within
a remote DMS should also serve as possible input data. The required data mover
tools have therefore to be integrated as dependencies into the DMP. In addition, the
users can provide information about the expected IO profile, aiding the proposed
DMP tool to find the best storage tier on heuristics configured by the ad-
mins. on further metrics, the available bandwidth of a remote DMS and
the system, or the amount of data, the DMP tool can also determine, whether
data should be staged synchronously, i.e., during compute time, or asynchronously,
i.e., as an additional, dedicated, and dependent step before the compute task starts.
Describing Datasets:The can and should add further information to the data
sets that are getting processed or created. To improve the findability a should
provide domain-specific metadata, or define a task to extract those. The required,
and optional domain-specific metadata fields can be defined in the DMP. This can
even ensure a homogeneous metadata quality across a larger group working
collaboratively on a joint project. In addition, the data life cycle should be defined,
i.e., what is the retention time, or what are the deletion policies. To meet the data
governance policies required by the , additional aspects such as control
must be defined to prevent unwanted data leakage.
4.4.2 Modifying Tasks
Compute jobs on systems are dispatched to the actual resources resource
managers such as Slurm. On this level, a job has to be prepared, annotated, and
linked to the step within the workflow description. This can be done with a simple
argument provided during job submission. In addition, a can further restrict
and specify the input data. Here, the largest change compared to the traditional interaction paradigm becomes apparent: Instead of working with explicit files,
a rather works with datasets defined by metadata. For instance, a specifies
the input data either on domain-specific metadata or simply due to the link
in the DMP. Therefore, the actual storage location is abstracted from the . The
actual data directory, which a program still needs to specify in API calls, can be 4.4. Novel Governance-Centric Architecture47
automatically exported via environment variables or generated via support tools in
the job script. Before reserving dedicated compute resources, the proposed DMP
tool decides to either synchronous or asynchronous data staging and stages the
data respectively.
One key requirement in science is reproducibility. In a first step, this requires at
least sufficient provenance information to allow for retrospective comprehensibility
of the lineage of the resulting artifacts. One important element to retrospectively
comprehend an job is the run script for batch processing. This can be
automatically archived along with the artifacts by the proposed DMP tool. However,
these batch scripts, which contain the actual compute job to be run in the form of
a shell script, can have multiple ambiguities. One simple example of this would
be the execution of an interpreted script, e.g. aPythonscript. Here, one would
have a simple line within the batch script which would look similar to:$ python
my_script.py
The challenge within this call is to track differences between multiple invocations
of this script, where the content ofmy_script.pyhas been changed. For this, three
different high-level modi are proposed.
The recommended way is to a Git repository, where changes in code are
properly tracked. In this case the DMP tool checks in the directory of the script
for the existence of a git repository and saves the information about the git
commit hash. Then, this information can be stored in the metadata of the created
data products. The DMP tool will create a dedicated sidecar file for this metadata in
the output directory. The usage of version control systems can, and should, also be
part of the required specifications within the DMP.
Alternatively, if no Git repository is set up, the batch script is parsed and un-
tracked dependencies in the namespace, a Python script, are tried to be
identified and archived along with the artifacts. Since this method is potentially
more error-prone compared to a proper version control system, Git, it is not
recommended, but should still offer a better chance for retrospective comprehensi-
bility when compared to other strategies. These dependencies will be listed in the
before-mentioned sidecar file, and are archived alongside it. One important distinc-
tion to make is the of containers. In this case, the container image should be
archived and linked to the sidecar file. Of course, utilizing provenance-specific tools
and translating them to the required standard in the sidecar file, is also an option to
explore.
The third option is that users compose the sidecar file by adding code to the
batch script, where this provenance information are provided. This can be added to
the directory where the output, which should be archived, resides. This file will also
override information that was automatically tried to extract in the previous step.
4.4.3 Implications of the Design
This presented design has different positive implications on the experience that
are discussed in the following, on the characteristics stated in Table 4.1.
IntegrationFirst of all, the abstraction of files on storage towards more high-level
data sets achieves the tight integration of storage and compute. Abstracting the stor-
age from the data will motivate users to proper metadata management systems
and establish a data catalog, instead of encoding information into file paths. 48Chapter 4. Integrating Compute and Data Management
PerformanceSince users are only working with datasets and not with a filepath
anymore, admins can configure data placement strategies, therefore relieving
this burden from the users and optimizing the performance.
ReproducibilitySince compute tasks, their input, and resulting data are strongly
linked with each other, the lineage of artifacts is much more comprehensible and less
subjected to errors. Utilizing further tools containers and a version control
system will ensure full reproducibility, which is integrated into this paradigm by
design because this is just another policy in the defined data governance, which will
be enforced for the users.
Enforcing DMPAlthough the general idea of data management plans in is far from , the novel advantage of this particular tool and its design is
that it can be enforced. That is because as a response to the control flow provided
and triggered by the , an information flow is received about the changes in the
overarching state, which is then acted upon. There are various ways to achieve this
goal. A naive compatible with existing systems is to a cronjob that
reads in the workflow and task definition files, which a has provided, and com-
pares the specified, desired state of the storage systems of the against the actual
state at hand. If output data are detected and the required sidecar file for the
necessary metadata is available, the output data is handled as specified. However,
if the required sidecar file is not, or only with insufficient content provided the will be reminded to provide the missing information after a specified grace time.
Similarly, if data is detected that can not be matched to the dataset specification in
the workflow definition, an error or warning can be raised to the as such unclas-
sified data shall not exist. Thus, the DMP becomes actionable and hereby ensures a
homogeneous system state in sync with the experimental description and ex-
pectations.
Predefined and Encapsulated JobsWithin the experimental description shown in 4.6, tasks are fully defined in advance. This includes the operations that are
done on the input data, as well as the software environment in which the process-
ing is done. The execution of these predefined, non-interactive jobs is similar to
the invocation of a predefined , as it is common within cloud infrastructure
providing a -as-a-Service (FaaS) interface. Thus, the FaaS idiom is an ideal
interface to provide homogenous to heterogeneous and ecosystem-agnostic
compute capacity. 49
Chapter 5
Remote to an System
In this chapter, the required communication layer between the DMS layer and
the system is designed. For this, the related work is analyzed and gaps
are identified in 5.1. Then the different design options are discussed
and the most suitable is chosen in 5.2. on this, a service
called HPCSerA is developed, where first the general architecture is presented in 5.3, the -as-a-Service capabilities are introduced in 5.4,
and then the security architecture is explained in 5.5. Details of the
implementation are provided in 5.6. Lastly, the general applicability of
HPCSerA is demonstrated in 5.7. Parts of this chapter are published in
[21, 22, 106, 194].
As identified within theDecomposition and Encapsulationstep in Chapter 4 a commu-
nication layer between a DMS and a compute cluster is required. Specifically for
compute-intensive tasks, an system should be utilized. Considering the de-
scribed mechanism in the DMS-centric paradigm 4.3.2, that a should
be able to process data via a DMS frontend, this application needs to connect to
the system on behalf of the . Therefore, one needs to establish a commu-
nication interface between the DMS frontend and the system, which ideally
happens within the space of the requesting . Usually, a would con-
nect to an system via SSH and is then directly working in a terminal, i.e., the
traditional interaction paradigm.
An alternative execution model popular with cloud systems is FaaS. As pro-
posed within the governance-centric interaction paradigm, doing large-scale, data-
intensive research on systems requires a DMP that consists of predefined func-
tions. Therefore, the FaaS interface is ideally suited to provide homogenous to heterogeneous and ecosystem-agnostic compute capacity. For this, the commu-
nication layer introduced in 4.3.2 needs to expose a FaaS interface. Within
the FaaS computing model, a platform for the execution of functions is provided,
i.e., code can be submitted and configured by the . Afterward, the can trig-
ger the execution of the with parameters via an exposed endpoint, where
communication is typically done via HTTP [59]. A runtime system executes the in an isolated environment, usually a container, and automatically scales
up the of running containers according to the of incoming requests.
This interface and its interaction with it become increasingly popular due to several
advantages, including cost-effectiveness, fault tolerance, and ease of . Providing
such an interface for systems would enable an -native drop-in replace-
ment. This enables users to choose for their individual cases and tasks if they
would benefit more from a cloud- system likeOpenFaaS[149], or if the full capability and environment are required. Therefore, one has a homogeneous
interface that can connect to different compute resources, depending on the compute 50Chapter 5. Remote to an System
intensity. This is one of the main goals stated in 2.2 and a main gap in the
current state-of-the-art of data lakes as identified in 3.7.5.
While, on one hand, there are efforts to ease and open up the of systems, providing graphical interfaces to manage complex and compute-intensive
workflows on systems [214], there is, on the other hand, a constant threat by
hackers or intruders. Since users typically interact with the host operating system
of an system directly, local vulnerabilities can be immediately exploited. Two
of the most favored attacks by outsiders are brute-force attacks against a password
system [182] and probe- login attacks [108]. These attacks, of course, become
obsolete if attackers can find easier to credentials. Therefore, it is of
utmost importance to keep , and credentials, to systems safe.
In this context, services easing the of and to systems should be
treated with caution. For example, if via SSH to an system is only pos-
sible usingSSH keysdue to security concerns, these measures are rendered ineffec-
tive if users re-establish a password- authentication mechanism by deploying a
RESTful service on the system that is exposed on the internet. Observing these
developments, it becomes obvious that there is a requirement to offer a RESTful ser-
vice to manage data and processes on systems remotely which is comfortable
enough in its usage to discourage spontaneously concocted and insecure solutions
built by inexperienced users with the main objective of “getting it to work”, but
which adheres to the highest security standards. In order to prevent those security
risks by users, systems are increasingly secured, including Two-Factor Authen-
tication (2FA) for SSH connections [30], which is a problem for automated workflows
in general and the offloading of compute-intensive tasks from the data lake in par-
ticular, since they need to run without any manual interaction.
To summarize, a of objectives for the communication layer between the
data lake frontend and the system can be defined:
• Communication should be REST- and follow the FaaS-idiom
• It has to scale to arbitrary amounts of users and calls
• Communication has to be secure and must not circumvent a second factor
5.1 Related Work
There is without question a general trend towards remote for systems,
for instance in order to web portals instead of terminals [31]. These applications
actually have a long-standing history with the first example of a web page remotely
accessing an system via a graphical interface dating back to 1998 [125].
Newer approaches are theNEWTplatform [43], which offers a RESTFul API in
front of an system and is designed to be extensible: It uses a pluggable au-
thentication model, where different mechanisms Open-Authorization (OAuth),
Lightweight Directory Protocol (LDAP), orShibbolethcan be . After au-
thentication via the/authendpoint, a gets a cookie which is then for further
. With this mechanismNEWTforwards the security responsibility to external
services and does not guarantee a secure deployment on its own. This has the dis-
advantage thatNEWTis not intrinsically safe, therefore providers of an system
need to trust the provider of aNEWTservice that it is configured in a secure man-
ner. Additionally, no security taxonomy is provided, which is key when balancing
security concerns and usability. 5.2. Analysis of Possible Designs51
Similarly,FirecREST[49] aims to provide a REST API interface for systems.
Here, the Identity and Management is outsourced as well, in this case toKey-
cloak, which offers different security measures. In order to grant to the actual resources after successful authentication and authorization, anSSH certificate
is created and stored at aFirecRESTmicroservice. Although this is a sophisticated
mechanism, there seem to be a few drawbacks. First of all, thesshdserver must
be accordingly configured to support this workflow. Secondly, it remains unclear
how reliable status updates about the jobs can be continuously queried when short-lived certificates. Lastly, these certificates need to be stored at a remote loca-
tion, which might conflict with the terms of service of the data center of the . A
similar is byHEAppE[193] where the communication is between the
API server and the system is done via SSH. To do so, for each project an SSH
key is managed by the API server. Users are not supposed to connect to the system
via SSH at all. However, in order to upload data via Secure Copy (SCP) users obtain
a temporary SSH key. To manage the exposure of a possible data breach of the API
server, the developers recommend one instance of HEAppE per account.
Additionally, systems are often configured to allow logins from a trusted
network only, which means that theFirecRESTmicroservice can not serve multiple systems at a time.
While theSlurm Workload Managerprovides a REST interface that exposes the
cluster state and in particular allows the submission of batch jobs, the responsible
daemon is explicitly designed to not be internet-facing [169] and instead is intended
for integration with a trusted client. Clients that shall execute Slurm jobs authenti-
cate the trusted Slurm controller via theMUNGEservice [58] that relies on a shared
secret between client and server. Slurm can be deployed across multiple systems
and administrative sites and there are various options for Slurm to support a meta-
scheduling scenario or federation. However, if the Slurm controller is compromised,
it can dispatch arbitrary jobs to any of the connected compute systems. In addi-
tion, decoupling the API implementation from the choice of the job scheduler, as we
propose, allows interoperation of multiple sites, possibly different schedulers.
In conclusion, there is currently no existing service that can fulfill all the require-
ments, i.e., a scalable and secure, REST- FaaS interface.
5.2 Analysis of Possible Designs
Before starting to design a system, a comprehensive comparison between the
different options is made in the following. All of the discussed methods can be
sorted into two different classes, those that push from a remote system to the system, and those that pull. Pulling requires that there is a process already running
on the system, which can perform this task.
5.2.1 Pushing Methods
All discussed pushing methods will be on SSH, however, with different work-
flows and server-side configurations. Therefore, in all of the following methods,
there is an external system, an API server, or the discussed data lake frontend,
which tries to build up an SSH connection on behalf of the . 52Chapter 5. Remote to an System
Password- Authentication
The simplest form of building up an SSH connection is by passwords. In this
scenario, the would provide the password to the external system, which then
has full to the space of the system. Since password authentication is
the most insecure, some, or even most systems do not support this mechanism.
Key- Authentication
In this scenario, a would provide his/her private SSH-key to the external ser-
vice. Then, this service can connect to the system and acquire a shell within
the space of the , similar to the before-mentioned password- authen-
tication. This mechanism can not support newer key authentication schemes the FIDO2- security keys. In addition, if the external system gets hacked, the
attackers can these private keys to connect to the system and likely com-
promise this as well.
Certificate- Authentication
SSH certificates seem the ideal solution for this problem. When the makes
the compute request on the external system, a ephemeral SSH certificate needs
to be created for this external system. This can be done an OAuth flow, that
connects to the identity provider which the system uses. Here, a can
acknowledge the request the external system made on his/her behalf. This step
can also be combined with a second factor. The admins of the systems can
freely configure the expiration date of the issued certificates. By only short-
lived certificates, the attack surface is drastically reduced. In addition, certificates do
not need to be written to disk, which further decreases the risk of theft. However,
this method requires special infrastructure that needs to be provided for each system. Also, this still gives the external system full to the ’s space,
which requires a large trust by the users in this external system.
Force Commands
To tackle the problem that a has to provide full to his/her space
so-called force commands can be . These force commands restrict the usage
of an SSH connection to a fixed executable. They can be configured for both, SSH
certificates and SSH keys. Although until now, only the trust a has to put into a
remote system is mentioned, there is also another dimension. service providers
might require users to keep their SSH guarded and basically forbid users to
grant full to a third party.
Therefore, when SSH keys with force commands, it is not enough to sim-
ply put the force command within theauthorized_keysfile within the space to
restrict the of a key. Instead, this needs to be supplemented by a correspond-
ingMatchstatement within thesshd_configfile. This requires an admin to add the
corresponding clause.
When SSH certificates, each certificate can be tied to a specific force com-
mand. This has to happen when the certificate is created usingssh-keygen. Certifi-
cate creation requires the private key of theCertificate Authority(CA). Therefore an
extra service would be required, that passes on the force command, an external sys-
tem wants to execute, to the server holding the CA. Ideally, this is combined with a
second-factor authentication where a can be even prompted with the command 5.2. Analysis of Possible Designs53
that should be executed. Although this would solve the problem, this requires a lot
of infrastructure and would consume large amounts of development and adminis-
tration time.
5.2.2 Pulling Methods
A different is to pull tasks into the space, instead of pushing into it.
The advantage of this idea is, that this can be developed and setup completely in space, therefore not relying on any processing by the admins. Therefore, from
the systems perspective, no additional setup is necessary, since users are only
doing, what they are typically allowed to. In both discussed implementations, a
process is running on the system in space, e.g., as a cron job. This task
then pulls the required information and data from an external source.
S3- Communication
The first is to S3 as a middle layer. Within a bucket, each unique job gets a dedicated prefix, e.g., a jobid. To this prefix, all the corresponding input
data can be uploaded. In order to enable the necessary control flow, a job specifi-
cation file has to be provided. That file is a simple JSON file, containing all infor-
mation required to execute a specific job, for instance, which job to execute, what
arguments to pass to it, environment variables that should be exported, a callback
URL, or where the input and output data should be stored. Additionally, a status
file has to be present for each job, i.e., has to be present within each prefix.
When a job should be submitted to the system, all these previously
described files are created and uploaded, and then the status of the job within the
status file is set tonew. The agent periodically crawls through all available prefixes
within the bucket and checks all the status files. Once this agent detects a func-
tion prefix with a statenewit will read the job specification file, stage the input data
accordingly, and submit the job with all optional arguments and environment vari-
ables to the batch system. Then, the state of the is updated torunning, and
the corresponding Slurm job-id is stored. The job status of allrunningjobs is con-
tinuously tracked by the agent. After a job is finished, the output data is uploaded
back into the S3 bucket into the same prefix, the state is updated tofinishedand
an optionally provided callback URL, within the job specification file, is triggered to
notify the external system that the job has been finished and that the output data can
be fetched from the S3 bucket. This mechanism assumes that the has to
the S3 bucket and that all functions, that shall be executed, are already available in
the ’sHOMEdirectory, ideally as a containerized image.
This workflow drastically reduces potential damage when tokens are
leaked since an attacker can only execute pre-configured jobs and can not gain full
shell which is required to exploit a local vulnerability. However, this illus-
trated workflow does not scale arbitrarily, since traversing through all prefixes by
the agent is (unnecessarily) time-consuming. In addition, one has to implement any
synchronization carefully since the eventual consistency of S3 does not allow sync-
ing processes via file locks. 54Chapter 5. Remote to an System
MethodSecurityUser Space
Limited Transaction
Safe
ScalabilityPortability
Implementation
Effort
SSH Password-+-++-+
SSH Keys-+-++++
SSH Certificates+--++0+
SSH Keys
Force Commands
+-+++-+
SSH Certificates
Force Commands
+++++--
S3+++0-++
HTTP API Server+++++0+
TABLE5.1: Qualitative comparison of the different evaluated com-
munication layers. A + indicates that the requirement is always ful-
filled, a 0 indicates that it is not always fulfilled but might be, and a -
states that this requirement is never fulfilled.
HTTP- API Server
The idea of a dedicated HTTP- API server has already been mentioned within 5.1. In this case, it is solely to support the control flow of a job submis-
sion and to solve the shortcomings of the previously discussed S3- method,
although it shall be emphasized that it can generally be in both, pushing and
pulling methods. a dedicated API server, instead of just an S3 bucket, allows in an otherwise
equivalent setup, that a job is triggered by calling a specific endpoint on the API
server. The job specification file is provided as a payload within this call. The API
server has several advantages, being fully transaction-safe and scaling at much
lower costs since no tree traversal is required. In addition to these advantages, they
can also be designed to be very secure and -friendly, as the remainder of this
chapter shows.
5.2.3 Evaluation
In Table 5.1 a qualitative comparison of the discussed method is shown, which sum-
marizes the previously discussed results. Due to security concerns, both SSH pass-
word and SSH key authentication are not feasible. SSH certificates are due to their
limited lifespan and their sole existence within the SSH agent more secure against
theft but still allow full shell to a third party. force commands with SSH
keys is also undesirable due to the continuous effort, that admins have to add cor-
respondingMatchclauses within thesshd_config. SSH certificates combined
with force commands seems the ideal solution, however, implementing the nec-
essary workflow to issue these tokens the mentioned OAuth flow where a can sign off on a certain command is extremely complex and costly. the
discussed pulling methods has the advantage, that no complex development and
setup by admins are required. The discussed S3 method, however, lacks trans-
action safety and scalability. Thus, the method a dedicated HTTP API server
and an agent that pulls all information into the space seems the most
promising way to tackle this problem.
5.3 General Architecture
HPCSerA consists in total of three components that enable the and remote
control of an system via a REST API. These three components as well as their 5.3. General Architecture55 API
RESTful API
Web UI
GitLab
XNAT
Data Lake
POST
Agent
Agent
Slurm
PBSPro
sbatch
qsub
GET/UPDATE
FIGURE5.1: Components of the proposed architecture, consisting of
clients, an API server, an systems.
interactions are depicted in 5.1: The main component is the API server, which
at first glance looks a simple message broker. Clients, shown on the left side in
green, can the REST API of the API server to post a task. On the oppo-
site side, there is a cronjob running, in the following called agent, which periodically
queries the API server for available tasks and pulls them if available. Once pulled,
the agent will execute the task and will update the state of the task on the API server
accordingly. This simple has several advantages:
• If the egress firewall rules allow to the API server, which would be pos-
sible even for systems which do not allow general internet , the
entire setup can be done in space.
• The agent is independently configurable. This means that the agent does not
require a fixed interface, a certain resource manager, and can be customized
to work with any kind of system.
• The agent can only do, what it is configured to do. Therefore, a can con- what should be exposed. The highest form of exposure would be to
allow arbitrary code ingest and execution, sending a shell script and exe-
cuting it. A smaller level of exposure would be to just allow the submission of
preconfigured batch jobs to the resource manager.
• A can hook an authorization mechanism into the agent in space
and therefore does not need to completely trust the administrators of the API
server. This mistrust allows a large exposure of the agent in a secure manner.
In the following the three components are presented in more detail.
5.3.1 The API Server
As a central component of theHPCSerAarchitecture, the API server handles HTTP
connections from the client and agent (described below), maintains the internal state
of all jobs and functions, and resolves dependencies between functions. In addi-
tion, it provides the necessary maintenance endpoints to allow configuration via a
web UI. It communicates with the database for the persistence of the internal state
and verification of any authentication tokens. Since every job has to be kept in the
database at least until it is completed, the API server is not stateless. All other con-
nections are initiated by other components, therefore the API server is the only part
of the architecture that has to allow incoming connections. It is also the responsi-
bility of the API server to ensure separation between jobs of different projects, i.e.,
these are only visible in response to requests that are authorized for the same project. 56Chapter 5. Remote to an System
Client 1
Client 2
API Server
Agent 1 2 3
///function1
//async-/function2
FIGURE5.2: Basic schema of the FaaS methodology.
5.3.2 The Client
Any application or service that needs resources as a backend implements the
Clientcomponent, which initiates HTTP connections to the API server in order to
submit jobs, call functions, and retrieve information on the job or state.
5.3.3 The Agent
On the system theAgentcomponent regularly connects to the API server in
order to retrieve jobs that are ready for execution. Depending on the being
called, the batch system might be involved and is regularly queried on the state of
each pending or running cluster job. This information is for further calls to the
API server in order to keep the job state up to date. In the case of calls which
depend on each other only via the cluster jobs they need to run, a corresponding
set of jobs including the dependency information is being submitted to the batch
system.
5.4 Advanced Execution Models
Extending on this general idea, a more formal execution model can be defined. Gen-
erally one can observe that the execution model of predefined tasks triggered by a
REST call is the aforementioned concept of FaaS.
5.4.1 FaaS for In FaaS it is typical that a has a preconfigured task or which is pack-
aged into a container to be called with varying inputs. These functions are avail-
able by -defined REST endpoints. Since in HPCSerA every has a dedicated
namespace on the API server, this expected behaviour can be replicated on an system the respective scheduling mechanism for batch jobs.
The basic mechanism for this is shown in 5.2. It can be seen that the can send REST requests to the API server resembling FaaS requests. For this,
each has their own namespace/<username>//<functionname>, where
custom functions can be registered at their own discretion. It is important to state
that the name must be unique within the namespace of each and is
not being further isolated by additional structures the notion of projects. Once
a client has posted a call to the API server, it will be available for the agent
to be pulled with the corresponding GET request. The agent then actively pulls
these calls and dispatches them by calling a starter script with the same
<functionname>located in a preconfigured path, e.g., in the ’s home directory
in∼/hpcsera/functions/<functionname>. These functions, which are then being
executed, can be anything. It can be a Bash script that is being executed on the
frontend, it can be a script fetching data from a remote source and staging it on the
fast parallel filesystem of the system, or it can be a simple job submission to 5.4. Advanced Execution Models57
the respective batch system, to give just a few examples. Since only executables are
being executed, there are no inherent limits to the capabilities of these functions.
5.4.2 Long Running vs. Short Running Functions
Since HPCSerA does not enforce any boundary condition on the -defined func-
tions, it is important to differentiate between long-running and short-running func-
tions from the beginning. The most important difference from the ’s perspec-
tive is that long-running functions will be usually executed asynchronously, whereas
short-running functions can be executed synchronously as well. The reason is that
in this case a Transmission Control Protocol (TCP) connection between the client
and the API server can stay established during the entire time. Therefore, the HTTP
response will correspond to the output of the , see 5.4.8, e.g., a re-
sponse code200would directly mean that the ran successfully. There might
even be some payload data attached to the response, which can be immediately by the client. The client process is blocked for the duration of the HTTP re-
quest. These functions, however, do not only need to have a short runtime, but also
need to have limited resource requirements. In those cases, an oversubscribed queue
(commonly to enable interactive jobs) can be , which can be created and
managed by typical resource managers Slurm.
In the other case, during an asynchronous execution the client would
get an immediate HTTP response from the API server. Here, the return code202
would however only mean that the request to execute the was successfully
accepted from the API server. This allows for the established TCP connection be-
tween the client and the API server to be terminated. Therefore, the client process
would only be blocking for the duration of the initial communication with the API
server, but not for the entire time the needs for processing. However, this
leaves the client without the optional output data of the , which might be
required. This can be solved on the client side by providing a callback URL in the
HTTP header when the initial REST request is made. The API server or the agent
would in that case make a callback to the client once the has finished. This
is possible since the API server offers statefulness for the functions. Since the API
server itself is not meant to handle large data transfers, usually S3 will be for
these cases. Therefore, it might be advantageous to implement some event handling S3 rather than the API server.
About the differences between synchronous and asynchronous jobs which re-
quire to the compute nodes that are managed by a dedicated resource man-
ager Slurm it can be stated from the perspective that the synchronous case
usessrun, whereas the asynchronous call is usingsbatch.
For HPCSerA a single configuration is enough to execute the same func-
tion synchronously and asynchronously. The client can then choose the mode of
execution at runtime and just distinguishes between those modes by a differ-
ent Endpoint, i.e., either the/<username>//<functionname>for the syn-
chronous execution or the/<username>/async-/<functionname>for the
asynchronous execution.
5.4.3 Differentiation Between Control Flow and Data Flow
In general, one has to make a distinction between a control flow, and a data flow.
The control flow represents the flow, i.e., the transfer of instructions that should be
executed. One common example of such a control flow is SSH. On the other hand, 58Chapter 5. Remote to an System
the data flow represents the transfer of data, for which SCP is a common example. As
already motivated, HPCSerA is designed to enable only the control flow as a first-
class citizen. Therefore, one could compare it to SSH, just that HPCSerA is HTTP and the FaaS idiom, instead of providing to a terminal session.
Although the usefulness of sending small payloads along with an HTTP request,
for example as input data to a , is acknowledged and should be offered as
previously described, the general separation of concerns should be considered more
seriously for larger payloads. Here, some systems will even provide different
nodes for these two different tasks. For instance, the usual frontend nodes for
the control flow, where the can connect via SSH, or instantiating functions HPCSerA, can be accompanied by dedicated data mover nodes, which are only for transferring data.
There are different ways to move data to and from systems via HTTP, rang-
ing from deploying an NGINX in space as in Open OnDemand [87] for
push , to object storage systems, the aforementioned S3, for a pull .
Considering this wealth of options, one can justify neglecting the data flow in a first
attempt to build this system.
5.4.4 Remotely Building Complex Jobs
Offering a FaaS infrastructure on which enables long-running, data-
intensive, and highly parallelized functions is a useful addition for those users who
are already in the FaaS ecosystem. There is, however, also the -native group, where people would to be able to and an system as be-
fore, just with a RESTful interface. In order to combine these two scenarios, a closer
look at the typical usage is necessary. The usual workflow for users working
on systems can consist of several steps:
• The environment and binaries for the computation are prepared. This is mostly
done interactively on the frontend.
• Input data for IO-intensive applications is staged on a fast parallel filesystem
prior to the job submission.
• Last changes to the batch script are done and the job is submitted to the batch
system.
• After job completion the results can be inspected and possibly backed up.
Since there are no restrictions on the capabilities of the functions, one can recreate
the workflow described above under two conditions:
• The execution of a can depend on conditions.
• Code ingest needs to be supported.
The first condition is derived from the requirement that a job can only be submitted
to the batch system once its environment is built and its input data is staged. There
can also occur other examples and more complex conditions. Since HPCSerA is not
in any way supposed to replace a workflow engine it is also not supposed to handle
complex conditions on its own. Therefore, one can only add the condition to a func-
tion that it should start only after one or more other functions have (successfully)
finished. The logic to determine whether a call has been successful or not
has to be within the itself. 5.4. Advanced Execution Models59
Example Job
Prepare BinaryStage Input Data
Dispatch Batch Job
Postprocess Data
FIGURE5.3: Sketch of a Job data structure in which four different
functions are organized.
In order to build complete end-to-end jobs with this mechanism these func-
tion calls need to be embedded in a suitable data structure.
In 5.3 it is shown that all calls are organized within a data struc-
ture called Job. A has to first create a Job, which gets a uniqueJobID
assigned by the API server. Afterwards, a can call functions within the context
of a Job. These calls then get a -ID associated to them. Conditions
can be assigned to these calls, i.e., other functions within this Job structure
have to be (successfully) finished before this can start. This mechanism
allows building up a typical, multi-step job as described above, by calling con-
secutively the exposed FaaS REST API. Independent calls will be executed
concurrently. In the example in 5.3, this applies to both thePrepare Binaryand
Stage Input Datafunctions which have no dependencies.Dispatch Batch Job, on the
other hand, can only be run once both previous calls are completed. Finally,
Postprocess Datais run once all other functions are completed.
Alternatively, one can define a single job in HPCSerA a single YAML
file. In this case all functions need to be known when submitting the job request to
the API server. If not specified, the functions are executed in the order they appear
in the YAML file, and an implicit dependency on the previous is assumed.
In the consecutive buildup where independent functions are called in the context of
a job, additional calls can be issued to the same Job-ID at a later time.
5.4.5 Virtual Calls
Since all dependencies that HPCSerA is supposed to resolve should only cover the
exit status of a , i.e., with or without error, a mechanism is needed to map
more complicated conditions onto this boolean. One example of such a more com-
plicated condition would be that a should only start after some special re-
source, a certain block device, is provisioned.
To cover these cases, one can defineVirtual Functions. These functions differ from
normal functions in that they do not need to be pulled by the agent and then need
to be executed. Instead, these functions are only existing on the API server. There
they expose a REST endpoint, where from an external source the state of the Virtual can be changed. This means that some external program can make a REST
call to that endpoint to set the state of the to (successfully) finished. This
is an alternative, similar to the call-back URLs provided by clients when triggering
an asynchronous . When an external REST call is , the necessity to ex-
ecute functions which do busy waiting to check if a certain condition is met can be
avoided. However, functions that do busy waiting can also be in a straightfor-
ward manner, as long as the necessary logic is implemented to differentiate between 60Chapter 5. Remote to an System
the waiting state since the condition is not yet satisfied and the failed state where
the condition will be never satisfied. In the latter case, the should be ter-
minated with the corresponding unsuccessful state. If a proper failure condition can
not be formalized, when a condition fails and will not be met in the future, a final
wall-clock time should be specified after which the is terminated and the
state of the is unsuccessful.
5.4.6 Configuration
There are two different ways to configure and deploy a . The first option is
to connect to the system via SSH and prepare the executable which is called
when the is triggered. This executable can be a binary, or a shell script,
for instance. In case a binary should be executed within a certain environment, one
can wrap the call of the binary within a shell script. If more complex environments
are required, the executable can be packaged together with its dependencies into a
container, e.g., Singularity/Apptainer [105] can be . Once the is config-
ured within such an SSH session, it can be called afterward by the agent, therefore it
is also immediately available for the client on the API server.
The alternative is to configure a via the API server, i.e., completely
within the context of HPCSerA. For this, all necessary files can be zipped or tarred
and sent along with the configuration request, which is available on a dedicated
REST endpoint. The agent will then accept the archive, unpack it into a temporary
directory, and will execute a preparation executable. This preparation executable
can be as simple as just copying the executable into the directory
of the agent. More complicated examples may include some code that needs to be
compiled. For large files, a Singularity container, it is recommended to upload
those via REST via the dedicated data flow mechanism,e.g., to an S3 Bucket. Then
just passing a preparation executable to the agent, which fetches this large file from
the remote bucket and places it in the necessary path on the system, is enough
to configure such a .
5.4.7 Passing Arguments to Functions
Some or even most functions will require that some arguments are passed to them
when calling them. These can be passed to the API server of HPCSerA either as URL
query parameters or as a JSON file. In the first , an arbitrary list of key-
value pairs can be passed with the calling REST endpoint, e.g.,/<username>/func-
tion/<functionname>?k=val. This call would forward the keykwith the valueval
to the in two possible ways: Either the agent would export an environment
variable<PREFIX>_kwith the valueval(where<PREFIX>can be set by the )
before calling the executable corresponding to the called or alternatively,
these key-value pairs can be formatted into a single command line string which is
appended to the binary call, as it is common when executing an executable on a
Linux shell.
In case a requires more extensive arguments, this previously discussed
method is not handy anymore. Instead, one can a JSON file which is sent along
with the REST request to trigger the . This JSON file is then simply for-
warded from the API server to the agent which accepts the JSON file and stores it
locally. The file path can then be passed as an argument to the . Neither the
API server nor the agent will in any way process the content of the JSON file. If a 5.4. Advanced Execution Models61 requires this kind of complex input data, the logic needs to be implemented
by the itself or a wrapper script.
5.4.8 Returning Output Data to the Client
When a call is completed, the method of returning its results depends on
the mode of execution and the volume of the produced data:
• For synchronously executed functions, see 5.4.2, the results are avail-
able by the time of completion and can be included in the HTTP response.
If applicable, the results can be completely included in the form of a JSON
structure produced by the , e.g., for scripts that query the status of the
system, such as custom calls of the batch system or storage Command Line In-
terface (CLI) tools. Binary data, such as base64-encoded Binary Large Objects
(BLOB) can be included, although it is recommended, especially in the case
of a high volume of produced data, that the JSON structure merely contains
information about the location of the output data, for example, a file system
path on shared storage or the URL of an S3 bucket.
• If the call is asynchronous, only information about the data struc-
tures created on the API server can be relayed, in particular theJobID, or
FunctionID. This has to be kept on the client side and for later status
requests.
5.4.9 Error Handling
Since the functions have a state, which is managed by the API server, and for in-
stance distinguish between a successful and an unsuccessful exit, the -defined
functions are ideally able to distinguish between those states. However, some error
in the code execution itself is not the only error that can occur. It could also happen
that during the execution the process is unexpectedly killed, or the host is
suddenly turned off, for instance, due to a power outage. When the agent is able
to detect those interruptions, where a stopped processing without sending
a proper exit code, it assumes that a crash unrelated to that particular has
happened and will trigger the execution again. In order to support this
behavior, a should be idempotent, i.e., it should be possible to execute a multiple times with the same input parameters and it will always produce
the same output.
5.4.10 External Job Dependencies
With a slight modification in the data structure describing a job and the included
functions, the proposed architecture can support medium-term storage of campaign
data as well: The set of functions within a single YAML definition file is originally
designed to be run in order, but a more generic solution is given by implementing
a DAG of dependencies. Hereby, each can define one or more dependen-
cies on another , which can either exist in HPCSerA or represent an external
event via a virtual . The latter is marked as done via an external source, for
example when campaign storage or a data source is ready to be as job input
data. This workflow is typically for research projects and can include depen-
dencies between compute jobs, storage provisioning, and data migration. However,
the conventional linear chain of functions is still included as the special case where 62Chapter 5. Remote to an System
Job A A1 A2 A3
Job B B1 B2 B3
Batch job 1
Batch job 2
FIGURE5.4: Jobs with implicitly defined dependencies and a custom
dependency DAG.
each step depends on its predecessor. As shown in the first half of 5.4 this
variant is implemented via dependencies between functions. However, in the more
general case, as depicted in the second half, multiple functions could depend on the
same prerequisite, in this caseB1. If a subset of the calls is implemented
via batch jobs and all other dependencies pointing outside of the set are already ful-
filled, the corresponding subgraph can be submitted in one step, thereby delegating
further resolution of the remaining dependencies to the batch system.
5.5 Security Architecture
The general idea for authentication on the API server is on staticbearer
tokens. The separation of tokens by the who created them and the ser-
vices (clients and agents) to which they are deployed, already enables revoking
trust by the API server in a setup with multiple services and multiple backend systems. However, during operation, there is global to the entire state for all
parties involved. In order to segment trust between groups of services and backends, fine-granular permission management is required. Therefore, each token
can be assigned one or multiple scopes, i.e., permissions, that restrict the combina-
tion of HTTP endpoints and verbs for which it can be . The token’s lifetime is
implied by the granted scope but can be adjusted by an admin. control over each individual task and job that is allowed to be run or sub-
mitted, respectively, is enforced by introducing an intermediate authentication step
that requires interaction via an external application. This could be run on a mo-
bile device or hardware token, the ones being for 2FA or integrated into
the web- interface for token and device management for fast iter-
ations on the workflow configuration. Metadata about the action to be authorized
is included in the prompt in order to allow an informed decision. However,
the measure is restricted to this most critical step of the process, while non-critical
endpoints, such as retrieving the state of pending jobs, can continue to respond im-
mediately. For submitting a job, the necessity of individual confirmation is
also determined by whether code is ingested or an already existing job is merely
triggered to run on input data.
From the ’s perspective, setting up the workflow would start with logging
into the web interface and creating tokens for each service to be connected to the API,
and configuring them in each client and agent, respectively. In order to acquire a
minimal working setup, at least one token for the client service and one for the agent
communicating to the batch system on the backend system would be required.
OAuth-compatible clients could initiate this step externally, thereby sidestepping
the need for the to manually transfer the token to each client configuration. As 5.5. Security Architecture63
soon as each client has acquired the credentials either way, jobs can be relayed
between each service and the agent.
While the OAuth 2.0 terminology [79] allows a distinction between an autho-
rization server, which is responsible for granting authorization and creating tokens, and a resource server, which represents control over the entities exposed by
the API, in our case the tasks and batch jobs to be run, both roles are assumed by our
architecture, so the design can be as simple as possible and deployed in a single step.
However, since the endpoints for acquiring tokens and the original endpoints
that require these tokens are distinct, a separation into microservices (which
again need to be authenticated against each other) would also be compatible with
the presented design.
The steps necessary for code execution are illustrated in 5.5. As a pre-
liminary, it is assumed that the agent is set up and configured with the REST
service as an endpoint. The arrows indicate the interactions and the initiator. The
individual steps are as follows:
1. The workflow starts with a logging into the web interface. The Single
Sign-On (SSO) authentication for this purpose has to be trusted since
forging the ’s identity could allow an attacker to subsequently authorize a
malicious client to ingest arbitrary jobs.
2. The can create tokens for the REST service in the web UI.
3. The tokens are stored in the Token Database (DB), along with the granted
scope, project tag, and token lifetime.
4. The retrieved tokens can then be by a client, e.g., to run some code on
the system or have an automatic process in place, provided the code is
already present on the system, rendering manual authentication unnecessary.
5. The request is forwarded to the REST Service, which verifies the information in
the Token DB. On success, the code to execute is forwarded to the agent.
6. If the client chooses to the OAuth flow instead in order to avoid manual
token creation, the authorization request is forwarded to the Auth app instead.
7. The can choose to confirm or deny the authorization request. In the former
case, the generated token is stored (compare step 3) in the Token DB. Again,
further requests can then in general proceed via step 5 without further interaction.
8. any other client, the agent uses a predefined token or alternatively
initiates the OAuth flow in order to get to the submitted jobs.
9. For the most critical task of ingesting code into the system but also op-
tionally for other tasks such as executing code on the frontend or sub-
mitting batch jobs, the agent can be configured to get consent from the by the Auth app for authentication. This request is accompanied by meta-
data about the job to be executed, such as a hash of the job script, allowing
an informed decision by the . This step also avoids the need for trust in a
shared infrastructure, since the authentication part can be hosted by each site
individually. 64Chapter 5. Remote to an System
potentially shared infrastructure
in scope of HPCSerA development
Token DB
HPCSerA
Client agent
Batch system
Auth app
web UI token
4
Manage Tokens
1
8
Store
3
Validate
5
2
Get consent
9
6
2FA
7
CLI
10
FIGURE5.5: A sketch of the proposed token- authorization
flow.
10. Once the has confirmed the execution, the agent executes the code,
e.g., by submitting it via the batch system. In this case, information about the
internal job status is reported back to HPCSerA.
It can be assumed that the agent is secure as otherwise the system and account it runs on are compromised and, hence, could execute arbitrary code via the
batch system anyway. The web UI, agent, HPCSerA Service, and client are all
independent components. For example, a compromised REST Service could try to
provide arbitrary code to the agent anytime or manipulate the ’s instruc-
tions submitted via the client. However, as the will be presented with the code
via the authenticator app and can verify it similarly to 2FA, the risk is minimized.
There are two approaches for deploying HPCSerA across multiple clusters and
administrative domains:
ReplicationEach center could deploy the whole HPCSerA infrastructure, compare 5.5, independently, maximizing security and trust. By adjusting the endpoint
URL, a could connect via the identical client to either the REST service at one or
another data center – this is identical to the URL endpoints in S3. Although the now has two independent web UIs for confirming code execution on the respective
data center, the authenticator and the identity manager behind it could be shared.
An additional advantage of this setup would be that the versions of HPCSerA de-
ployed at each center could differ.
Shared InfrastructureThe maximum shared configuration would be that for each system, a has to deploy a dedicated agent on a frontend node, but
all the other components are only deployed once. As the agents register them-
selves with the REST service, the users can decide at which center they would to execute any submitted code. While a single web UI for many centers and
cloud deployments maximizes usability, it requires the highest level of trust in the
core infrastructure: If two of these components are compromised, arbitrary code can
be executed on a large of systems. However, authentication for to the
web UI via the ’s existing account from their center can be implemented
as SSO OpenID Connect Federation. 5.6. Implementation65
Role NumberRoleDescription
1GET_FunctionStatusClient can retrieve information about a submitted 2UPDATE_FunctionStatusUsed by client/agent to update the status
3GET_FunctionEndpoint by the agent to retrieve information
4POST_CodeClient to ingest code to the sytsem
5GET_CodeAgent pulls code. Might be necessary to run 6POST_FunctionClient triggers parameterized 7UPDATE_FunctionClient updates already triggered 8DELETE_FunctionClient deletes already triggered TABLE5.2: Definition of the eight roles. Operations marked in red
have to be considered security critical from the admin point of view,
whereas the orange marked operations from a point of view.
5.6 Implementation
In the following, more details about the technologies chosen for the implementation
are provided with a focus on the current scope definition and the authentication/au-
thorization scheme employed. Generally, the OpenAPI 3.0 [148] specification, which
is a language-agnostic API-first standard for documenting and describing an
API along with its endpoints, operations, request- and response-definitions as well
as their security schemes and scopes for each endpoint inYAMLformat, was to define the RESTful API. This API is backed by aFLASK- web application
written inPython. The token database is in a SQL-compatible format, thus SQLite
can be for development and, e.g., PostgreSQL for the production deployment.
The database schema contains only the (user_id) and project (project_id) that
the token belongs to as well as the individual permission-level (token_scope).
5.6.1 Definition of Roles
In order to give granular permissions for accessing each of the endpoints, OpenAPI
3.0 allows to define multiple security schemes providing different scopes to define
a token matching to the security level of each of the endpoints. Eight different roles
have been identified, which are listed and described in Table 5.2.
These roles are entirely orthogonal, which means they can be combined as nec-
essary. If, for instance, on one system only parameterized functions need to be
submitted, the agent can be provided with a token that has only the permissions of
roles 2 and 3, thus lacking role 5, which is required to fetch files. Similarly, if
a token is provided to a client that is not 100% trustworthy, one can choose to only
provide a token with the role 6, i.e., to only allow to trigger an already registered
. Important to understand is the difference in mistrust between the roles 3,
4, and 5. The security mistrust in role 4 comes from the admins , who want to en-
sure that a code ingestion is indeed done by the legitimate . Therefore, in order
to allow code ingestion, the possession of a token with the corresponding permis-
sion is not enough, the has to confirm the code ingestion via a second factor.
The mistrust in roles 3 and 5 comes, however, from the , who wants to ensure
that only functions s/he confirmed are being executed. This is, again, completely
orthogonal, to the enforced second factor in role 4 and can be optionally by the
. This fine-grained differentiation between the different security implications of
the discussed endpoints minimizes interference while providing a high level of
trust. 66Chapter 5. Remote to an System
5.6.2 Providing Tokens via Decoupled OAuth
The introduction of OAuth-compatible API endpoints has several advantages: Ac-
cess tokens can be created on demand in a workflow initiated by a client or agent. In addition, while there is a default API client provided, a standard-compliant
API enables users to easily develop drop-in replacements.
It is important to note here that the usual OAuth authorization code flow, where
a client gets redirected to the corresponding login page to authorize the client is
modified. This “redirect ” has two problems:
• The client is a weak link, where the Transport Layer Security (TLS) encryption
is terminated and therefore becomes susceptible to attacks and manipulation.
• It does not support a headless application, the agent, which is not
able to properly forward the redirect to the .
Due to these shortcomings, a modified OAuth flow is developed to enable the usage
of headless apps and improve security. This modified version decouples the confirmation from the client, which means that the client is not being redirected but
that the confirmation request is being sent out-of-band, e.g., via the web UI or via
notification on a smartphone.
Starting with the case that the script does not already come equipped with a to-
ken, analogous to the usual OAuth flow, the generation of a token is requested. Since
the case is initially built as an instance of machine-to-machine interaction, i.e.,
headless, the issue of a lack of interface is encountered; the usual OAuth flow
- implemented in the browser - would redirect the to an authorization server
where the could actively provide their username and password to the autho-
rization server. The authorization server would then return a code, in the case of the
authorization code flow, in the redirect URI, which would be posted in a backchan-
nel, along with a client secret assigned at the time of registering the client to attain
an token.
In order to circumvent this headless app problem a synchronous push notifi-
cation system analogous to the Google prompt where a notification is pushed to a
’s device awaiting a confirmation to proceed is . In the Minimum Viable
Product (MVP), it is implemented in the SSO-secured web UI in order to have a
more integrated interface. Eventually, the final product will see an Android and iOS
app that receives such notifications. This flow then grants the permission to execute
a security-critical operation, compare table 5.2.
This confirmation via push notification cannot solely rely on time synchronicity
since it would be susceptible to an attacker requesting tokens and/or 2FA confir-
mation for carrying out a security-critical operation in the same approximate time
frame. Therefore, a sender constraint has to be implemented. This is done in a sim-
ilar way to the original authorization code flow: The code is signed with a
client secret which is configured with the API server prior to the execution of this
workflow, and then sent to the API server. The API server verifies the secret and
only then sends the actual token. This secret is implemented public-private
key pairs, where the public key is uploaded to the API server in the initial setup to
register a client or agent.
Alternatively, in the case that a token is supplied along with the software or script
that is submitting a job to the HPCSerA API, the permissions are validated against a
token database. In the case that the token provided contains permissions for -
ing a sensitive endpoint, the second-factor check is triggered through the web UI,
and the notification/confirmation process is once again undergone. It is important 5.7. Demonstration with Different Cases67
to note that this is not a hindrance since already-running functions and non-sensitive
endpoints proceed without intervention.
5.6.3 Mapping of Roles to Functions
In order to provide the with a FaaS interface that is capable of handling auto-
mated machine-to-machine communication of headless apps the previously defined
roles need to be mapped on the FaaS endpoints. The most important differentia-
tion is still between thePOST_Functionrole and thePOST_Coderole. The latter is
required when a wants to configure a via the API server. Here,
the can upload code either directly as an archive, or via an external storage
target. Therefore, the configuration of a corresponds to thePOST_Code
role. The client making that request needs to have these elevated rights.
On the other hand, simply triggering the execution of an already configured
, for instance on input data, corresponds to thePOST_Functionrole.
As shown in Table 5.2, this role is not security-sensitive. Thus, it can be by a
client without any manual interaction as long as the client has a token with the cor-
responding role. Therefore, HPCSerA can support automated FaaS functionality for
its clients.
The agent side is not considered critical for the admins, but optionally criti-
cal for the users if they distrust the API server. Thus, they want to implement
their own 2FA mechanism here. To support the previously discussed endpoints,
the client needs to either theGET_Functionrole to receive the request to exe-
cute a or theGET_Coderole to pull some code and to configure a . The agent would then also require theGET_FunctionStatusrole and the
UPDATE_FunctionStatusrole to manage the state of the functions, which is main-
tained on the API server. Via theUPDATE_FunctionStatusrole the output data of a can be send to the requesting client. The client would then also need the
UPDATE_FunctionStatusrole in order to be allowed to receive the output data.
5.6.4 Assigning Roles to Clients and Agents
The fine-grained distinction between those different roles, discussed in 5.6.3,
is an important part of providing the highest level of security while enabling a high
degree of automation. This means especially that users should only tokens with
the minimum roles attached to them. For instance, if a configures functions
always manually within an SSH session, the token of the agent should not have
theGET_Coderole enabled. Users define the roles of the tokens in the current setup
within the web UI. Here, users can either check the needed roles, create a token, and
copy it out of the web UI or they can, upon request from a client or agent via the
presented detached OAuth flow, choose which roles should be associated with the
token that will be created. Once a token has been created, it can also be revoked if it
is not needed anymore or a potential breach is assumed.
5.7 Demonstration with Different Cases
The purpose of including the extraDecomposition and Encapsulationstep with the fol-
lowingProblem Generalizationstep in the employed scientific method is to maximize
the overall impact of the research artifacts. To prove the general applicability and its
importance, the usage of HPCSerA in different scenarios is discussed in the follow-
ing. 68Chapter 5. Remote to an System
5.7.1 Implementing Workflows with an External Workflow Engine
In order to implement workflows with an external workflow engine to automate
the orchestration, HPCserA can be . Due to multiple repetitions and time de-
pendencies, manual interactions severely limit the functionality and practicability
of a workflow, thus jobs should be fully automated without interaction.
Therefore, the of pre-configured functions is encouraged. There are various lev-
els where dependencies between jobs can be managed. The following descriptions
and examples refer to 5.6:
1. The preferred way to do the dependency resolution required to perform auto-
mated workflows is by completely handing over the involved logic to a work-
flow engine. In this case, workflow tasks are submitted as individual functions
via HPCSerA. If there is a dependency between two jobs that require a batch
job to finish, on completion of the first cluster job the agent updates the job
state on the API server from which the workflow engine eventually obtains
the state. In the example, this is the requirement to proceed from Task
I to the dependent Task II. Only then can the second job be submitted to the
API and is finally retrieved by the agent and submitted to the batch system. In
conclusion, this variant is the easiest to implement but involves a high amount
of latency for resolving job dependencies.
2. For jobs that are submitted with multiple IDs, the API server will
handle dependencies by only providing calls to the agent for
which all calls on which they depend have been successfully com-
pleted. Compared to the previous scenario, once the agent has marked the last
batch job of Job A (A2 in the example) as completed, the status of A2
on the API server is updated and the next one ( A3) can be immedi-
ately retrieved and run. While the dependency chain has to be implemented
by building more complicated calls to the REST interface, there is no back-and-
forth communication with the client contributing to the latency.
3. In view of 5.4.10 the most low-latency resolution of job dependencies
occurs when multiple -IDs which contain batch jobs are presented by
the API server to the agent. In this case, the completed first batch job (A1)
directly leads to the scheduling of the second batch job (A2) by the batch sys-
tem without interference from any HPCSerA components. Here, the resource
scheduler of the system takes over the dependency resolution at runtime.
To do so, the agent has to forward these dependencies at submission time.
5.7.2 GitLab CI/CD
Since theGitLabRunner can be configured to run arbitrary code without includ-
ing secrets in the repository, thanks to GitLab’s project Continuous Integration and
Integration Development (CI/CD) variables [68], the required tokens can be made
available to the CI/CD job so it can in turn the API endpoints required to
transmit the current repository state to an system where the code can be tested the software environment or even multiple compute nodes.
A commit might of course introduce arbitrary code to the environment,
therefore it is advisable to enforce the extra authentication step when code from a commit is submitted to the system. The corresponding hash, available by
default via theGIT_COMMIT_SHAvariable, would be a helpful piece of information to
display to the when asking to authorize the request. 5.8. Summary69
flow of time
Workflow Engine
API server system
Task I
Job A A1 A2 A3
Batch job A1
Batch job A2
Task II
Job B B1
Batch job B1
1
2
3
FIGURE5.6: Overview of the levels at which dependencies
can be resolved.
5.7.3 XNAT
The Extensible Neuroimaging Archive Toolkit (XNAT) [119] is a research data man-
agement system for neuroimaging and related data. It organizes data within a hier-
archical structure, starting withProjects, which consists ofSubjectsandExperiments.
Individual measurements, e.g., anMRI Scan, are associated with a Subject via anIm-
age Session. XNAT offers a we UI and a REST API, through which users can interact
with the system. For instance, users can look at MRI scans in their browser the web interface, or they can start an analysis task a drop-down menu on
the available data. These tasks are executed by pre-configured Docker containers.
The corresponding Docker daemon can either run locally or on a dedicated server,
which allows for some compute capacity to be provided. However, for large-scale
and compute-intensive jobs for which an system is favorable, no simple solu-
tion is available.
To provide one possibility to enable job offloading from the XNAT web inter-
face to an system, a curl- client is built within a Docker container and
registered within the XNATContainer Plugin. The container can be started the
Container Plugin. The can then provide the name s/he wants to execute
on the system within a pop-up window. Then the Docker container is started
as usual theContainer Plugin, whose whole purpose is to send the correspond-
ing REST request to HPCSerA. These requests are processed and the corresponding
functions are started. Important is, that to enforce the control lists of XNAT,
data is always accessed by the functions through REST API of XNAT. Similarly,
resulting artifacts have to be ingested into XNAT via the REST API by the to make the data available within XNAT.
5.8 Summary
In conclusion, within this chapter, a concept for a secure communication layer be-
tween a remote interface, such as a data management system, and an system is proposed. This concept enables a RESTful FaaS interface for sys-
tems. Since these FaaS- interfaces are increasingly common in different com-
pute ecosystems, this interface is capable of providing ecosystem-agnostic comput-
ing infrastructure. on this idea, a general architecture is designed consisting
of a stateful API server and a stateless agent, which is deployed on the sys-
tem. Leveraging the FaaS idiom within this interface enables the definition of a 70Chapter 5. Remote to an System
fine-grained role- model, which allows the execution of pre-configured
functions on arbitrary input data without requiring the interactive confirmation of a
second factor by the . The remarkable aspect is that it is shown that this is pos-
sible without relaxing the generally applicable restrictions, even if the center requires a second factor for SSH . This key characteristic enables the
automatic execution of pre-configured pipelines on systems without any interaction. on this concept, a novel service called HPCSerA is developed
and extensively tested within different cases. In all of the presented cases,
HPCSerA is able to orchestrate the control flow between a remotely deployed client
system and an system. To summarize, HPCSerA can be to provide a
generic communication layer between a remote DMS and an system and as
such can be to design a data lake architecture that uses a FaaS idiom to offload
computations to cloud and infrastructure depending on the compute intensity. 71
Chapter 6
Data Lake Design
Within this chapter, a general data lake architecture is designed, which can fulfill
the requirements of the described case as discussed in 2.2. For this,
the advantages and disadvantages of the abstractions currently in data lake
designs are discussed in 6.1. on this, a novel Digital Object-
architecture is presented in 6.2. This design is extended to incorporate
FAIR Digital Objects in 6.3. The applicability of the Digital Object- data lake design is demonstrated in 6.4 by the usage within the
"Big Data MRI"-project. Parts of this chapter are published in [21, 144].
Within this chapter, a novel data lake is designed to serve as an institution’s cen-
tral data repository. For this, it needs to integrate heterogeneous data from diverse
sources, which requires a generic data modeling technique. This data lake should be
specifically focused on researchers with data and compute-intensive projects, which
require the integration of scalable storage systems and similar powerful compute
infrastructure. However, this compute infrastructure should not be provided by a
single ecosystem but should support diverse systems. This capability is a general
lack in state-of-the-art data lakes as it is identified in 3.7.5. To provide scal-
able compute infrastructure, cloud and systems are of particular interest since
these are by far the most widely compute clusters. on a detailed analysis
in Chapter 4, a unified interaction paradigm, called the governance-centric interac-
tion paradigm, shall be to overcome the gap between the DMS-centric and the
compute-centric interaction paradigms. In the end, both groups should be sup-
ported. To provide the required communication layer between the data lake and
the system, as it is identified in 4.3.3, HPCSerA, which is presented in
Chapter 5, is . Since HPCSerA also implements a REST- FaaS interface,
this concept can be to achieve the required ecosystem-agnostic interface, since
it can also be to connect to cloud resources. However, the integration of diverse
compute infrastructure is only part of two of the seven in 2.2 identified ob-
jectives. Thus, the proposed architecture is discussed more broadly i the following.
6.1 Choosing the Right Abstraction
In all of the presented data lake systems in Chapter 3, abstractions have been to
structure the overall system. These abstractions are employed on different levels and
by different stakeholders. For instance, the zone architecture consists of individually
linked subsystems [66]. Here, these sub-systems are the individual pieces that are
being abstracted in order to infer a hierarchy in an otherwise flat namespace. There
are several disadvantages entailed when abstracting systems.
1. Admins providing such a data lake service have to do the abstraction step for
the users by setting up and further managing dedicated systems. Therefore, 72Chapter 6. Data Lake Design
even later on during the production phase of such a data lake, the actual end-
users can not do everything within space but rely on active collaborations
with the system administrators.
2. Depending on the granularity in which the hierarchy needs to be employed,
multiple instances of the same zone need to be deployed. For example, if a
differentiation between the k-space data and the scanner reconstructions, see 2.2, is required on the zone level, two individual systems need to be
deployed. That is, because according to [66] each zone should be identified by
its interfaces to other zones, the modeling , groups, data charac-
teristics, including granularity, schema, syntax, and semantics, and the prop-
erties of a zone, e.g., protected, or governed. Thus integrating datasets
with characteristics entails the introduction of zones, and since zones are
abstractions of systems, the provisioning of systems is required.
3. Since each zone is, to a large extent, an encapsulated system that is integrated
via well-defined interfaces into the overarching data lake, it is difficult to ob-
tain a global view. This is also the reason why data governance is defined and
enforced on a zone level, not on a data lake level. This lack of global over-
sight is particularly disadvantageous for scientific cases, where a globally
enforced provenance record is required to adhere to good scientific practice.
That is because the zone architecture lacks the possibility to guarantee a glob-
ally consistent state since data governance can not be globally enforced. This
also extends without any limitations to a very fine-grained and distributed ac-
cess management, which can only be done on a per-zone level.
4. The definition of a single zone is a mixture of the data contained in it, the group it serves, the technical properties it has, and the interfaces it offers.
Although it defines each zone uniquely and in a structured and reproducible
way, this mixture can be confusing for users, who have rather a data-centric
view, i.e., they just want to get to some data.
On the other hand, the zone architecture has some undeniable advantages, which
explains its success:
1. It manages to break down a large and highly complicated system with very
diverse requirements, i.e., the overarching data lake, into small, and individu-
ally manageable pieces, i.e., the zones. Defining these individual zones can be
done in a standardized and structured way. When combining these individual
zones, developers only have to care about matching the interfaces of the zones
between which data and optionally an information exchange should happen.
2. Working with these independent zones means that changes in each zone can
be done completely decoupled from any other zone of the overarching system,
as long as the interface stays compatible. For instance, within each zone, fine-
granular data governance can be enforced, or a very specific -facing inter-
face can be provided without any consideration about possible consequences
for other zones.
3. By sorting data into different zones according to their characteristics, e.g., their
degree of processing, schema, syntax, or semantics, a hierarchy is inferred on
the data lake. This hierarchy ensures the manageability of the data lake and
the findability or explorability of data within the overarching system. 6.2. Novel DO- Data Lake Design73
To summarize, one needs an abstraction that allows one to modularize a data lake
into smaller, independent subsystems. Additionally, these abstractions should infer
a hierarchy on the data lake, so that users and admins can sort and distinguish on
a higher-level data and functionalities. However, this abstraction should ideally not
be done on a per-system level and should not mix data association and technical ca-
pabilities. In addition, it should enable a global view of the available data within the
entire data lake, and should not only offer a restricted view on a single subsystem.
6.2 Novel DO- Data Lake Design
In order to address these shortcomings of the prevalent zone architecture, a entity needs to be found, which can hide the complexity and technical details. Con-
sidering that a data lake is primarily all about data, these seem to be a good choice
for a central building block.
6.2.1 the Abstraction of Objects the abstraction of objects in computer science has a long and rich history
of representing information in a more human-comprehensible way, ranging from
the object-oriented programming paradigm to storage systems and digital objects
[94]. For this novel data lake architecture, the notion of Digital Objects (DO) is re- to provide an abstraction of the data and forms well-defined, encapsulated
FIGURE6.1: Sketch of a
Digital Object.
entities which each can be interacted with independently
of other constituents of the data lake. A DO in this sense
consists in its core of the actual data, which one wants to
store as a single entity. This raw data is described by se-
mantic metadata with key-value pairs. Since each object,
and therewith each atomic data item, is unique, each ob-
ject requires a Unique Resource Identifier (URI) in order
to resolve a specific entity. Each object must have a type.
A type is uniquely defined by a name, and a schema, i.e.,
a set of mandatory and optional keys, and a target space,
to which the corresponding values of the keys can resolve
to, for instance, strings within a certain enumeration, which has to be part of an
ontology or thesaurus, or integers within a certain range. These defined datatypes
should support the concept of inheritance to reduce the manual, and in the case
of updates, error-prone maintenance tasks, and further allow a more fine-grained,
hierarchical organization, when compared to only relying on pre-defined, and gen-
eralized datatypes. For instance, raw data that is being ingested into the data lake
should have a data type that inherits from a predefinedRawDatatype. This infers
the meaning, that the data comes from an outside source, ideally directly from a data
source. On the other hand, artifacts, i.e., data that is created by processing input data
from the data lake, should be modeled with a data type that inherits from aProcessed-
Datatype. This automatically infers that these data types are derived by other input
data from within the data lake, and therefore should have a matching retrospective
provenance linking to their input data. Inheriting from an existing data type should
always infer that the defined key-value pairs with the corresponding target space
are transferred. Derived data types can therefore only further restrict or concretize
an existing data type, but not generalize it again. 74Chapter 6. Data Lake Design
6.2.2 Architecture Overview
The aforementioned DO’s, which consist of data, metadata, and a URI, are rather a
conceptual idea, than existing, physical data structures. Instead, these DO’s have to
be generated, in order to provide the described abstraction to the users.
FIGURE6.2: Sketch of the data lake
frontend and backends.
Therefore, the core component of the proposed
data lake architecture is a central web applica-
tion that generates the described DO’s and their
functionalities on the fly upon request. For
this, the data lake frontend then communicates
with the required backend services, which are for storing the core data, and the associ-
ated metadata, and possibly enabling data pro-
cessing, compare 6.1. This architecture
can be seen in 6.2. Here it is emphasized,
that users do not communicate with the required
backend services themselves, but the pro-
vided frontend instead. This ensures, that all
operations on the data lake the same orches-
tration functions, which guarantees that the data
lake is always in a consistent state. This architec-
ture always allows admins to swap out existing
back-end services without requiring any adop-
tions by the users for existing workflows and
pipelines. This data lake frontend also provides a central and global view of the
entire data lake. This data lake frontend is also one of the in 4.3.6 identified
and required components.
Depending on the exact functionalities such a data lake should offer, there might
be multiple back-end services necessary. Such an introduces a combinato-
rial problem for the overarching, data lake-wide governance. That is because each of
these services will have their own mechanism to enforce certain policies, Control Lists (ACL’s). Ensuring a compliant state on the data lake would therefore
mean that one has to ensure a consistent, homogeneous, and compliant state on all
employed back-end services. This is, of course, a difficult, error-prone, and labor-
intensive task and can be drastically eased if the governance is only enforced on a
single system. This single system is the described frontend in this proposed archi-
tecture.
Even if some stakeholders decide to retain the full data sovereignty, for instance
for sensitive data, and want to manage control with case specific work-
flows, the of systems is still reduced compared to the zone architecture,
since no additional distinction with regard to data, , and interfaces needs to be
made. In this case, it is sufficient to simply provide the required information on how
to get to the data, or metadata within the DO itself.
6.2.3 Ingestion
Although it was at first heavily debated, it is now widely accepted that a data lake,
as the central data repository of an institution, should accept any data, i.e., any type
and format, from any source [117]. Therefore, the ingestion layer of a data lake has
to be highly flexible. This has in particular implications in two parts. First, the data
transmission should common protocols HTTP. Here, it can be even more 6.2. Novel DO- Data Lake Design75
favorable if a data lake is not only a passive data sink but can actively pull data
from other systems in order to stay up-to-date. This can also reduce maintenance
overhead since a lot of different and possibly heterogeneous data pipelines can be
managed from a single system and interface. Second, the storage and metadata
systems need to be able to cope, or at least easily adapt to, any kind of input data in
order to be able to automatically extract descriptive metadata to update the central
data catalog.
In order to be able to upload a file containing raw data, the first has
to configure a data type, containing the key-value pairs with their respective
mappings. Then, data can be uploaded to the data lake, or pulled by the data lake
from a data source, as such a previously defined data type. That means that every
uploaded dataset becomes a concrete and unique instance of the abstract data type.
The accompanying metadata to create the described DO can be delivered in different
ways. Either a sidecar file is provided, whether manually written or automatically
generated, or a metadata extraction needs to be registered on the data lake
prior to the ingestion of data. This is then executed on a copy of the previ-
ously uploaded data and has to return the extracted metadata mappings of the cor-
responding data to the data lake frontend. Here, the provided metadata is checked
in both cases against the provided data type definition, that all mandatory defined
keys have a value set, and that all provided values are within the pre-defined target
space. This ensures the data quality in the data lake.
6.2.4 Data Processing
After the raw data has been ingested into the data lake, the next step is to trans-
form and/or analyze these data sets. The resulting artifacts should then also be
re-ingested along with their provenance information. For processing the notion of
functions is . This intuitively enables the usage of FaaS- compute infras-
tructure which can provide homogeneous to different compute ecosystems clouds or systems, as explained in Chapter 5. Hereby, a is a, usu-
ally pre-configured, piece of software which takes some input data, and computes
some artifacts, i.e., some output data. Due to the compute intensity of most analyt-
ics tasks, outsourcing to a remote system is preferred over, e.g., local computation.
There are in general two ways, in which such a processing task on a data lake can be
performed.
The first one is inspired by JUNEAU, which performs the provenance auditing
by capturing the code prior to its execution, by being a proxy between the users’ in-
put and the executing kernel. To reuse this idea when offloading compute-intensive
tasks to another system, the remote compute services have to expose a suitable API
for the data lake frontend to , for instance, the REST API of HPCSerA. Then a can describe the desired tasks unambiguously in aJob Manifest, which states
the input data, the to execute, as well as any run time arguments that get
passed along. Depending on the concrete implementation of the , its ver-
sion is also tracked, e.g., in the form of a git commit hash. If a container is to
provide the necessary dependencies on the remote compute host, not only the recipe
file can be tracked via git, but also the container image can be uploaded into
the data lake. This would then also be a DO, in this case, derived from a base type
Container. Integrating the very image to derive a certain result allows for better
reproducibility, since the versions of the provided dependencies, e.g., libraries, can
change over time, if an explicit rebuild of the image is triggered later on. In order 76Chapter 6. Data Lake Design
to offload the computation of a job to an system HPCSerA, see Chapter 5, pro-
vides the necessary communication layer to execute a and enables the data
lake frontend to pass any required argument to the system. A similar to offload tasks to a cloud system can be with, e.g., OpenFaas [149]. Alter-
natively, by integrating different interfaces into the data lake frontend, also other
techniques can be directly connecting to a remote Docker Engine.
The second option is to the remote compute system, have the input data in
the data lake staged or accessed from there, and perform the computation outside of
the scope of the data lake’s frontend. This has the challenge that users are
now solely responsible to re-ingesting any artifacts along with their correct prove-
nance information and further metadata. If this is done as a manual process it can
be error-prone and therefore lead to an inconsistent, and unusable state of the data
lake. However, this is a completely fine practice, if proper data management is done
by the . To support this, and bridge the gap to the first option, the governance-
centric interaction paradigm is conceptualized in 4.4 which can be realized
with the proposed DMP tool.
6.2.5 Publishing Analysis Tasks
Particularly the first presented option for performing analysis tasks on the data lake
has the advantage that it can be easily shared among users. For this, developers can
provide their software suite as described in 6.2.4, along with a generalized
job manifest. The job manifest is generalized in that sense, that for instance, the input
data is variable, and some task-specific configurations might be requested from the
, or are automatically derived by the metadata attributes of the DO’s. Each task
should explicitly check if the provided input data, in the form of a DO, is suitable
for this specific job. Hereby one achieves an analysis service for data which
can then be by other data lake users by uploading suitable data into the data
lake and starting this pre-configured analysis task on it. For this process, the policies of the uploaded data can still be completely private. The analysis service is
accessed by its own unique resource identifier which can be resolved by the data lake
frontend. Although such published analysis services will be fairly easy for
other users, since most of the configuration has been done by the actual developers,
it should be able to provide, upon request, a minimum set of documentation of the
available options. Here, particularly the specification of suitable input data is of
utmost importance.
6.2.6 Linking Tasks to Workflows
In order to obtain an automated workflow these single tasks need to be chained to-
gether. Since the essential element of a data lake is the data itself, it seems reasonable
to focus on data-centric workflows which are often represented by a DAG. Having
all the necessary task definitions in place, i.e., the generalized job manifests as de-
scribed in 6.2.5, the independent tasks simply need to be linked by
their respective input and output data. This can be done in a DMP which describes
the overarching data lifecycle. These are already within the governance-centric
interaction paradigm discussed in 4.4.1. There, tasks should be either gen-
eralized job manifests or published analysis tasks to offer a higher degree of au-
tomation. The data lake functions again as an additional abstraction layer between
the workflow definition and the actual workflow implementation. As discussed in 6.3. Extending the Design FAIR Digital Objects77 4.4, the DMP can bridge the gap between the data lake frontend as a syn-
chronization layer, and remote systems an system. If external workflow
engines, CWL [3] or Apache Airflow, are required suitable adapters are neces-
sary for that specific tool. These mappings also support portability outside of the
context of the data lake to re-run and verify a workflow locally. However, the inter-
mediary workflow manifest concepts make the data lake highly adaptable as it en-
ables tailor-made solutions for individual projects while still enforcing well-defined
and homogeneous provenance information and artifact handling.
6.2.7 Constantly Evolving Metadata
The originally envisioned schema-on-read semantics of a data lake need to be in
practice accompanied by some metadata modeling if diverse sources and process-
ing tasks are integrated into a homogeneous system. This initial contradiction can
be resolved by allowing frequent changes on the employed schemata, and also on
the individual entities inside the data catalog [98, 121]. These frequent changes are
enabled in this proposed data lake architecture by allowing users and analysis tasks
to add, or update the metadata of an DO. This process will again trigger the schema
matching control to ensure the data quality within the lake. version control
for the schema and storing the schema version information of the template as
part of the technical metadata of each DO allows for continuously evolving meta-
data, even in the case of changing attributes or changing mappings. One ,
which can get frequently updated is the forward provenance, i.e., a link on DO’s
within the data lake pointing to a job manifest where these DO’s were as input
data to derive some artifacts. These job manifests then link to their respective output
data. On the other hand, the backward provenance is also maintained on the arti-
facts by linking to the same job manifest to derive them. This process builds
up a provenance-centered graph linking the DO’s within the lake, where the nodes
are DO’s representing either input or output data, and the connecting edges are the
job manifests, i.e., aprocesswithin the notion of the Open Provenance Model (OPM)
[133]. The exact format, in which the provenance information is stored, e.g., OPM,
PROV-O, or others, can be adjusted on the data lake frontend. However, during the
project only the PROV standard is implemented.
In addition to the metadata attributes of an individual DO, also relationships
between data types, apart from their lineage, can be continuously modeled. These
relationships will be on the level of the data types, or schemata, not on the level of
individual DO’s.
6.3 Extending the Design FAIR Digital Objects
Although this DO- data lake architecture has a lot of advantages compared to
other existing systems, one until now unaddressed concern is the interoperability
and the implementation effort particularly for the data lake frontend to create these
abstract DO’s. Therefore it is advantageous to re- existing infrastructure for this
concept. In this scenario, where a research data management infrastructure should
be build, the concept ofFAIR Digital Objects, which have been recently promoted,
seem promising [51, 173, 175]. In the following it is investigated, how well these two
concepts are aligned. 78Chapter 6. Data Lake Design
6.3.1 What Are FAIR Digital Objects?
Put simply, FAIR DO’s (FDO) are DO’s which are specifically designed to fulfill the
15 high-level FAIR principles [209]. One key component of FDOs is the globally
uniquely resolvable and Persistent Identifier (PID) which is the reference to
resolve a single FDO. Therefore, one needs a PID service, a handle system, which
resolves a PID to a PID record. This concept is utilized by FDOs by requiring a set
of necessary information in these PID records so that it qualifies as an FDO record.
An FDO record is a data structure of a fixed type, which is machine-readable, in-
terpretable, and particularly actionable. This is achieved by providing not only the
data as a bit sequence but also providing typed metadata attributes. Mandatory,
optional, and recommended attributes, i.e., typed key-value pairs, are defined in a
FDO profile. Each type has to be registered.
The requirement that each FDO has to have a registered type is derived from the
FIGURE6.3: Sketch of an
FAIR Digital Object.
goal of machine actionability.By relying on defined
types, machines can automatically look up possible op-
erations on these data types, or resolve the relations be-
tween types. Representing an FDO in a sketch simi-
lar to 6.1 yields an almost identical representa-
tion, as can be seen in 6.3. The largest differ-
ence is that FDOs have operations defined by their regis-
tered types. In the previously presented data lake, these
functions, or operations are an of the data lake.
Defining functions on the level of an FDO has the disad-
vantage that publishing a as described in 6.2.5 requires to the data type definition,
which likely not every will have. However, the advantage is that each object
has information about how it can be processed.
Another difference is, that instead of a local URI, a FDO has a PID. Therefore,
the immediate frontend when working with FDOs is the PID resolver which returns
the FDO record, instead of the aforementioned data lake frontend. Only allowing to
resolve PIDs is however not enough, since a semantic search over indexed data must
be possible in a data lake. Generally, the definition of FDOs allows that the metadata
entry in an FDO record links to an external service. This external service then has to
a data catalog in the case of a data lake, which can also be accessed independently to
query and explore the available data set without a concrete PID. Similarly, the data,
i.e., the actual bit sequence can also be located on an external service, and only a
reference has to be put into the FDO record. The communication between clients and
an FDO- data lake would follow the Digital Object Interface Protocol (DOIP)
defined by DONA.
To summarize, one can build a data lake, as previously proposed with FDO by
extending the PID resolver with the discussed additional data lake functionalities.
This is generally possible and in no contradiction to the fundamental concept and
was even foreseen, with the idea of "value-added reference services" [94].
6.3.2 Canonical Workflow Framework for Research
One particular challenge when moving from the more generic DOs to FDOs is the
processing. In the DO- architectures, functions were defined on the data lake
frontend, whereas in the FDO- architecture, at least some functions have to be
registered in the FDO profile and are part of each object. However, in data-intensive 6.4. Example Deployment for the MRI Case79
research projects often not only a single is being executed, but entire work-
flows consisting of many dependent and independent calls. Therefore it is
important, that only some abstract functions those required to the actual
data should be modeled at this level. All other, more advanced processing tasks
should be, as described in 6.2.4, modeled as their own FDO. Here, the data
at the core can be, for instance, the container image, and the functions defined on
that FDO can describe how to execute it on some specified input data. This enables
the publishing process of tasks, as described in 6.2.5, possible in a machine-
actionable way.
To make these workflows and in particular the data they create more FAIR, the
Canonical Workflow Framework for Research (CWFR) is proposed by the FDO-
CWFR working group in 2020 led by Peter Wittenburg
1
. CWFR is supposed to
be an additional layer on top of existing workflow technologies and provides the
highest level patterns of recurring canonical steps. To reuse these canonical steps
a library of canonical steps is provided, which can link each canonical step to the
library of more specifically tuned packages. The described concept of CWFR further
suggests storing all information in a single FDO and making each individual state
of the workflow resolvable by its own PID. In particular, each of these PIDs resolves
again to an individual FDO.
The previously proposed FDO- data lake architecture is a concrete concept
to implement the abstract idea of CWFR. The described recurring patterns are the workflows. The canonical steps are represented by the generalized job man-
ifests, which can be fine-tuned with respect to some input parameters to create a
package repository of canonical steps. The overarching composite FDO represent-
ing the entire workflow and making each state individually resolvable via PIDs is
achieved via the provenance-centered graph connecting input and output data.
6.4 Example Deployment for the MRI Case
To further emphasize how the previously described data lake architecture can be
, an example implementation on the previously described MRI case,
see Chapter 2, is discussed in the following.
6.4.1 Data Modeling
The first step one has to do is to model the raw data that one wants to ingest into the
{
...
"SystemVendor" : "()",
"SystemFieldStrength" : "Float (1.5 ,3 ,7)",
"NumberReceiverChannels" : "Integer ()",
"Lamor_frequency": "Float()",
"BodyPart" : "()",
"PatientName": "Keyword ()",
...
}
FIGURE6.4:Shortexcerptfromthe
mri_kspace_data.jsonused to define the k-space data type.
data lake. In this particular case,
the raw k-space MRI data shall be
ingested. To do so, first the cor-
responding data type has to be
registered. A data type can
be defined within a JSON doc-
ument, stating the typename, in
this case,mri_kspace_data, and
assigning an arbitrary amount of
key-value pairs to it.In this
example, 59 different key-value
pairs with domain-specific, se-
mantic relevance are defined. The
1
https://osf.io/9e3vc 80Chapter 6. Data Lake Design
defined keys range from the system vendor, to the Lamor frequency, the bodypart
and patient specific information the name, weight, age, or sex, see 6.4. In
this particular case, only a relaxed quality control for the values is enforced on
the type. In this case, an empty constructor maps onto the entire valid value space.
As an example, theSystemFieldStrengthhas additional arguments passed in its
constructor, which will restrict this key to only the specified values. the REST
interface of the data lake frontend, one can upload this data type definition.
Since in this overarching (F)DO- concept each data type has to be unique, one
can decide, whether the definition should be possible from space, or of this re-
quires a more privileged . However, there are no technical limitations inferred
by this architecture.
6.4.2 Data Ingestion and Metadata Extraction
Once this newmri_kspace_datadata type is registered on the frontend, one can start
to upload to it. Upon upload, the data lake frontend will try to the data model
and the uploaded data to create a concrete instance of the abstract data type. For this,
it needs to populate the defined metadata keys with the corresponding information
about the ingested data. This can be done by providing a sidecar file containing the
metadata. Within this project, this step is also outsourced to the data lake. For the
case of k-space data, which format is exported from the scanners in vendor-specific
formats, the process of extracting all related metadata can be fully automated. In the
case of Siemens scanners, the k-space data is stored in a.datfile. This file format has
an extensive, utf-8 encoded header, which can be read out. For this particular project,
a Python script that was developed by Martin Schilling is for this. This script
was packaged within a container image, which is uploaded to the data lake. Each
container has a unique URI, with which it can be resolved. Further, each container
image has a version, which enables updates of the metadata extraction tools while
ensuring full resolution of the provenance.
To make this image executable, the execution of the container has to be config-
ured on the data lake frontend. Here, the exact execution command, and the mount
paths, where the input and output data reside, have to be provided. Once this is
done, data can be uploaded to the data lake frontend and the container can be auto-
matically executed on it. Once the metadata is extracted and the data of the DO is
saved, the metadata is indexed in a document- database to represent this sin-
gle entity to have it searchable in a data catalog. The original k-space data is saved
on a highly durable storage tier.
All of the previously mentioned steps can be done from a client without any
complex configuration or dependency handling by only the REST interface
of the data lake frontend. However, it is very important to strictly enforce the sep-
aration of concerns. The data lake frontend should only orchestrate, abstract, and
govern, not itself be involved in executing tasks other than that. Therefore, the fron-
tend needs to have a backend service where this computation can be reliably done.
In a naive first for this particular case, it is done by connecting to a re-
mote Docker Engine. This can leave some entanglement between these two systems
due to their blocking behavior, which further can lead to connection timeouts when
the filesize, and hereby the data that has to be copied, or sent via UNIX sockets, in-
creases. Therefore, a stricter separation with a well-defined interface is much more
desirable, which allows for more fine-grained control. 6.4. Example Deployment for the MRI Case81
6.4.3 Querying
Having ingested the raw MRI data, it is possible to select a single scan or a subset
of the available scans by querying the data catalog, or by providing the URIs of the
data of interest. Queries are defined by stating the of the DO(s) of interest,
i.e., the key of the metadata, and its value. For example with respect to 6.4,
one can define the simple queryattribute:SystemFieldStrength, value:3. This
query will select all the available DOs that have a keySystemFieldStrengthset to
the value of3. Important to note is, that this query is per default executed on all
DO types, thus not only DOs of typemri_kspace_datacan be selected, but also oth-
ers, which also poses this . By defining the type of the DO in question, the
results can be further restricted. Multiple key-value queries can be defined and con-
catenated by anandoperation. Concatenations onoroperations can mapped
by sending several independent queries and merging the resulting data sets.
6.4.4 Processing
Once a suitable query is constructed to select the precise data set one is looking for,
this data set can be processed. For this, the notion of a job manifest is , which
is a JSON document, which unambiguously describes the job. To do so, different
options have to be provided by the :
• A list of input data, or a query for the data catalog which resolves to the desired
input data.
• Ideally, a container image is specified that contains all the dependencies a job
needs to run. If this container image is ingested to the data lake it is also rep-
resented as a DO within the data lake. The URI of the container DO can be to link resulting artifacts to it, significantly enhancing the quality of the
provenance data.
• Similarly, code that should be built dynamically for each job execution should
be version controlled, e.g., by usinggit.
• Environment variables which shall be exported into the job context.
• A single or multiple compute commands that should be executed.
• Specific resource requirements, the expected compute time, the of
CPUs and Graphics Processing Units (GPUs), or conditions to the
internet.
• The output data directory can be specified as well as the datatype of the DO as
which the artifacts should be ingested into the data lake.
• Further comments can be added to aid comprehensibility. the REST API of the data lake frontend, such a job manifest can be submit-
ted to be executed. For this, the entire job is split into three distinct phases: pre-
processing, processing, and post-processing. Since these processing tasks are gener-
ally assumed to be compute-intensive and/or data-intensive, the computations are
done on an system.
Within the pre-processing phase, the input data is staged. For this, a list of URIs is
created on the -provided query. A data staging script then loops over this
list and pulls all stated input files. If no other storage target is defined, the input files 82Chapter 6. Data Lake Design
are stored on a Scratch, or Work filesystem. These filesystems are typically accessible
from the high-speed interconnect, which offers a high bandwidth and low latency,
but do not offer any backup, and are only accessible for a limited amount of time.
Since all input data is already stored on a durable storage system, these limitations
are neglectable.
For one particular reconstruction pipeline, a fine-grained data staging mecha-
nism is chosen. Here, the kspace data are stored in a specific folder structure given
by/kspace/hum_$/$DATE/. Thehum_$NUMBERis a pseudonym representing
the patient name. This folder structure allows to store all scans taken at different
dates of one patient within a single path in a structured way. This exemplifies that
input data does not need to be staged within a flat folder structure but can be cus-
tomized.
Once the input data is staged, including the container image, the required soft-
ware can be dynamically built, if specified in the job manifest. In the different pro-
cessing tasks involving the reconstruction of k-space MR images, this is typically
done by two distinct git repositories. The first one contained BART, which is
for each case typically built a distinct branch. The second repository is then
usually a case-specific one, containing some driver scripts. These repositories are
cloned and bind-mounted into the container where these repositories can be built.
The container image then provides the dependencies likelibfftw3, orlibopenblas,
which require only infrequent updates. On the other hand, the code that is actively
being developed can be updated with every job execution. This offers large time
reductions when compared to rebuilding an entire container image for each job. Ad-
ditionally, saving these images would require large amounts of storage, and would
therefore be unfeasible. The git commit hashes are updated within the original job
manifest.
Once the pre-processing is finished, the job can be submitted by evaluating the
compute commands the users stated in the job manifest. For this, the -specified
environment variables are passed to the context of the job, theTOOLBOX_PATH,
which provides the location of thebartexecutable.
One example of such a job is developed by Moritz Blumenthal, where he re-
constructs the staged k-space images within a fixed folder structure. It uses first
thebart ecalibcommand to auto-calibrate for the different coil sensitivities theESPIRiTmethod [199]. Then, thebart picscommand, a parallel image
(SENSE) reconstruction an L1 wavelet regularization with a speedup factor
of 2 is performed [25, 114, 161]. Afterward, the images are formatted to the Neu-
roimaging Informatics Technology Initiative (NIfTI) file format to be available for
the next processing steps. These are stored within a similar folder tree, prefixed
byreco/$VERSION/instead ofkspace. Each image reconstruction is hereby a sin-
gle batch job, and the corresponding batch script is stored for each image. Further
metadata information can also be provided within the filesystem by a sidecar file, as
it is done for the provenance information that is stored as PROV document in JSON
format. All information is also indexed within the data lake, allowing for efficient
searches to retrieve specific file paths.
Once the job is finished, the post-processing is started. Here, the resulting ar-
tifacts are ingested into the data lake. For that, the directories, which were bind-
mounted into the container during the job execution, are checked for files. This
default behavior can be overwritten by explicitly stating the directories or files that
should be ingested into the data lake within the job manifest. Upon ingestion, these
artifacts are modeled as DOs and are linked to the job manifest to store their lineage. 6.4. Example Deployment for the MRI Case83
FIGURE6.5: Image of the DOT code serialization of the PROV docu-
ment of one of BART k-space reconstruction jobs.
In addition, their provenance information is automatically attached as aPROVdoc-
ument. One example of this is shown in 6.5, where a DOT serialization of one
of the PROV documents is plotted as a graph. Here, one can see, what the paths are
for the input and output data, what the Slurm job ID is, that a Singularity container is
, and that three git repositories are bind-mounted into the container along with
their corresponding git commit hashes. These PROV docs are created automatically
for every task and are stored as a sidecar file within every output directory
and are indexed for each processed data object type under theprovenanceattribute.
Therefore, searches with respect to certain job specifications, what data has been
processed with this particular git hash, are possible.
In this example, there are three calls to HPCSerA required: the first one to trigger
the execution of the preprocessing, the second for triggering the actual data process-
ing, and the third one to trigger the postprocessing. That is because each of these
phases is a single . The pre- and postprocessing functions are quite generic
and can be reused across different cases. The processing step is the actual step
where and job-specific code is executed. To execute this automatically with
HPCSerA, this code, and its environment, e.g., a specific container image, has to be
provided beforehand, for instance by uploading it as a DO into the data lake.
6.4.5 Interacting with the Data Lake from the system
The k-space data and the reconstructed images are not only represented by DOs
within the data lake and would be therefore solely accessible via the data lake fron-
tend but are also stored on an system within a specific folder structure. This
is done since some of the researchers are to work with and organize their data
in a specific way. These researchers and their processes require that their datasets
are organized according to the Brain Imaging Data Structure (BIDS) [71]. An ex-
ample of such a folder structure is shown in 6.6, a shortened version pro-
vided by the authors of the format
2
. The proposed DMP tool, see 4.4, can
in this case be to ensure that the data set is indeed always stored according
2
https://bids.neuroimaging.io/, Accessed 17.12.2023 84Chapter 6. Data Lake Design
dataset/
participants.tsv
sub -01/
anat/
sub -01-T1w.nii.gz
func/
sub -01_task -rest_bold.nii.gz
sub -01_task -rest_bold.json
sub -02/
sub -03/
FIGURE6.6: Example folder structure according to
the BIDS standard.
to the BIDS standard, the
overall folder structure, the in-
dividual filenames, that all sub-
jects are also listed within the
top-levelparticipants.tsvfile,
or that for all derivatives the
provenance information is pro-
vided by theGeneratedBymeta-
data field within the correspond-
ing sidecar file.An exam-
ple of such a sidecar file is
shown in 6.6 where the
sub-01_task-rest_bold.jsoncontains the metadata of thesub-01_task-rest
_bold.nii.gzscan. The provenance recording is not only retrospectively controlled
by the DMP tool, but is also supported beforehand since explicit linking of every job
invocation with a task within an experimental description is required. This not only
allows researchers to work with their dataset as usual, for instance the BIDS
Manager Pipeline [93], but also offers additional quality control.
One of the advantages is that the DMP tool, as it is introduced in 4.4,
would also be capable of monitoring the folder structure for subjects, and deriva-
tives and automatically triggering an ingestion into the data lake pre-configu-
red DOs. This enables the intuitive integration of these neuroimaging datasets and
workflows with completely different data to allow other researchers to search for
correlations between neuroimaging data and, e.g., information about the subjects’
physical activity or diet. These novel tasks can then be performed on integrated
data that is not bound to the BIDS format. Therefore, the overall availability of these
datasets is increased for researchers working outside the scope of the BIDS commu-
nity. When ingesting these datasets into the data lake, all entities get indexed within
the overarching data catalog. This also allows for efficient search for specific data,
e.g., those created by a certain version or hyper-parameter combination. In the ex-
ample mentioned in 6.4.4 this is just indicated by a at the top level of
each dataset, which then requires a dedicated sidecar file for each directory stating
the exact provenance information for this workflow.
6.5 Evaluation of the Proposed Architecture on the usage of this proposed data lake within the "Big Data MRI"-project
an evaluation with regard to the in 2.2 defined objectives is made in the
following:
Data Modeling:By modeling data as an abstract, typed DO on pre-
configured schemata, one obtains an entity- metadata model, which is generic
and allows for fine-grained quality control. These DO can hierarchically organize an
otherwise flat hierarchical data lake on their types. In addition, these DOs of-
fer a simple yet powerful mental model for humans to interact with the constituents
of the data lake to easily comprehend their and content.
Generality:In this DO- architecture, the data itself is to organize and
infer a hierarchy on its intrinsic value. This allows for the integration of much
more diverse data in a single system since no assumptions about the data and the 6.5. Evaluation of the Proposed Architecture85
envisioned tasks have to be made a priori. This is also reflected, in that every datum
can be modeled as an object, including processing tasks. This is fostered by the
capability to continuously evolve the DOs and their definitions homogeneously over
time, and not be limited by presumptions made at the beginning which later turn out
to be incorrect or incomplete.
Modularity:The DO- data lake design is highly modular, as it offers a ho-
mogenous interface for a potentially, but not necessarily, heterogeneous backend.
This allows the complexity of multiple storage systems, databases, and compute
systems to be abstracted away for the users. This, in turn, enables admins and data
engineers to include different systems in an efficient way, since only the interface
between a single component and the data lake frontend needs to be handled, but
not every component has to work with every other component. This is further em-
phasized when extending to FDOs, since the FDO records only require a machine-
actionable instruction on how to retrieve the data and metadata, but do not require
the actual information to be centrally accessible.
Performance:Due to the integration of diverse compute infrastructure, including systems, computing tasks can be processed very efficiently. Enabling different
backends not only for compute but also for storage and metadata indexing enables
users to optimize for specific scientific workflows.
Scalability:Due to its modular design, the DO- data lake is highly scalable,
since the underlying systems, providing the required functionalities storage and
compute, are not bound to a single specific implementation. Instead, they can be
chosen to offer the aspired scalability. In this sense, dispatching tasks to an system or cloud infrastructure is highly scalable by design, since these systems are
built for this specific purpose. Similarly, highly scalable storage systems can be ,
to retain all data.
FAIR Data:Extending the DO- architecture to FDOs will provide FAIR-
ness by design. That is because the employed DOIP is specifically designed for the
purpose of fulfilling the 15 FAIR principles.
Continuous Integration into Scientific Workflows:Due to the generally applica-
ble metadata modeling, as well as the modularized design which allows to exchange
or add specific back-end components, the proposed data lake can continuously inte-
grate scientific workflows.
Data Lifecycle Management:The overarching data lifecycle management is done
within the concept of the governance-centric interaction paradigm discussed in
Chapter 4. This allows users to define an envisioned state and continuously ensure
its consistency with the defined DMP, while still enabling the of different inter-
faces.
Security:Generally, the DMP is capable of ensuring that permissions are set cor-
rectly, i.e., ensuring that a does not accidentally provide to private data.
However, this requires that the permissions are always correctly enforced. In the 86Chapter 6. Data Lake Design
case of a privilege escalation, there is currently no mechanism to prevent unautho-
rized to sensitive data or metadata. Therefore, additional measures are re-
quired to isolate the underlying services from these attack scenarios.
6.6 Summary
In conclusion, a generic and modular data lake design is presented. It eases the ad-
ministration by providing a more suitable abstraction in the form of DOs compared
to the prevalent zones. The organization of this potentially flat namespace is done
via unique data types, the definition of which can then be for quality assurance
upon data ingest. The proposed data lake provides scalable and ecosystem-agnostic
compute resources on a private cloud and an system by hiding their pe-
culiarities behind homogeneous FaaS- interfaces. When executing these func-
tions on the data lake, transparent provenance auditing is done via the provided
middle layer. The corresponding input data for these functions can be selected us-
ing semantically meaningful queries and are automatically staged if necessary. The
proposed data lake is successfully utilized within the MRI- case, by au-
tomatically extracting all available metadata from thedatfiles and indexing it in
a data catalog. Once the raw data is available, one can select the data a list
of identifiers, or a query to reconstruct the kspace images usingBARTon the system.
The concluding evaluation shows that except for the full data sovereignty, i.e.,
security, all the identified requirements are fulfilled. Due to the modular design,
data privacy can be consecutively included, by securing the individual components
of the data lake. These are, on a high level, storage, compute, and data cataloging.
Since data storage is an implicit part of its processing, it is sufficient to focus on the
processing and metadata handling to secure the data lake. Since metadata handling,
i.e., indexing, updating, and searching through a data catalog, can be considered to
be a specialized form of processing, a generic platform for processing sensitive data
is developed first in Chapter 7. Afterward, the obtained knowledge about the secure
processing of sensitive data is applied in Chapter 8 to develop a secure data catalog
service. 87
Chapter 7
SecureHPC: A Workflow Providing
a Secure Partition
Within this chapter, a secure partition on a shared system is developed
which enables the processing of sensitive data. For this, first, the relevant re-
lated work is explored in 7.1, followed by a general introduction to the
architecture of systems in 7.2. Then the general design of the
workflow is discussed in 7.3, with which to the special secure par-
tition can be obtained. Afterward, implementation details of this workflow and
the partition are provided in 7.4, followed by a dedicated presentation
of multi-node support in 7.5. on this design a security analysis
is done in 7.6. Then extensive benchmarking is done to determine the
additional cost associated with the extra security in 7.7. Lastly, the ap-
plicability and -world performance are demonstrated in three different cases in 7.8. Parts of this chapter are published in [145, 146] and are
submitted to [141].
Until this point, the in Chapter 6 proposed data lake is only capable of managing,
i.e., storing, indexing, and processing, insensitive data. Due to the specification of
the DOs, it is possible to store sensitive data by only uploading it encrypted [94].
Leveraging the modular design of the proposed data lake, it is possible to consider
the processing of sensitive data as an isolated service, which can also be out-
side the scope of the data lake. Once available, it can be as a drop-in replace-
ment for the current "normal" service. For this, it needs to support the previ-
ously utilized FaaS method so that it can be exposed to the proposed data lake the in Chapter 5 developed interface, called HPCSerA. Processing highly regulated
data, see GDPR or HIPAA, requires a special setup since systems are typically
built for the highest performance, not the highest security. That is because different
users share the available resources and can run their compute jobs simultaneously
on shared or an exclusive subset of the available nodes. Due to the optimization
for performance, it is very common, that users interact directly with the operating
system of the host. Therefore, users are trusted to some extent and, thus, any lo-
cal vulnerability can be immediately exploited by users or bots that gain control of credentials. Taking into account that there are continuously attacks dis-
covered that lack a reliable solution over a sustained period of time [92], sensitive
data should only be transferred, stored, and processed with care in public data cen-
ters. Some industry and government data centers, such as for weapon research, limit and employ strict policies regarding system , even to the point where
sensitive data is physically disconnected if not needed. However, restricting system does not resolve the problem with the data , since administrators basi-
cally have full . Therefore a novel service is required where even in the case 88Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
of a privilege escalation leading to a compromised cluster, the data integrity should
be guaranteed. This general need for secure compute capabilities and the result-
ing requirements for refinement of existing security concepts, particularly for systems, is acknowledged in the literature. Christopher . describe in [44] that
“At UC Berkeley, this has become a pressing issue“ and it has “affected the campus’
ability to recruit a faculty member“.
7.1 Related Work
In BioMedIT [46], a distributed network is described, where virtualization is to create completely isolated compute environments that are exclusively reserved
for a particular project, in a private cloud. However, the of virtualization for
isolation purposes is only effective, if it is ensured that other users can not the host system directly. Therefore, this requires dedicated hardware and
software, which drastically increases the cost of hosting such a system.
A similar is described in [170], wherePrivate Cloud on a Compute Cluster
(PCOCC)
1
is to deploy a private virtual cluster. The outlined Slurm integration
allows for direct integration into an existing system. However, a dedicated
Lustre file system is needed and it remains unclear how this virtualized cluster is
secured against a compromised host.
Systems likeSELinux[185] orAppArmor[10] are theLinux Security Mod-
ulesto enforce mandatory control with the general goal to prevent zero-day-
exploits. However, once an attacker finds a vulnerability these mechanisms become
useless. Here, intrusion detection systems, likeauditd, could be to detect such a
system [95]. In contrast, the goal of the envisioned secure partition is to ensure
data privacy under the assumption that a root exploit is already successfully done.
Therefore, these two approaches work completely independently of each other.
In order to limit the actions a malicious root can take on a compromised system,
one can kerberized filesystems [129]. This would then, however, limit the func-
tionality of non-interactive batch jobs, and/or would require the additional usage of
e.g.Yubikeysby all its users.
One possible way to isolate a single task on a multi-tenant node is the of
Trusted Execution Environments(TEEs). Here, to sensitive data or code, which
is loaded into memory, is secured from from the host kernel. There exist sev-
eral different solutions, including commercial solutions Intel’s SGX [122] and
open-source solutions likeKeystone[107] which are on basic primitives pro-
vided by the respective hardware. In order to utilize those so-calledenclaves, changes
to the source code of the corresponding application are necessary. To mitigate this
issue, solutions likeGraphene[197] enable users to run unchanged code within an
enclave. Similarly,SCONE[7] is developed to support Linux secure containers for
Docker. These solutions for TEEs are very interesting to secure and isolate a run-
ning process, including its data, from malicious but are in itself not sufficient
to provide an end-to-end workflow to securely upload, store, and process sensitive
data on an untrusted, shared system. In addition, there is currently no sufficient
solution to provide to an accelerator, a GPU.
Containers are processes that are executed on the host operating system and are
pseudo-isolated bynamespacesandcgroups. This allows the provisioning of a private
root file system in order to execute software in a portable environment. As with any
1
https://github.com/cea-/pcocc 7.2. General Usage of Systems89
other process, containers are executed with the rights of the , which can be ex-
tended with asetuid-bit. Containers solely isolate the processes running within their
context from direct to the host, not vice versa. Therefore, even an encrypted
container is without the usage of a TEE decrypted in kernel space and can be fully
read by the host.
Secure storing of sensitive data on shared, untrusted storage on an system
is explored in [186]. Here,Ceph Object Gateways[207] are deployed on single-tenant
compute nodes alongside anS3FSwhich bind-mounts the corresponding S3-Bucket
as an POSIX compatible directory onto the host. The host- configuration then
performs automatic encryption/decryption of data that is written/read to/from this
specific directory. While this is an important part of secure data processing, it is only
a part of a holistic solution that is required to provide a secure end-to-end workflow.
For instance, features such as secure transfer of the necessary keys for accessing the
S3-Bucket and for performing the decryption/encryption are important. Addition-
ally, some data center policies may require a strict separation between the and
the storage networks.
Other ways to securely store sensitive data on a typical filesystem includeLinux
Unified Key Setup
2
(LUKS),eCryptFS
3
, andGoCryptFS
4
. LUKS, which is on
dm_crypt, provides encryption for a block device and allows for multiple passphrases
to be . On the other hand, eCryptFS and GoCryptFS offer filesystem-level en-
cryption which is on a File System in Space (FUSE) filesystem. To allow
for efficient file while still a block cipher, they are splitting each file up
into independent blocks, orextendsas it is called in the case of eCryptFS. The advan-
tage is, that random to a file is faster since only each involved block needs to
be de- or encrypted, instead of the entire file.
In contrast to the related work, a blueprint for a holistic end-to-end processing
pipeline suited for sensitive data to be on a shared, untrusted system is
required. This is complemented by a detailed discussion about the security implica-
tions as well as extensive benchmarking to assess the general applicability.
7.2 General Usage of Systems
This describes the architecture of systems, as well as the general usage
of systems. on this introduction, a security risk analysis is performed to
determine different attack vectors that can be to compromise the data privacy
of sensitive data during transit, at rest, or during compute.
7.2.1 Architecture of Systems
Generally, systems are composed of different node types. They serve different
purposes and have, therefore, different security policies applied to them. In the
following, an overview of typical node types is provided and their interactions are
explained. This will further serve as the basis for the nodes that are deemed secure,
even in the case of a privilege escalation of a .
The general architecture of an system is illustrated in 7.1. sys-
tems are commonly guarded by a perimeter firewall, requiring users to connect via
a Virtual Private Network (VPN) or a jump host. Afterward, they can log in via SSH
2
https://gitlab.com/cryptsetup/cryptsetup/
3
https://www.ecryptfs.org/
4
https://github.com/rfjakob/gocryptfs 90Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
FIGURE7.1: A simplified sketch of an system.
on a frontend node. Frontend nodes are shared by all users and are to build
software, move data, or submit compute jobs to the batch system. to compute
resources is granted by a resource manager, Slurm, which schedules jobs in
such a way, that the general utilization of the system is maximized. The batch sys-
tem dispatches jobs to the compute nodes. Although an interactive compute job is
generally possible, the majority of the available compute time is consumed by non-
interactive jobs, i.e. they run completely without any interaction. The frontend
as well as the compute nodes share at least one parallel file system, likeLustre[29]
orBeeGFS[85].
The management nodes are comprised of several different nodes that are solely
reserved for admins. Hence, they share the basic requirement, that they need to be
protected from any . Typically, there is a specific admin node, which is just for login. Very important is the so-called image server, i.e., the node which
is to provision the golden images to all the nodes, including the frontend and
the compute nodes. If an attacker would gain to this server, the images could
be compromised and distributed to the nodes. In order to increase the security of
this node, it is placed in the Level 2 security zone, where is highly limited
and requires further activation and authorization. In order to decrease the attack
surface from the image server in the Level 2 security zone, compared to the admin
nodes which are being operated in the Level 1 zones, the image server does not -
low from any other system located in a lower security level, i.e. the Level 1
admin nodes and the nodes. In addition, there are no -accessible daemons
listening on any ports. Write to the images is strictly limited to this image
server. The read for the compute nodes via the Preboot Execution Environ-
ment (PXE), necessary for the actual deployment of the images to the compute and
frontend nodes, is only done by an admin node located in the Level 1 layer. In this
way, a possible exploit in the cluster manager, i.e., the system responsible for the
distribution of the golden images, cannot lead to a compromised image.
Another important node is the one, that the resource manager is running on. This
server is responsible for enforcing the correct assignment and of the jobs and
users to the compute nodes. The last pieces of infrastructure are the networks that
connect the different nodes.
The provisioning of software is usually done via an environment module system,
likeLmod[123] orSpack[63], and is cluster-wide available, which allows replicating
the desired working environment by loading the appropriate modules on any node
of the cluster. 7.2. General Usage of Systems91
7.2.2 Typical Workflow
FIGURE7.2: A schematic sketch of the typical workflow for users to
submit a job on an system.
The typical workflow to execute a job on an system is depicted in 7.2.
A logs in and can write or submit a batch script. This is typically similar to a
shell script, where the desired resources and the commands to be executed are spec-
ified. The resource manager checks, whether or not the specified resources are eli-
gible for theuidthe request is coming from, i.e., if the is authorized to the
specified resources. If the request is permissible, the resource manager schedules the
job in an appropriate time slot for execution with the overarching goal of maximiz-
ing overall system utilization, although other strategies can also be specified. The
input and output data on a parallel file system can be accessed from all nodes. The
necessary communication between the storage nodes and the compute nodes or, in
the case of multi-node jobs that are communicating via MPI, ideally takes place via
a high-performance interconnect likeOmni-Path[23] orInfiniband[159].
7.2.3 Possible Attack Scenarios on the system architecture and workflow, potential security risks
can be identified that are discussed in the following. It is assumed that a privilege
escalation, i.e., a gainingrootprivileges, can happen on any system accessible
to users, in particular the frontend and the compute nodes. Because the nodes and
storage systems users have direct to are solely protected by the permission
system of the Linux kernel, the trusted code base is very large and has a large attack
surface which presumably yields unknown vulnerabilities that have been exploited
in the past [38]. For the following security analysis, it is therefore assumed, that an
attacker can gainrootaccess or can impersonate an arbitrary on one of these
nodes.
Data Stored on a Shared File System
Starting from the node the attacker gainedrootprivileges,rootcan get to any
file that is stored on one of this node or a shared file system mounted on this node.
This direct can be made a little bit more uncomfortable, for instance, by an
NFSroot-squashprohibiting direct root . Such issues can be circumvented by
an attacker by changing theuid. Therefore, all data should be considered as compro-
mised. 92Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
Data Stored on a Compute Node
Additionally, after the job has started, the is also able to log in on these nodes,
for instance via SSH. The is hereby granted by the resource manager solely on theuid. Thus, arootuser has immediate to all nodes allocated to any and therefore has to the local data and processes. Furthermore, aroot can submit jobs to the batch system with an arbitraryuid, thus gaining to compute nodes reserved by the resource manager for a specific group.
Manipulation of the Provided Software
A maliciousrootuser can also tamper with the provided software stack or with the
individual system image of the node the attacker is currently on. Such a compro-
mised system can then continuously leak data.
Manipulation of the high-speed interconnect
When the processing power for an application needs to be scaled up, the applica-
tion’s data is typically divided between multiple processor cores and nodes and is
executed as parallel tasks, often calledranks. The most common way for these indi-
vidual processes to communicate intra- and internode is the usage of MPI libraries
which transfer information and data between the ranks. For small numbers of ranks,
this can be on a single node, for larger numbers, multiple nodes are required. In
these cases the capabilities of the interconnect become important if a lot of informa-
tion needs to be exchanged between the different nodes. To get the best scalability,
that is the efficiency with which additional cores deliver additional performance,
high-speed interconnects OPA or Infiniband are in systems to provide
better latency, message rates, and bandwidth than a conventional Ethernet would.
The MPI data is injected into the high-speed interconnect unencrypted. Encrypt-
ing the MPI data, even with hardware, would add latency. Even a minor increase
in latency can have a significant impact on scalability, and so encrypting MPI data
is undesirable. In practice, the unencrypted MPI data consists of numerical data
exchanged between the ranks of the job as it performs the calculations required by
the application. As such, a bad actor managing to this data is unlikely to be
able to derive any value from it. Every node in the job knows the identities of the
other nodes, so for a bad actor to partake in the MPI data exchanges, it would need
to spoof the identity of a valid node. Therefore, the risk of data leakage, even in an
unsecured network, is fairly low.
However, it is always possible that if an unsecured node hosting a bad actor can
send data to a secured node, then it may be able to exploit some previously un-
known weakness within the secured systems. In addition, these Network Interface
Cards (NIC) allow for Remote Direct Memory (RDMA). This means another
computer in the same network can the memory of a node without the host
being aware of it. Thus, this cannot be prevented via a local firewall.
These high-speed interconnects are switched networks that rely on asubnet man-
ager, also known as aFabric Manager (FM), for configuration, including the creation
of the routing tables. An attacker can try to imitate the FM on hijacked nodes
and bombard the switches with malicious configurations. In addition, an attacker
could try to spoof its source node and ingest packages in order to maliciously ma-
nipulate the execution of the job on the secure node. 7.3. General Design of the Secure Workflow93
FIGURE7.3: A schematic sketch of the secure workflow on an system, which is divided into eight distinct steps.
7.3 General Design of the Secure Workflow
As stated before, all systems to which users have direct could be considered
to be compromised and insecure as unknowingly malicious software by a may
have gained administrator permissions. Also, an administrator should not be con-
sidered completely trustworthy and permissions should be limited as much as pos-
sible. In order to design a secure workflow, data and software need to be protected
on such an exposed system and a mechanism is necessary to trust selected nodes,
on which the actual computations can be securely done. on the discussed
security problems identified in 7.2.3 a secure workflow is designed which
mitigates these problems. This secure workflow is presented in the following.
7.3.1 Assumption
In order to provide trust in an otherwise untrusted system, this trust needs to be
derived from a secure source system. Therefore, it will be assumed that i) theimage
serverof the system as well as ii) the local system of the , for instance,
the respective workstation or laptop, is secure. These assumptions are reasonable
because on the one hand theimage server, as shown in 7.1, is located within
the 2nd security level of a cybersecurity onion of the already highly guarded admin
nodes, which deploy only limited services. On the other hand, the local system of
the is the system where the data resides unencrypted at the beginning of the
workflow. Therefore, it has to be secure since otherwise the data would be leaked
without any involvement of the secure workflow.
7.3.2 Overview
As discussed in 7.2.3 there are different attack scenarios that a secure partition has to protect against. These scenarios can be divided into the protection of
the data in transit, at rest, and during compute. In order to secure data during transit
and at rest, encryption is typically . To secure data during compute, one needs
an ideally air-gaped, but at least reasonably isolated system or node. on these
two simple ideas a generic secure workflow is developed as depicted in 7.3.
Unlike the case of the typical submission workflow, presented in 7.2, it is
not possible to upload data, containers, and batch scripts directly into the shared file
system, since this would leave the workflow vulnerable to attacks. The solution is 94Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
to encrypt everything state-of-the-art encryption. Therefore, the first step is
to encrypt the data as well as the container on the local system of the . After-
ward, in Step 2 the encrypted data is then uploaded onto the shared storage of the system. The corresponding key is uploaded onto a key management system.
This communication channel is completely independent of the system and can
therefore be considered out-of-band. Analogously should be proceeded with the
containers.
In order to be able to retrieve the keys from the key management system, a valid token needs to be provided in the batch script. To prevent this token from
being leaked to an attacker on the frontend, the batch script needs to be encrypted
as well. This can happen completely independent of the resource manager by
implementing this mechanism on the secure node, on which the job should run. For
this, a public-private key pair is created on the system image of the secure node.
The public key is then distributed to the users and can be to encrypt the batch
script while the private key has to be highly guarded. Since it is only available on
the secure node and on the image server, this mechanism depends on the safety of
the latter. This encrypted batch script can be submitted to the resource manager just any other unencrypted script. For this, a correspondingdecrypt_and_execute
command needs to be implemented on the secure node, which takes as input the
encrypted shell script.
The resource allocation made by the resource manager in Step 4 is only depen-
dent on the useduidof the on the system. This is not secure enough,
therefore the authenticity of the batch script needs to be checked. In this proposed
reference workflow, this problem is solved by the signing the batch script after
it was encrypted with a private key. Here, the corresponding public key has to be
made available to the secure node before a job can be submitted.
During Steps 5 and 6 the counterpart of the previously described Step 1 is done.
This means, that first the signature of the batch script is checked to verify that this
batch script was legitimately submitted. Here, the stored public key of the who
has made the request is to match the signature. If that verification is successful,
the batch script is decrypted, yielding a shell script that can be executed.
Within this script, an token for the key management is provided. This
token is in Step 7 to retrieve the keys needed to decrypt the data and container.
In Step 8 the keys are to decrypt the data and container image from the
shared file system on the secure node. Now, the intended job of the can be
executed within the container. a container at this stage probably is the easi-
est way to maintain a heterogeneous software stack that is required to support the
diverse processing steps. As mentioned, the additional advantage here is, that the
container image itself can be encrypted and thus the integrity can be ensured, since
tampering is only possible if the key is known. In addition, even in the case that
some potentially sensitive information might be contained with this image it will
not be leaked to an attacker. Furthermore, mounting all unsafe file systems per de-
fault read-only into the container prevents an accidental data leak, for instance by
files that are temporarily written by the program without the knowledge of the .
7.3.3 Securing an OPA Fabric
Broadly, network security of the high-speed interconnect is applied in two areas: (1)
Partitioning the job’s nodes from the other nodes in the cluster, and (2) Securing the
FM from malicious interference. 7.3. General Design of the Secure Workflow95
Partitioning:This prevents packets from an unsecured node from reaching a
secured node, and packets from a secure node from leaking to an unsecured node.
Omni-Path, in common with Infiniband, uses a system of Partition Keys (PKeys).
PKeys are a mechanism to support soft partitions, which enable the creation of
multiple, overlapping communication domains. PKeys, secured nodes can
be run in a separate partition from the other nodes of the cluster. Within the
configuration of the FM, the partitions that are required and the nodes that should
be members of those partitions can be specified.
The FM sends the PKeys over the fabric to the switches and the adapters in the
nodes. The FM updates the PKeys whenever there is a change in the FM’s PKey con-
figuration. If the switch receives a packet for a node that does not have the required
PKey, it will block it. This is a very secure system, implemented in the hardware of
the switches and adapters, and can only be controlled by the FM. The adapter is log-
ically split into a fabric side and a PCIe side; the PKeys are managed from the fabric
side, so there is no way that malicious code on the node can change the PKey con-
figuration of the adapter. Therefore, by PKeys, it is ensured that no unsecured
node can send packets to a secured node.
Because a node can be a member of more than one partition, if it has been given
more than one PKey, a needs to specify which PKey to when running a
service that accesses the high-speed interconnect. For example, to launch an MPI job
on PKey0x0012, the sets thePSM2_PKEYenvironment variable to0x0012.
For the IP network interfaces typically by storage systems, a network in-
terface device is configured for each PKey that needs to be accessed. From within
Linux, these devices behave in the same way as Ethernet devices and each will be
configured on a different IP subnet. The network interface running with the default
PKey appears as the familiar ib0, and an additional interface on PKey0x0022would
appear as the deviceib0.8022
5
.
Securing the FM:The FM provides the PKeys to the switches and adapters in
the fabric. Therefore, it is the core component that ensures the legitimate operation
and isolation of the different networks. Thus, it is of utmost importance to prevent
it from becoming compromised or that some malicious interference with the FM can
take place.
It is possible that a bad actor on the bare metal of an unsecured node could bring
up a competing FM. It would be difficult to do this with predictable results, but the
activity could be disruptive. The FM’s traffic runs in the Admin vFabric with its own
PKey, and in a secure configuration, only the FM node(s), identified by the hardware
GUID of its adapter card, are full members of this vFabric. Thus, any FM- activity
from a node that has not been authorized will be blocked.
Therefore, it is important that the FM node (and its standby) are secured, mean-
ing that only the most privileged admin users are able to log in to these nodes.
7.3.4 Isolating Compute Nodes
In order to be able to provide secure compute capacity, an isolated node or subclus-
ter is exclusively available to an individual or group for an arbitrary amount of time.
These nodes have restrictive firewall settings and running administrative services
are slimmed down. Therefore, no login capability, e.g., via SSH, is available. Fur-
thermore, the nodes need to reliably boot a trustworthy operating system and must
enforce that only authorized users can the node. This is done in two steps:
5
The 0x8000 bit is always set in the device name. 96Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
i) Users can only submit jobs that are running in a non-interactive mode, therefore
they are required to submit a batch script to the resource manager, and ii) this batch
script needs to be signed with a secret on the local system of the . This secret
is a private key, which must never become compromised. The matching public key
is available on the secure node to verify the authenticity of the submitted job. Since
this secret will never be uploaded to the system, an attacker can’t get to
it and can’t submit jobs to a secure node from a falseuid.
Software that is usually provided via environment modules needs to be installed
in the system image of the secure node since these shared modules would
mean importing an untrusted codebase into the secure node. Alternatively, the
filesystem where the software modules reside needs to be exported read-only to all
exposed nodes, in particular, login and compute nodes, and only be writable from a
specially secured admin node.
7.3.5 Key Management System
The usage of the Key Management System (KMS) should also follow certain best
practices. Although the depicted security measures should be sufficient, it is favor-
able to a one-time token mechanism to retrieve the keys from the KMS. If an
attacker gets their hands on a token, with which the keys can be retrieved, the le-
gitimate request from the will fail, because the token was already . Thus,
it is immediately obvious that a security incident took place. The token should be
short-lived as well, to limit the availability of the keys in the case that a job crashes
before the token can be .
Additionally, a reverse proxy is placed in front of the KMS. Here, the received
API calls can be filtered on the source Internet Protocol (IP) address and the HTTP Verb. The goal is to allow an upload of keys from anywhere, however,
limit the legitimate requests to retrieve keys via HTTP solely to the secure nodes.
7.4 Implementation of the Secure Workflow
The design and implementation are done with a minimal of assumptions so
that it can be considered as a blueprint for other systems with different requirements
as well. on the general design presented in 7.3, implementation was
done on an system at GWDG.
7.4.1 Key Management System
Vault
6
is for the KMS. It allows for the distribution of personal tokens to indi-
vidual users. With those, users can generate tokens with limited permissions and a
short, configurable lifetime. Aresponse wrappingis on these tokens in order to
enable single- tokens to the deposited keys. In addition, the root token can
be reliably deactivated, preventing the root from spying on the keys.
In front ofVault NGINX
7
with thengx_http_geo_moduleis deployed as a reverse
proxy. It performs IP-address filtering on the HTTP verb in order to allow an
upload of a key from external systems but restricts the request for the key retrieval
to the known secure nodes.
6
https://www.vaultproject.io/
7
https://www.nginx.com 7.4. Implementation of the Secure Workflow97
7.4.2 Data and Software Management
Since most applications expect a POSIX-IO compatible file system, LUKS can
be to encrypt the data. These LUKS containers can be mounted, if the de-
cryption key is available, thus providing the expected interface while transparently
encrypting everything written to that mount. Encrypted containers are provided via
Singularity. Similar to the LUKS data containers, these encrypted Singularity images
are decrypted in kernel space as well. This means they reside decrypted in the RAM
of the host, thus swapping needs to be deactivated on these secure nodes, to prevent
that sensitive data is written unencrypted onto a non-volatile storage medium a
local SSD. By bind mounting only the LUKS data containers into the Singularity con-
tainer, it is ensured that only encrypted write is possible from the container
onto the file system. Since the encrypted Singularity images are also as a sig-
nature, i.e., to ensure that the image has not been tampered with, the secured
nodes need to be configured to only allow the execution of encrypted images.
7.4.3 Isolating a Secure OPA vFabric
The OPA FM uses a concept of VirtualFabrics (vFabrics) that can be to cre-
ate partitioned groups of nodes and/or apply a Quality of Service (QoS) to differ-
ent classes of traffic. In order to isolate nodes and networks from each other, only
partitioning is required. Here, vFabrics the previously described PKeys as the
underlying mechanism.
Nodes are assigned a membership of partitions within the configuration of the
FM. Nodes can be Full Members or Limited Members. Full Members can commu-
nicate with all members, but Limited Members can only communicate with Full
Members. This feature is useful when nodes in different partitions need to
the same shared resource. The following description is simplified to remove some
of the site-specific configurations at GWDG, see Table 7.1.
• 3DeviceGroupsare created in the FM configuration: General, Secure, and Stor-
age. These DeviceGroups hold the identities of the relevant nodes.
• 3 vFabric partitions are created: General, Secure, and Storage which contain
the corresponding DeviceGroups as Full Members.
• Additionally, the General and Secure DeviceGroups are defined to be Limited
Members of the Storage vFabric.
In this way, the nodes within the DeviceGroups General and Secure can both the Storage nodes, but the nodes in General and Secure cannot each other. The
Storage nodes must be secured in a similar way as the FM node(s), as a bad actor on
these nodes could get to the nodes in the Secure vFabric.
vFabricFull MembersLimited Members
GeneralGeneral
SecureSecure
StorageStorageGeneral, Secure
TABLE7.1: Partitions and their Members.
The FM responsible for defining and
enforcing these policies has to be lo-
cated within the security onion of the
admin nodes.Additionally, the FM
needs to be configured to quarantine
nodes if they try to spoof their identi-
ties, for instance in order to reach into a
secure vFabric.
Note:Currently, the OPA FM is not able to implement the Full/Limited -
ality in the GWDG environment, and so a workaround is required only Full 98Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
Membership. The workaround provides the same protections but with less flexibil-
ity and will be removed when the functionality is available.
7.4.4 Isolating a Secure Node
In order to isolate a secure node, the system image is hardened. To prevent an
attacker from logging into that node, a restrictive firewall configuration is imple-
mented usingiptables. In addition, suitable services for accessing these nodes, SSH, are turned off. For all services that need to be listening on a specific port, theSlurmd, only the IP address of the known counterpart, the node where the
Slurmctldis running on, is reachable. In order to ensure these settings, a node needs
to directly boot into these restrictive configurations and the image server, as well
as the network which is for the PXE boot, need to be trusted. Therefore it is
mandatory, that an attacker can not reach the management nodes, and particularly
not the level 2 layer of the employed security onion.
7.4.5 Submitting a Batch Job
In order to encryption for the batch script, a 4096-bitRSA[166] key pair is cre-
ated in the system image, and the public key is shared with the . Since the
scheduler might, Slurm does, require a valid bash script to be submitted, a
small workaround needs to be . The actual command, one wants to execute,
is encrypted in agpg
8
message and passed to a helper located on the secure
node, yielding a valid and effectively encrypted bash script. Such a batch script is
shown with a shortened PGP message in the following listing:
#!/bin/bash
/usr/bin/decrypt_and_execute <<EOF
-----BEGIN PGP MESSAGE-----
hQIMA8ErHWKpRkmoAQ/+LyPQJORoQwC5UjqNqXPcebqVYXfqgdt1rPo
[...]
=cRKs
-----END PGP MESSAGE-----
EOF
As previously discussed, the authenticity of the compute request needs to be en-
sured on the secure node. In order to do that, a detached signature of the batch
script is created on the local system of the and is uploaded into the same di-
rectory as the batch script on the system. When submitting the batch script to
Slurm via thesbatchcommand, the batch script is parsed and sent to the slurmctld.
From there it is copied to a secure node a communication channel between the
slurmd on the secure node and the slurmctld. After this is done, the slurmd starts
executing the job. However, before the just received and locally stored bash script is
invoked, theSlurmd Prologis executed. Since this is the first piece of code that gets
triggered to be executed on the secure node, here the detached signature the provided is compared to the local copy of the batch script. Only when this detached
signature matches, the -provided code starts to be executed. Upon failure, one
can choose to either cancel the job or quarantine the node.
After the batch script is decrypted, the resulting bash-script is executed by the
decrypt_and_executemethod. Here, the provided token is to get the keys from
8
https://gnupg.org/ 7.5. Extension for Multi-Node Support99
Vault. These keys are then briefly stored in atmpfsto mount the LUKS data contain-
ers and to execute the Singularity container. Since only the LUKS data containers
have a writable bind mount within the Singularity containers, results can only be
stored there, thus enforcing compliance with data security regulations per design.
After the job has finished or was killed by the resource manager, all mounted LUKS
data containers are unmounted and the stored keys are deleted from thetmpfs. This
behavior can be enforced within theSlurm Epilog. In the end, the can download
the LUKS output container, where the results are stored for further inspection.
Since the authentication of the is done by the Slurmd within the Slurmd
Prolog, one has to ensure that the execution of the Slurmd Prolog can not be cir-
cumvented. For admins, this is possible by an interactive session without an
allocation. This feature needs to be removed within the source code of the Slurmd.
7.5 Extension for Multi-Node Support
The described secure workflow is so far only intended for single-node jobs. When
extending this to multi-node jobs, which run for instance viaMPI, one is
confronted with three distinct challenges. The first is to ensure secure communica-
tion between the nodes running the job. This is already achieved by the previously
presented secure OPA configuration. The second challenge is the key distribution.
Since there might be multi-node jobs where the of nodes is not known be-
fore the job actually starts, or the orchestration across nodes is difficult and therefore
error-prone, it is advantageous to keep the general idea of single- tokens and
distribute the keys after the successful retrieval. The third challenge is to provide
suitable, encrypted parallel I/O.
7.5.1 Parallel Starter
In order to port the single-node to support multi-node applications, the de-
cryption keys need to be distributed to all involved nodes in a secure manner. Since
multi-node support requires a secure network to allow for safe MPI communication,
one can reuse this network to safely distribute the keys to all nodes. For this, aPar-
allel Startercan be , which is an MPI-capable program, where only the process
with Rank 0 will the single- tokens to receive the keys from the key man-
agement system. Afterward, the process with Rank 0 uses a broadcast to distribute
the keys to all other processes, where then each process needs to acquire a lock, a node-local semaphore, for the particular node they are located on. This process is
depicted in 7.4. If multiple processes are running on a single node, the first
process that gets the lock will write the file to the/keyspath, which is atmpfs, and
initiates the mounting of the encrypted data on the node. The other processes on
a shared node will skip this step. This ensures that on a single node that the keys
are only written once to the/keyspath and no race condition occurs. Since this is
a standalone tool, it is completely independent of the scheduler. Alternatively,
when Slurm, one can also simply asrun –procs-per-node=1.
It is noteworthy, that there might be other, more specific methods thesbcast
tool from Slurm. Although this can be a full-fledged drop-in replacement, one needs
to ensure that the communication only takes place inside the secured network. This
is guaranteed if the node is properly isolated as described in 7.4.4. 100Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
FIGURE7.4: Schematic sketch of the parallel starter.
7.5.2 Creating and Managing a Secure Partition
ASecure Partitionspecifies an isolated sub-cluster within the system.
This sub-cluster is also managed by the resource manager within a private partition
which only predefined users can submit to. In addition, the nodes that comprise the
secure partition can communicate with each other over a high-speed interconnect.
This is securely possible since the secure nodes of a secure partition are all within
a dedicated vFabric. This means, that no traffic is being routed from an unsecured
node to a secure node, therefore enforcing network isolation of the secure partition
on the switch level. Of course, all of the nodes in a secure partition are booted into
the same secure system image. In summary, all changes necessary to create a secure
partition can be done in software, rendering an additional procurement unnecessary.
Since only software and configuration changes are necessary, switching nodes from
insecure to secure operation can be done dynamically to match the provided re-
sources to the actual demand. For this, a script can be executed on the admin node
which automatically changes the configuration of the resource manager, the cluster
manager, the fabric manager, and the reverse proxy settings on the key management
system to adjust the IP filtering of the HTTP verb, and triggers a reboot of the corre-
sponding machine to boot it into the secure/insecure system image. Due to this high
degree of automation, the partitioning can be done quickly without much effort by
the administrators. The required time to reconfigure a secure partition as described
above is mainly determined by the time a node needs to reboot.
7.5.3 Parallel Input/Output
The concurrent write to a LUKS container from multiple nodes is not possible
by design and would corrupt the file system. Read-only mounts are supported as
they prevent the modification of the file system, therefore, the input containers could
be mounted on all nodes.
Assuming that the amount of data to be written by the application is unknown
prior to execution but can be bounded, e.g. 10 GiB, a LUKS container with the max-
imum expected size can be created as a file with holes and formatted
9
. Thus, the
actual occupied capacity of the LUKS container is small regardless of its size. For
example, a 10 GiB container file formatted with ext4 requires 69 MiB of space and
85 MiB if created with LUKS and then formatted with ext4.
In the following, when usability is mentioned it refers to the overall complexity
that is introduced. Although some tools can be provided that automate and simplify
the handling for any of these cases, some difficulty would still remain for the appli-
cation. In order to support parallel writes, there are various design alternatives with
individual advantages:
9
Forexampleusingthecommand$ dd if=/dev/zero of=/tmp/test bs=1024 count=1
seek=$((1024*1024*10)). 7.5. Extension for Multi-Node Support101
Single rank I/O with an encrypted LUKS container (SL):Only one rank in par-
ticular Rank 0 writes to a dedicated output container.Pros:the container could be
automatically created and mounted; no change of applications with traditional (non-
parallel I/O) is necessary; the output container can be fetched by the and di-
rectly mounted.Cons:It does not allow output from multiple nodes, hence perfor-
mance scalability is limited. This strategy is suitable for small volumes of output.
Parallel I/O to independent LUKS containers (PIL):Here, the ranks of each node
write output to the node-local LUKS containers.Pros:it utilizes a parallel file system
with the maximum performance and provides a node-local metadata cache.Cons:
usability, the of containers is dependent on the of nodes - making it
difficult to gather and aggregate the results for the . This may require adjustment
of the application and post-processing toolchain. This strategy is useful for cases
with demanding I/O and a robust post-processing pipeline.
Parallel I/O to encrypted files with keys stored in a LUKS container (PFKL):Each
process writes the data directly to the parallel file system but they encrypt each file
(individually). Practically, at program runtime, the processes decide upon an en-
cryption key and Rank 0 stores the key for this file in the initially prepared LUKS
container where it can later be fetched by the . A file might be created collec-
tively (requires coordination of the key) or individually.Pros:individual keys for
each file increase security. Compatible with independent or shared files.Cons:diffi-
cult to handle key retrieval in the post-processing and data analysis. Either the I/O
path or the application must support encryption. This strategy is not advised due to
the introduced complexity.
Parallel I/O to encrypted files with pre-shared key (PFSK):In this scenario, the would embed a key into the application or retrieve it fromVaultthat then is for the encryption of any created files.Pros:individual keys for each file in-
crease security. Compatible with independent or shared files.Cons:the I/O path or
the application must support encryption. As the key is known by the a-priori,
this strategy is superior to PFKL while integrating the advantages of the other meth-
ods.
Parallel I/O with an overlay file system with pre-shared key (POSK):In this sce-
nario, a would start an MPI job, where Rank 0 would fetch the key from Vault.
This key is then distributed across all nodes and an encrypted mount is created on all
compute nodes a stacked cryptographic file system eCryptFS, GoCryptFS,
EncFS
10
, or fscrypt
11
.Pros:easy way to provide a POSIX-compliant file system. No
support by the application is needed. Minimal barrier for the .Cons: must
be cautious to not write files unencrypted, i.e. to a wrong path. If a container is
, this can be handled similarly to LUKS containers by only bind-mounting the
encrypted path into the container. Parallel I/O to a shared file is not possible. If
multiple processes should write to a single file, strict file locking is required.
A qualitative comparison of the different methods is given in Table 7.2. Compat-
ibility refers to the supported I/O modes in the application and the complexity of
the overall setup and resulting artifacts. PFSK and POSK are currently the preferred
10
https://github.com/vgough/encfs
11
https://github.com/google/fscrypt 102Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
MethodPerformanceCompatibilityUsabilityComplexity
SL--++
PIL++---
PFKL++--
PFSK++++0
POSK++++++
TABLE7.2: Comparison of different storage strategies. A ++ indicates
the best solution within a category, a + a sufficient solution, a 0 indi-
cates a neutral evaluation, whereas a - indicates an explicit lack.
solutions for multi-node support. However, for PFSK to achieve optimal compati-
bility the transparent encryption in the utilized I/O library is necessary. Therefore,
PFSK can be superior for special applications that support this behavior but can-
not be as a default option. POSK is a good default option, although some
performance penalties might occur. Files with holes are a problem for stacked cryp-
tographic file systems, as a file that is sought beyond current EOF and written is
completely filled.
7.6 Security Analysis on the general design, presented in 7.3, and the actual implementation,
presented in 7.4, a concluding assessment of possible attack scenarios along
with their respective mitigation strategies is done in this .
7.6.1 Man-in-the-Middle attack
A man-in-the-middle attack can happen in this secure workflow during the execu-
tion of Step 2, as shown in 7.3. One can see, that on the one side, a man-in-
the-middle attack can happen during the communication with Vault. This commu-
nication is done via the provided REST API and is secured via TLS. On the other
side, an interception of packets can also happen during the upload of data to the system. Here, data is secure since it is encrypted on the client side and the
communication itself is guarded via SSH.
In both cases, the attacker would end up with state-of-the-art encrypted data,
which can not be without the corresponding decryption key. As presented,
these are highly guarded and only retrievable for authorized users. Thus, to
the network infrastructure outside of the system can not diminish the security
of this workflow.
7.6.2 Privilege Escalation
A only uploads encrypted data and encrypted Singularity container images,
thus the attacker can neither gain to the decrypted data nor can the software
environment that accesses the data directly be compromised. The same argument
holds for the submitted batch scripts. These are encrypted as well and thus ensure
the confidentiality of the token of the KMS.
As discussed, arootuser can submit jobs from theuidof a legitimate . This
can neither be prevented by the kernel nor by the resource manager relying on the
kernel. The obvious mitigation would be a multi-factor authentication which is 7.7. Performance Analysis103
prompted upon the submission of a batch script by a trusted management server.
This, however, needs to be supported by the individual resource management soft-
ware in . A resource manager independent way is presented before, where the
batch script needs to be signed.
To summarize, arootuser can neither get to the decrypted data, tamper
with the software or system image, and can not impersonate a on the system.
7.6.3 IP-Spoofing
In order to prevent an attacker from retrieving the keys stored in the KMS with a
stolen token,Nginxis as a reverse proxy in front of the KMS. Here, it filters out
GET requests from an IP address, which is not a secure node. This is configured on
the node of the KMS. To change that, to this system is required, including ac-
cess to the administrative network where the SSH port of the KMS node is available.
An attacker can, however, a false source IP address and mimic that the request is
done from a secure node. Then, the KMS would send the requested keys but would
do so to the specified secure node. Thus an attacker would still need to get to
such an isolated node.
7.6.4 Operating Errors
Since the presented secure workflow has quite some steps that a has to exe-
cute correctly to ensure the integrity of the processing, mistakes can happen and
potentially impair the security measures. In order to simplify the application for
a , wrapper scripts are provided, which, for instance, automatically create and
mount LUKS containers on the local system of a while strong random
passwords. Furthermore, it is ensured, that the created keys are only uploaded to
ourVaultinstance, and not accidentally on an untrusted system. Lastly, once a has written locally a batch script that is ready for submission, a script can be locally, to encrypt, sign, upload, and submit the batch script. That means, that a
secure client only needs to be set up and configured correctly once, and can then be safely by the users since all steps are executed automatically. This is still flexi-
ble enough to allow manual or automatic changes to the batch script a wants to
submit on a per-job basis.
7.6.5 Network Manipulations
Depending on the high-speed interconnect, there are additional threats associated.
In 7.4.4 it was discussed that an OPA fabric can be securely locked down to
ensure reliable operation even in the case of a privilege escalation on the connected,
-accessible nodes.
7.7 Performance Analysis
In order to determine the performance costs when switching from the unsecured
workflow depicted in 7.2.2 to the secure workflow presented in 7.3
and 7.4, different benchmarks are done. These benchmarks can be roughly
divided into two distinct groups. One type of benchmark is designed to quantify
the static overhead associated with the secure workflow, while the other measures
the dynamic cost of the encryption. The performance measurements of the
encryption are done once for a single node with LUKS and once with multiple nodes 104Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
Operation (unit)Performance
EncryptedUnencrypted
ior-easy-write[GiB/s]0.62.8
mdtest-easy-write[kIOPS]15.224.4
ior-hard-write[GiB/s]0.060.01
mdtest-hard-write[kIOPS]15.96.2
find[kIOPS]270.8211.8
ior-easy-read[GiB/s]0.62.2
mdtest-easy-stat[kIOPS]194.0121.1
ior-hard-read[GiB/s]0.30.4
mdtest-hard-stat[kIOPS]69.644.4
mdtest-easy-delete[kIOPS]22.633.4
mdtest-hard-read[kIOPS]0.72.1
mdtest-hard-delete[kIOPS]17.83.5
TABLE7.3:IO500results onBeeGFS.
on an eCryptFS and GoCryptFS stacked cryptographic filesystem. Since the secure
nodes are otherwise isolated, there are no additional costs during compute.
7.7.1 Measuring Encryption Costs
Encryption and decryption take place during write and read operations to a storage
device, a parallel file system. In order to simulate different I/O patterns to
get a better understanding of the potential performance decrease, theIO500[103]
benchmark is . The encryption is done withAES512[132] in the case of LUKS
and with AES-256 in the case of eCryptFS and GoCryptFS.
Performance Comparison on the Parallel File System on a Single Node with LUKS
As discussed in 7.3, the typical case for the secure workflow is assumed
to be that users upload their encrypted data onto the shared parallel file system and
only decrypt them on the secure nodes. In order to measure the performance costs,
two different scenarios are benchmarked. In the first case, an unsecured workflow
is , where an unencrypted Singularity container executes the before mentioned
IO500 benchmark on a native bind mount on the parallel file system. In the second
case an encrypted LUKS container, usingcryptsetup, is mounted locally on the node
with a loopback device. The latter case represents the secure workflow, therefore also
an encrypted Singularity container is to perform the IO500 benchmark. Both
container images in these two benchmarks are created the sameRecipe-file.
The benchmarks are done on a dedicated node of theScientific Compute Cluster
hosted by GWDG. It features an Intel Xeon Platinum 9242 CPU with 376G of DDR4
memory operating at 2934 MT/s and runs on an3.10.0-1160.36.2.el7.x86_64 Linux
Kernel. The filesystem runsBeeGFSand has 4 metadata servers and 14 storage
servers. The node is connected to the BeegFS storage via a 100 Gb/sOPAfabric.
Before the benchmarks is started, 343G of the 376G available memory is filled up
and the swap was deactivated. The LUKS container is opened viacryptsetup 2.3.3
and is mounted as an ext4 file system.
The results of the performed benchmarks are presented in Table 7.3. The first
observation is, that theior-easy-write, which is sensible to streaming performance,
reaches in the encrypted case only≈23% of the bandwidth of the unencrypted
case. For the other operations, it is much harder to interpret these results correctly. 7.7. Performance Analysis105
The reason is, that two concurrent effects which are close to impossible to disen-
tangle have an influence on the performance. The first effect comes from the pre-
viously mentioned encryption which is done bydm_crypt, which therefore leads
to a decreased read/write performance when being flushed out of the page cache.
The second effect is, that the unencrypted IO500 run uses for its metadata opera-
tions the metadata servers of the BeeGFS cluster. The encrypted IO500 run, how-
ever, does (almost all) metadata operations on the local node, since it has a locally
mounted ext4 filesystem. For the BeeGFS filesystem this LUKS container containing
this ext4 filesystem is only one large file. The metadata operations, which are pri-
marily benchmarked in the differentmdtestruns, are therefore handled by the local
node itself, not by the metadata servers from the BeeGFS filesystem. This problem
becomes even greater when comparing theior-hardruns since here 47008 Bytes are
written/read/stat/deleted. During themdtest-easyruns no data is written, and
during themdtest-hardruns only 3901 Bytes are written. Therefore, it is tricky to
directly compare the metadata performance between those two runs.
In summary, one can observe a non-negligible performance degradation, partic-
ularly during streaming IO, when compared to the unencrypted measurement. This
can be seen in the operations containing aneasy.
Analysis of Cryptsetup
A recent analysis of the dm-crypt implementation found that the different work
queues to enable asynchronous processing of I/O requests can actually dras-
tically slow done performance. To circumvent this problem, dm-crypt can be in-
structed to avoid a queuing of I/O requests and execute them synchronously. This
feature was merged into the Linux kernel in version 5.9
12
.
In order to further analyze the origin of the previously observed performance dif-
ference between the encrypted and the unencrypted case, the kernel of the node is updated to the most recent version 5.16.3, andcryptsetup 2.4.3is compiled
from source. Since the clients for the parallel file systems of theScientific Compute
Clusterdo not support newer kernel versions, the performance difference can only
be measured on the node. For this, a tmpfs is , which has the additional ad-
vantage of offering the lowest latency and highest bandwidth. This means, that any
additional overhead can not be hidden by bottlenecks located on the storage device.
The file for theloopback devicehas a size of 340G of the available 376G. In order to
support thevader BTLofOpenMPI[62] additional 10G is provided in a tmpfs.
The results of these measurements can be seen in Table 7.4. In both cases, i.e.,
the encrypted and the unencrypted ones, an ext4 filesystem residing in a tmpfs is
mounted via a loopback device. This means, that unlike in the case before, here also
the performance of the metadata operations can be compared, since all deviations
can be accounted to the overhead ofdm-crypt. One can see, that there are only slight
differences, however no clear trend can be recognized.
One important observation is that it can be confirmed that encrypted Sin-
gularity containers does not have any measurable performance impact at runtime.
The second observation is, that in this particular case one profits from an asyn-
chronous execution of the encrypted I/O operations during parallelized execution
with 10 processes. This can clearly be seen in the highlighted cells containing the re-
sults from the streaming intensiveior-easy-writeandior-easy-readoperations as well
as in theior-hard-readtest. The reason for this could be the of loopback devices
12
https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit
/?id=39d42fa96ba1b7d2544db3f8ed5da8fb0d5cb877 106Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
Operation [unit]SynchronousAsynchronous
10 Processes1 Process10 Processes1 Process
EncryptedUnencryptedEncryptedUnencryptedEncryptedUnencryptedEncryptedUnencrypted
ior-easy-write[GiB/s]1.21.21.11.01.61.71.01.0
mdtest-easy-write[kIOPS]111.7123.370.070.6111.4111.569.069.0
ior-hard-write[GiB/s]0.60.61.11.10.70.71.11.0
mdtest-hard-write[kIOPS]18.819.831.633.915.515.331.135.6
find[kIOPS]3821.43858.61493.41460.77093.15847.21456.01490.6
ior-easy-read[GiB/s]1.01.11.01.02.11.81.31.3
mdtest-easy-stat[kIOPS]558.9537.9183.5180.0566.7567.1179.1181.6
ior-hard-read[GiB/s]1.31.31.41.51.91.91.51.5
mdtest-hard-stat[kIOPS]391.7422.5186.4186.7448.3403.7184.8187.6
mdtest-easy-delete[kIOPS]78.374.2102.6103.381.482.2103.5102.4
mdtest-hard-read[kIOPS]188.0180.748.549.2213.8209.345.746.2
mdtest-hard-delete[kIOPS]64.962.475.379.463.260.778.673.7
TABLE7.4: Results of the IO500 benchmark on an encrypted LUKS
container residing in a tmpfs. The specification Encrypted and Unen-
crypted refers to the Singularity container.
Operation (unit)Performance
1 Process10 Processes
ior-easy-write[GiB/s]0.92.1
mdtest-easy-write[kIOPS]68.9131.0
ior-hard-write[GiB/s]0.80.8
mdtest-hard-write[kIOPS]32.430.8
find[kIOPS]1391.25832.1
ior-easy-read[GiB/s]2.12.6
mdtest-easy-stat[kIOPS]180.3584.3
ior-hard-read[GiB/s]2.62.0
mdtest-hard-stat[kIOPS]173.8380.4
mdtest-easy-delete[kIOPS]98.772.4
mdtest-hard-read[kIOPS]72.8206.9
mdtest-hard-delete[kIOPS]77.561.4
TABLE7.5:IO500results on anext4mounted loopback device resid-
ing in antmpfs.
and device mappers, which causes differences in the execution of the I/O requests
at the block device level when compared to a natively encrypted block device, a
hard drive or an SSD.
In order to estimate the actual encryption cost, a baseline for an unencrypted
scenario is measured, wherein the exact similar setup the same file is mounted as an
ext4 file system with a loopback device without the usage of cryptsetup. The results
are shown in Table 7.5. By comparing the performance increase when scaling from 1
process to 10, there is still very limited scalability exhibited. The source for this issue
is assumed to lie within the usage of a loopback device. Comparing the results of the
ior-easy-write, where by far the most data is being written and therefore is mostly hit
by the cryptographic overhead one can see that by comparing to the asynchronous
test in Table 7.4≈80% performance is achieved. The achieved value of≈2.1 GiB/s
is very close to the value of≈2.3 GiB/s one obtains when running the provided
benchmark suite of cryptsetup.
In summary, one can clearly see a performance advantage of newer Linux ker-
nels, however, it is not possible to replicate the advantage of synchronous crypto-
graphic I/O execution on this system. 7.7. Performance Analysis107
Operation (unit)eCryptFSGoCryptFSNative
Nodes, Procs. per Node1,11,1010,11,11,1010,180,101,11,1010,180,10
ior-easy-write[GiB/s]0.41.53.40.21.61.66.01.43.83.89.2
mdtest-easy-write[kIOPS]1.97.613.41.911.815.559.4525.931.383.5
mdtest-hard-write[kIOPS]1.72.76.71.0525.77.82.45.17.512.0
find[kIOPS]83.1271.828.312.934.553.2156.174.9251.2268.4966.1
ior-easy-read[GiB/s]0.41.52.50.41.42.53.20.522.98.7
mdtest-easy-stat[kIOPS]20.598.34.610.63084.1374.115.4124.4137.8392.7
mdtest-hard-stat[kIOPS]20.656.829.19.376.475.117.955.5112.8138.6
mdtest-easy-delete[kIOPS]5.18.81.9317.316.453.17.131.132.357.5
mdtest-hard-read[kIOPS]0.221.20.33.22.511.80.42.12.423.8
mdtest-hard-delete[kIOPS]4.43.61.22.32.8610.53.92.84.613.3
TABLE7.6:IO500results of an eCryptFS layer on top of a BeeGFS
cluster compared to a native BeeGFS mount.
Performance Comparison on the Parallel File System multiple nodes
with eCryptFS
In order to extend the secure workflow to multi-node support, and to offer alter-
natives to LUKS, also filesystem-level encryption is examined. For this eCryptFS is
set up on 10 nodes, which are otherwise identical to the system described in Sec-
tion 7.7.1. On these 10 nodes, the same directory on the shared parallel filesystems,
i.e., the previously described BeeGFS cluster, is mounted the same passphrase.
The specified symmetric cipher is AES with a 32-byte key and activated filename en-
cryption. Unlike the OpenPGP standard, eCryptfs allows for random in a
single file [76]. To this end, eCryptFS breaks a file into different distinct parts so-
calledextents, which have been encrypted with a cipher operating in block chaining
mode. Therefore, any read or write operation only requires the entire extent to be
decrypted, not the entire file. This however also entails, that parallel I/O of multiple
processes from multiple nodes needs to respect this offset which is defined by the
size of the extents. Unlike the case of an unencrypted file system, it is not enough
to define a non-overlapping offset among all processes to ensure undisturbed file
, but this non-overlapping needs to be ensured on an extent level.
Therefore, the IO500 configuration needs to be adapted, to accommodate this
issue. Here, all shared file I/O is disabled to ensure an undisturbed run. The results
of these reduced runs are shown in Table 7.6. One can see nearly linear scalability in
streaming operations, i.e., primarily ior-easy-write and ior-easy-read, when scaling
from one node to ten. Please note, that in Table 7.6 the ior-easy-write for one process
on one node is rounded up from 0.35 GiB/s.
When comparing the performance of the eCryptFS stacked filesystem against
the native BeeGFS mount, one can see a performance drop of 75% when comparing
the performance of the ior-easy-write for a single process on a single node. How-
ever, when scaling out to 10 processes on a single node, this difference is already
reduced to a 60% performance drop. When 1 process per node and 10 nodes
the performance difference is reduced further to 8% performance penalty. These
values are obtained by dividing the unrounded values from the eCryptFS rows with
the matching ones from the native table. The rather constant performance differ-
ence in themdtest-easy-write, where an empty file is created, shows the additional
latency of going through the FUSE filesystem before actually triggering the under-
lying filesystem. This performance difference, due to the additional latency, shrinks
again drastically in themdtest-hard-writeoperation, where 3901 bytes are written to
the file. Here, one also has an additional overhead due to the initialization vectors
and the mismatch of the 4KB sized extents, which need to be fully processed for 108Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
the employed block cipher. Since there is a slightly higher performance drop when
there are 10 processes on a single node, there might also be the effect of shared, and
stacked caches, as described by [204]. Similar arguments hold for the reverse opera-
tion, i.e. the read.
Performance Comparison on the Parallel File System Multiple Nodes
with GoCryptFS
GoCryptFS also provides file- encryption on a FUSE filesystem and is
inspired by EncFS. It encrypts files AES-256. Similar to eCryptFS, it also splits
a file up into individual blocks of 4KiB in size and generates a 128-bit Initialization
Vector. Additionally, filenames are also encrypted AES-256. Analog to the
previous analysis in 7.7.1, the IO500 benchmark is run with 1 process on
a node, with 10 processes on a node, with 1 process, and 10 nodes, and with 10
processes on 80 nodes.
Comparing the performance of eCryptFS with GoCryptFS one can see a clear ad-
vantage of eCryptFS when comparing the scaling from one node to ten nodes. Here,
the single process and the multi-nodeior-easy-writeperformance is twice as large,
while the saturation on a single node with multiple processes is similar. However,
GoCryptFS exhibits still scalability when extending from 10 nodes to 80. Although it
can not catch up to the performance of the native BeeGFS mount, the measured per-
formance shows reasonable results with two-thirds of the native performance, while
still having a more predictable behavior when scaling since it does not depend on
the process distribution on the different nodes as much as eCryptFS does.
When comparing the metadata performance, e.g.mdtest-easy-writewith the na-
tive BeeGFS client in Table 7.6 one can see again the performance drop caused by the
additional latency of the FUSE filesystem which sits between the application and the
actual filesystem and needs to process every request. In the case of themdtest-hard-
writeoperation, one can see a lower performance as the eCryptFS, hinting that the
actual processing of the 3901 Bytes is less efficient. This matches with the observa-
tions from theior-easy-write.
7.7.2 Measuring the Static Overhead
In order to determine the static overhead of this secure workflow, a node is booted
into the secure image, and the workflow is executed 1000 times. The static overhead
contains the verification of the signature, the consecutive decryption of the batch
script, the retrieval of the keys from Vault, the mounting and umounting of the LUKS
containers, decrypting and starting the Singularity container, and the deletion of the
keys residing in memory. The reference job is executingsleep 10on bare metal as
a baseline and within an encrypted singularity container for the secure workflow
measurements. The complete wall clock time is measured withtime. This job is
submitted 1000 times with the normal workflow discussed in 7.2.2 and 1000
times with the secure workflow as implemented in 7.4. For each job, 3 keys
have to be retrieved, one for the Singularity container, one for the LUKS container
with the input data, and another LUKS container to store the output data. Both
LUKS containers have a size of 20 GB. The batch script, which needs to be decrypted,
has a size of 544 bytes unencrypted. The result of the benchmark is obtained by
subtracting the average amount of the 1000 normal submissions from the individual
wallclock time spent in the secure submission. The resulting distribution is shown
in 7.5. One can see that it follows a normal distribution with an expectation 7.8. Cases109
FIGURE7.5: Distribution of the individual static overhead measure-
ments of the secure workflow when compared to the same job exe-
cuted with the normal workflow.
value at 6.63 s and a 3 sigma limit of±0.04 s. This overhead is negligible compared
to a typical runtime of tens of minutes to several hours.
7.8 Cases
Within this , three different cases are presented. First, a detailed analysis
of the presented secure workflow is done a trained machine-learning model
to analyze nocturnal polysomnography data. The remaining two cases cover the
discussed MRI project, where first a k-space reconstruction is down. Then the result-
ing artifacts, i.e., NIfTI files are for further analysis. The latter is implemented SecureHPC as a stand-alone service which is included in a clinical workflow.
7.8.1 Sleep Analysis
This describes a case, where the secure workflow is to analyze noc-
turnal polysomnography data. The intent is to further illustrate a concrete setup of
an end-to-end secure workflow, with a particular focus on the client machine, and to
compare the performance of the secure workflow with a bare metal system on a case and hereby offer additional insight into its applicability compared to the
synthetic IO500 benchmark. For this, a machine learning case is chosen, since
those are typically heavily relying on IO performance. This case resembles the
case discussed in 7.7.1, where in the naive setup without any optimizations
a performance drop of≈75% in streaming performance is observed. The measure-
ments were done by Nicolai Spicher. 110Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
General Description
In order to compare the secure workflow introduced in 7.3 to an unsecured
workflow as described in 7.2.2, the performance a typical example
from the life sciences is evaluated. This workload is provided by a sleep stage as-
sessment which analyzes nocturnal Polysomnography (PSG) data. This data is ac-
quired in sleep laboratories and is inspected usually by medical experts who divide
the data into epochs of 30 seconds and assign a class to each epoch depending on
the depth of sleep [18]. Due to the large amounts of data, this is a tedious and costly
task, and – although the sleep classifications are intended to provide an objective
assessment – there is still room for subjective interpretation [217]. This raised the
interest in developing algorithms for faster and more objective classification.
PSG recordings typically include different biosignals such as EEG, Electroocu-
lography (EOG), Electromyography (EMG), and Electrocardiography (ECG) which
are measured continuously. These signals not only contain information about the pa-
tient’s health status but can also be to identify him/her [20, 205] which makes
the protection of this data highly relevant.
For the performed experiment, the SIESTA project [99], which includes record-
ings of 98 patients with sleep disorders (e.g. sleep apnea, periodic limb movement
syndrome) and 194 healthy controls, is . The data was recorded in different Eu-
ropean sleep laboratories and is stored in European Data Format (EDF) with a total
file size of 104 Gigabytes. From this dataset, 100 EDF files are randomly picked for
analysis.
Sleep stages classification is performed the pre-trained ”Stanford Stages”
algorithm [187] which consists of three independent convolution neural networks
for EEG, EOG, and EMG data which are jointly fed into a long short-term memory
network. The final output layer assigns the labels ”wake”, ”Stage 1 sleep”, ”Stage
2 sleep”, ”Stage 3/4 sleep”, ”Rapid Eye Movement (REM) sleep”, or ”Unscored” to
15-second intervals. This algorithm is pre-trained on multiple thousands of PSGs
stemming from 12 different sleep centers in diverse recording environments and
following different protocols.
The source code of the master branch
13
is ,condafor installing dependen-
cies as described in the README file is applied, and a custom JSON file containing
the configuration is defined. The algorithm processes each EDF file separately and
generates a visual output called a ”hypnodensity plot” showing time on the x- and
probability of each class on the y-axis. Fig. 7.6 shows this visualization exemplary
for one night (7.5h) of a single patient. The y-axis shows the probability distribution
per sleep stage and thereby allows to assess the certainty of the algorithm: Columns
with constant colors (stages) depict time ranges where the algorithm has a high cer-
tainty and the more colors (stages) there are in a column, the lower the certainty. A
higher level of uncertainty can typically be observed in the transition between stages
and is also present when medical experts annotate data. For the data depicted in Fig.
7.6, typical patterns of sleep become visible with phases of deep sleep in the first half
of the night, more often REM sleep in the second half of the night, and phases of brief
awakenings in the early morning.
13
https://github.com/Stanford-STAGES/stanford-stages 7.8. Cases111
Setup of the Secure Workflow
For illustration purposes and reproducibility, the code of this case is provided
on GitHub
14
. As shown in 7.3, the starting point of the secure workflow is a
safe client machine. In this particular setup a Virtual Machine (VM) in an OpenStack
environment [176] is to simulate this client machine. Once the described input
data is copied into a LUKS container a provided helper script, this LUKS con-
tainer can be uploaded to a specific path in the ’s scratch space. An additional
empty LUKS container for the output data is created and uploaded. The two keys re-
main securely on the local VM. A completely analogous procedure is followed with
the encrypted Singularity container, which is built once and is then uploaded into
the ’s home.
The remainder of the secure workflow is processed by a fully automated script.
The job-specific commands, i.e. the generalized batch script that should be exe-
cuted on the systems, are written into a single file calledcommand.sh.template.
After this job-specific file has been written, the automated secure workflow script
can be executed. Here,command.shis created as a copy of the template. Then
the keys are uploaded and the necessary single- tokens are generated in Vault,
which is then automatically inserted into thecommand.shusingsed. Afterward, the
command.shscript is encrypted and copied into arun.shas shown in the listing in 7.4.5. Lastly, usinggpg –detach-signa detached signature is created and
stored asrun.sh.sig. Both files,run.shandrun.sh.sigare copied to the sys-
tem into the same folder, which must be readable forroot. Since Slurm is in
this particular case,sbatchis to submit the job to asecurepartition. After
the job is finished, the output LUKS container is downloaded to the secure client
machine, in this case, the beforementioned VM, usingscp, and the LUKS container
is mounted locally to get to the data.
It should be mentioned, that the creation of the input data LUKS container is
not part of this fully automated process, since the job is run on the same input data
multiple times. Generally, the creation and uploading of the input data container
can be part of this automated script to offer a complete end-to-end secure processing
pipeline.
Results
Fig. 7.7 shows a comparison of run times when an unsecured workflow (max:
302s, min: 208s, median:213.5) vs. the secure workflow (max: 338s, min: 212s,
median:220). The runtime is measured differences in time stamps of the hypn-
odensity plots. As can be seen, results are bimodal with two clusters with the major-
ity of run times being in the first cluster within the interval of 208s-250s. The second
cluster begins at around 300s and represents larger EDF files due to longer night
sleep. In 79% of the times, the secure workflow has a longer runtime, in 4% it is
equal, and in 17% the secure workflow is faster than the insecure one. This is a clear
indication, that the runtime of at least 40% of all runs is so close to each other that
statistical fluctuations of the system performance are the dominating factor, not the
overhead of the secure workflow. When comparing the medians one can see that the
secure workflow takes≈3% longer than the insecure workflow.
Considering that the secure workflow enables the processing of sensitive data
and therefore creates the opportunity to reuse a shared and existing cluster
14
https://github.com/gwdg/secure- 112Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
FIGURE7.6: Resulting hypnodensity plot for a single night with
values on the x-axis representing 15s windows. Stages are ”wake”
(white), ”Stage 1 sleep” (pink), ”Stage 2 sleep” (turquoise), ”Stage
3/4 sleep” (blue), ”REM sleep” (green).
FIGURE7.7: Distribution of the running times when processing single
nights for sleep stage classification the secure workflow com-
pared to the insecure workflow.
for analysis of patient data which, due to privacy constraints, would be impossible
otherwise, a 3% performance drop seems fair.
7.8.2 K-Space Reconstruction
Although confidential computing is not explicitly required for the k-space data within the "Big Data MRI"-project, an example reconstruction BART is created
by Vitali Telezki to serve for a tutorial
15
. Unlike the previous data lake example pre-
sented in 6.4.4, where BART is dynamically developed and can therefore be
built for each submission, this can not be done in this scenario. That is because the
bind-mount would have an exposed binary which an attacker could tamper with.
Therefore, all dependencies must be included within the container image. An ex-
ample of such a recipe is shown in 7.8. In order to trigger the reconstruction
of Shepp Logan a container image derived from this recipe, it expects to get
passed a path as a command line argument, where a folder calledshepp_loganis lo-
cated. It then performs a uniform fast Fourier transform on the provided input data
and saves the result inoutput_fft/. Afterward, the LUKS container containing the
reconstruction can be downloaded and locally decrypted.
This simple example can be extended to enable more functionalities, for instance,
the parallel image reconstruction thebart picscommand, as described in 6.4.4. This might require, that the corresponding git repository gets cloned
and to build one specific branch within thepostsection in 7.8.
15
https://github.com/gwdg/secure-/blob/master/tutorial 7.8. Cases113
Bootstrap: docker
From: debian:testing
Stage: spython -base
%post
apt -get update
apt -get -yy install bart
%runscript
bart fft -u -i 3 $*/ shepp_logan $*/ output_fft
FIGURE7.8: Excerpt from the Singularity recipe to perform an
inverse fast Fourier transform.
7.8.3 Volumetric Analysis of MR Images
Once a k-space image is reconstructed and formatted to the NIfTI format, further
processing can take place. For brain scans one of these tasks is a volumetric anal-
ysis and surface- thickness analysis. These can be done withFastSurfer[84],
FIGURE7.9: Segmented and reconstructed
brain scan after the FreeSurfer- process-
ing is done.
a deep-learning- neuroimaging
pipeline that is fully compatible with
FreeSurfer
16
. It allows to run segmenta-
tions and volumetric calculations of the
brain and can then reconstruct the cor-
tical surfaces and map cortical labels.
Due to the deep-learning , the
segmentation can run on an accelera-
tor, a GPU. However, the following
surface reconstruction requires a Cen-
tral Processing Unit (CPU). The result of
this pipeline can be seen in 7.9.
This pipeline can not only be run
within the context of the discussed "Big
Data MRI"-project as part of the over-
arching data lake but the novel Se-
cureHPC service can also be as
a stand-alone service to serve clinical
workflows, as is demonstrated in Fig-
ure 7.10 for the discussed volumetric
analysis in a case.
If a patient signs the consent form,
agreeing that his/her data is processed at GWDG, a node within thePATLANis trig-
gering the automatic segmentation. For this, the MRI console sends the T1-weighted
MRI scans to the Picture Archiving and Communication System (PACS) and addi-
tionally to the secure client. There, the images are formatted from the Digital Imag-
ing and Communications in Medicine (DICOM) file format to NIfTI. The NIfTI file is
then put into an encrypted LUKS container, which is uploaded to the system.
The key from the LUKS container is put into the KMS, and the short-lived, single- token is put into the batch script, which is then encrypted, uploaded to the system, and then submitted. Since the same job is running on different input data,
the corresponding Singularity container image is residing on the system the
entire time and is re- across all jobs. The key for the container image is also
16
https://surfer.nmr.mgh.harvard.edu/fswiki/FreeSurferMethodsCitation 114Chapter 7. SecureHPC: A Workflow Providing a Secure Partition
FIGURE7.10: Complete clinical workflow from the MRI scanner to
the doctors.
stored on the secure node and needs to be uploaded to the KMS for each unique job
execution. Within a dedicated database, personalized statistics are recorded to build
a large dataset over time. This has to be done pseudonymization instead of
anonymization to ensure that individual patients can be deleted upon revocation of
their consent according to Art. 7(3) GDPR.
To get the most benefit from this workflow, a doctor wants to have the result
of this pipeline, compare 7.9, at the same time s/he looks at the scanner re-
constructions, i.e., the final MR images, for diagnosing. In order to fulfill this time
criticality two things are necessary. First, the scanning protocols need to be adapted
to make the necessary T1-weighted measurements at the beginning, and the com-
plete measurements have to be immediately sent to the Secure Client by the MRI
Console. By linking the analysis to the original data within the PACS, doctors can the segmentations similar to the associated scans. Second, the computation
has to be done within a maximum of 20 minutes, ideally less, in order to be still
relevant for the diagnosing doctor. To satisfy this strict time limit it is advantageous
that the presented secure workflow enforces an isolation on a per node granularity
within the kernel space. This allows to transparently accelerators GPUs to
drastically speed up tasks inferences of neural networks. This feature would not
been possible by other isolation methods, the discussed TEEs, since these do not
support peripherals.
7.8.4 Evaluation
The presented workflow to the novel SecureHPC service is extensively tested synthetic and cases to demonstrate the general applicability. Due to
the modular design of the data lake, the processing of sensitive data can be treated as
an isolated and independent service. This successfully maximizes the overall impact
of this research, since it can not only be for the data lake within the driving "Big
Data MRI"-project but it can be deployed outside this scope even within the intranet
of a hospital as part of an overarching clinical workflow serving patients. The
additional cost due to the encryption is determined to be minimal in practice while
showing a significant impact on synthetic workloads. This performance cost reached
up to three-quarters of the entire computing time, compared to an unencrypted case.
On a realistic with a more balanced workload, this is diminished to only three
percent. However, in practice, a more fundamental drawback is the current coarse-
grained isolation. This is done on a per-server level to provide full capabilities, 7.9. Summary115
including secure to GPUs. Suppose a single , or institute sharing a secure
partition, cannot fully exploit all the resources of the provided partition. In that
case, these resources are currently unused, since no further sharing across the fixed
organizational boundaries is possible.
7.9 Summary
Within this chapter, a secure partition on a shared system is developed that can
be accessed a specific secure workflow. This partition provides the necessary
isolation required to process sensitive health data. It utilizes non-interactive batch
jobs, which are fully defined in advance within a batch script. These batch scripts
are written and fully encrypted on the client side. Therefore, these secure jobs
represent fully encapsulated functions. These functions can be invoked either by SSH connections or by HPCSerA, proposed in Chapter 5. Thus, the in-
teraction paradigm of the developed SecureHPC service integrates seamlessly with
the previously utilized FaaS interfaces. Therefore, the proposed SecureHPC service
can be considered within the context of the data lake as another compute backend,
next to the normal and cloud systems, which can be accessed the dis-
cussed FaaS interface. However, the proposed SecureHPC service can also be completely independent of the overarching data lake architecture within cases
solely requiring scalable and trusted execution environments. 117
Chapter 8
Secure Metadata Management
Within this chapter, two different methods for secure and scalable metadata han-
dling on systems are designed. One extends the in Chapter 7 presented Se-
cureHPC service, while the other is a completely independent method. First, the
background and related work is discussed in 8.1 followed by the design
of a cluster spawner, a throughput benchmarker, and an - encryp-
tion method in 8.2. The experimental setup is described in 8.3
followed by a discussion of the results in 8.4. on these measure-
ments a concluding evaluation is done in 8.5. Parts of this chapter are
accepted for publication in [143] and are submitted to [142].
The data lake, that is presented in Chapter 6, uses a modular design with three main
pillars, i.e., storage, compute, and metadata handling, which are unified by the ab-
straction of (F)DOs. Due to this modular design, it is possible to build a secure data
lake for sensitive data by securing each individual component. While storage and
compute are secured in the previous chapters Chapter 6 and Chapter 7, the last miss-
ing service is the overarching data catalog where all metadata are indexed. Such a
secure data catalog service can then be referenced within the (F)DO record, point-
ing to the location of the metadata, and providing machine-actionable instructions
on how to retrieve them. Therefore, this problem is again well encapsulated since
a well-defined interface can be provided to integrate this service into the overar-
ching data lake. In order to maximize the overall impact, this well-encapsulated
problem should be tried to be generalized, to provide a stand-alone service, that can
also be outside the scope of the data lake. During the implementation of the
data lake presented in Chapter 6 Elasticsearch (ES) is to provide the indexing
of the metadata of the DOs. Therefore, ES is also here as well to better facil-
itate the required changes when going from a normal to a secure deployment. For
this transition, two completely different workflows are proposed. The first utilizes
the in Chapter 7 presented SecureHPC service where the deployment of on-demand
ES clusters and the consecutive execution of data ingests or queries within usual batch jobs are considered to be just one very specific kind of workload that
can be executed on this generic platform. on the experiences summarized in
Chapter 7, only GoCryptFS is for filesystem encryption, since it exhibits the
best scalability and has a dynamic storage size since it is a stacked cryptographic
filesystem. This is in contrast to the presented LUKS containers, where the maximal
size of the filesystem needs to be known beforehand. The second proposed method
is completely independent of SecureHPC and solely relies on client-side encryption
and does therefore not require a trusted execution environment on the server side. 118Chapter 8. Secure Metadata Management
8.1 Background and Related Work
Historically, semantic metadata is often encoded by researchers within filepaths and
filenames. This practice has the disadvantage, that globbing has a much worse per-
formance compared to indexing due to its recursive nature, which does not scale.
In addition, higher-order logic aggregations always have to be implemented by
hand and therefore exhibit similar scalability problems. Thus, this is slow
and work-intensive. Therefore, tagging and indexing data on storage systems
has become increasingly popular. Examples of this are VAST Data, which offers a
tagging mechanism on an SQL engine, or the Ngenea Hub which uses ES on
top of a GPFS to provide unstructured data management. These systems generally
share the characteristic that they rely on a fixed of dedicated servers to sup-
port their query engine. This has the disadvantage, that these servers are idle, if they
are not sufficiently utilized, or if the of provided servers is too small to han-
dle the incoming requests. Therefore, it seems more appropriate to the available
nodes of an system to dynamically scale up or down on-demand ES clusters,
reuse existing resources, and offer dynamically and -case optimized scaled clus-
ters. For this, a workflow is required to spawn on-demand ES clusters in space standard Slurm job allocations.
To enable the usage of sensitive data within such a workflow additional security
measures have to be taken, similar to Chapter 7. For this two approaches will be
developed. First, the SecureHPC partition is to run this workflow, thereby pro-
tecting the data in the same way any other workflow is protected SecureHPC.
A different for encrypting information in a NoSQL database to ensure data
privacy even in remote environments a public cloud is to encrypt the data on the
client side. One example of this isSecureNoSQL[2], where a trusted Proxy is to
encrypt the JSON- documents before ingesting them into a NoSQL database in
an untrusted environment. On this proxy, a security plan defines the security poli-
cies the cryptographic modules. It uses Order-Preserving Symmetric Encryption
(OPE), a deterministic encryption method that maps integers from a range of [1,M]
to a range of [1,N] [26, 97]. If this mapping is uniform it maximizes the Shannon
entropy, while the total value of it depends on the value of N-M. The larger this
value, the higher the Shannon Entropy and the more secure this encryption scheme
is. The advantage of OPE against other encryption methods is that it still enables
efficient range queries on the encrypted data. A different is demonstrated
in [39] where an index in a document- NoSQL database is constructed anAdelson-Velsky Landistree [1] which is encrypted an Order Revealing En-
cryption (ORE) [109], a generalized formulation of OPE. Since no extensive changes
on the server side are desired, a similar toSecureNoSQLis designed, which
works completely on the client side, i.e., without the proxy for easier integration into workflows.
In order to determine the applicability of such a workflow, once designed and
developed it needs to be benchmarked. The canonical tool for ES benchmarking is
Rally
1
, the official macrobenchmarking tool fromElastic. It runs distributed bench-
marks with standard and -defined datasets. Different benchmarking scenarios
are defined as so-calledtracks. Every track contains one or morecorpora, which con-
tain all JSON documents to be ingested into the according indices. Furthermore,
every track contains manyoperationssuch as the ingestion or specific queries, which
are then structured into achallenge’s schedulein a fork-join model. This benchmark
1
https://github.com/elastic/rally 8.2. Design119
is sensitive to latency increases, therefore limiting its usefulness in high-load and
high-throughput workloads.
8.2 Design
In this , the design of the proposed on-demand cluster spawner, the through-
put benchmarker, and the - Encryption (ABE) are presented.
8.2.1 On-Demand Cluster Spawner
The on-demand cluster spawner uses MPI to automatically discover the available
hardware nodes of an job, by the pre-configured MPI environment of a
batch job. This means that no previous configuration by the users is required, the hostnames or IPs of the involved nodes. This dynamic host configuration is also
capable of managing changing hostnames from one batch job to another. This also
implies that the cluster can be dynamically sized, basically supporting any cluster
size. To increase portability and reproducibility, ES is packaged into a controlled Sin-
gularity container, which is fully node-transparent and runs the--cleanenv
option. Additionally, it supports statefulness between jobs, which removes the need
for reingesting on every job spawn. The container is on the Ubuntu 22.04 of-
ficial docker image. ES itself can be optionally included in the image, requiring that
each builds the container image themselves to ensure the correct uid/gid set-
tings. Alternatively, it can be bind-mounted at runtime. For this, atar.gzis shipped
with the spawner from which ES is extracted, and the uid/gid are set to the correct
. In this way, ES can be deployed fully in userspace without any need to re-
configure anything on the -server side. To achieve statefulness the data path
where the ingested data, i.e., indexes are stored is bind-mounted. In addition, by
bind-mounting also the configuration and logging paths, the dynamic movement
between different nodes, IPs, and even network interfaces, is achieved. Since the
hostnames and IPs are not persistent from run to run, these are abstracted by the
MPI ranks of the spawner. For this, anMPI_GATHER, all hostnames and their
ranks are sent to the root rank. Then, the root creates or updates the ES configs while
the other nodes are waiting at a barrier. The MPI root then updates the ES configs
for all nodes with their IP addresses. This does not affect the mapping of nodes,
i.e., the mapping of MPI ranks to the individual indexes stored on the shared filesys-
tem. The first ranks are master-eligible nodes, from which only one node can be the
active master. All nodes are active data nodes. Due to the abstraction of the host-
names by the MPI rank, any arbitrary permutation of hostnames will work, without
the users realizing it. After creating the configs, each host starts the Singularity con-
tainer with its config and the previously created ES configs bind-mounted in. The
only change that is required on the nodes of Emmy is a higher-than-default of memory-mapped areas per process, i.e.,vm.max_map_countof 262144, where the
default value is 65530. This is needed because ES uses ammapfsdirectory to store its
indices.
8.2.2 Distributed Thoughput Benchmarker
The proposed benchmarker is currently split into two tools. A write-focused bench-
mark measuring the ingestion throughput of a index as well as a read-focused
benchmark measuring the query performance of a previously ingested index. The
benchmarker itself is inspired by Rally and is in many parts compatible with it. 120Chapter 8. Secure Metadata Management
Therefore, it can even be alongside Rally to provide a more comprehensive
picture.
Ingestion Benchmarker
The ingestion benchmarker measures the ingestion throughput into a index for
each configuration. It uses the Newline Delimited JSON (NDJSON) format which
allows for trivial dataset creation and scaling. Furthermore, it allows for the usage
of all Rally corpora which facilitates the aforementioned interoperability. MPI,
the ingestion benchmarker can be arbitrarily scaled across nodes.
Before the ingestion can be measured, the root node creates the index. For this
a configuration file containing all field mappings can be provided, i.e., each type of
each of the dataset is strictly defined at index creation time. Although ES
allows for dynamic schemas, the benchmarker uses strict type mappings for repro-
ducibility. The type definitions the same ES Domain Specific Language (DSL)
as Rally. The ES index is by default configured with one shard per node but can
be overwritten a command line argument. After index creation, the offsets of
the globally shared NDJSON file containing the dataset are calculated and shared
across all processes which offsetlseekto start at the specified place for each rank.
Then, all worker nodes ingest the data the bulk API in parallel. For this, the of documents per request can be freely configured as a command line argu-
ment. All worker processes are simultaneously starting to ingest data and are then
independently recording the individual timings of each single request. In the end,
all measurements are gathered on the root rank and are streamed into a JSON file for
further analysis. Lastly, in order to isolate the ES performance, caching is explicitly
disabled for the index.
Query Benchmarker
The query benchmarker is also MPI- and measures the documents per second
as well as the latency of each individual request. Inspired by Rally, the benchmarks
are structured into multiple steps a fork-join model. The query benchmarker
has a custom, JSON- DSL for the benchmark design. The DSL embeds the ES
query language internally, allowing for easy development for ES users as well as
trivial portability to Rally tracks. Each request explicitly bypasses the cache, result-
ing in more realistic measurements. Lastly, it supports multiple alternating queries
in a single fork-join task to create a mixed usage load.
For each of the fully disjunct benchmark steps the following workflow is exe-
cuted: First, the root node waits until the ES cluster’s health is green, to ensure the
ES cluster has fully formed. If multiple queries are defined for a given task, each
rank chooses a random permutation. This is done to maximize the randomness of
the request pattern sent to the ES cluster. After that, if configured in the bench-
mark definition, warmup requests are made to fill the page caches and optimize
the Java Virtual Machine (JVM) Just-In-Time (JIT) compiler. If this is not done, the
query performance will increase over time, overshadowing other effects. Once the
warmup is done, each worker starts the benchmark until the configured execution
time is reached. The cache is explicitly disabled therequest_cacheparameter.
Per defaultSession Objectsare to facilitate persistent connections and reuse the
underlying TCP connections. Thereby, additional HTTP overhead is limited which
could otherwise taint the measured data, since handshaking and authentication are
embarrassingly parallel tasks. If a request is successful, the latency and of 8.2. Design121
received documents are saved, otherwise the error HTTP code is recorded. A sleep
between each request can be optionally configured. Lastly, all measurements get
aggregated in the root rank and dumped into a JSON file.
8.2.3 - Encryption
GocryptFS has two limitations. First, it does need a secure environment to be ex-
ecuted in, i.e., it can’t be deployed in any managed hosting. In addition, in order
to spawn a ES instance, the entire storage path has to be encryptable by the
. This means a has either full or none at all. Both of these limitations
can be mitigated an ABE scheme. Here, both the documents to ingest, but
also the queries later on will be encrypted. This allows hosting an ES cluster in an
untrusted environment and also enables to of different keys, not only on a per
document basis but also on a per basis, offering a very fine-grained control. However, performing operations on encrypted data might not be possi-
ble. For instance, a range query implies that the encrypted numeric values maintain
their original ordering. To enable specific numeric operations on ABE documents,
numeric data are encrypted an OPE method on[26]. The proposed ABE
algorithm is on the AES256 and OPE algorithms. While AES256 is for
non-ordered string values OPE is for ordered data types such as integers or
floats to preserve any ordering feature, a range query, or anafterorbeforequery
on a date. Since OPE is only defined on integers, further data type mappings are
required. Thus, the following data type case distinction is done:
•Strings:Strings and keys are encrypted AES256.
•Integer: Integers are mapped from an excepted input range to[0, 2
64
)
OPE.
•Dates:The dateYYYY-MM-DDgets interpreted as the integerYYYYMMDDwhich
will get encrypted as an integer the input range(0, 30000000).
•Date-Time:The timeYYYY-MM-DD hh:mm:ssworks analogously to the date
datatype with the input range of(0, 30000000000000).
•Float:Float to integer conversion inherently introduces loss of precision. Fur-
thermore, in order to provide cryptographically stronger encryptions, the in-
put range should be chosen as small as possible. This provides a tradeoff that
should be handled on a case-by-case basis by letting the specify the re-
quired precision. Therefore, the proposed algorithm takes not only the ex-
pected input range[l,r]but also the step sizesthat describes the mapping’s
granularity by partitioning the input range. Starting atl, the mapping returns
the of steps required until it is equal to the float. Thus, given a float
inputx, the mapping is defined as
m(x):=

x−l
s

The input ranges and step sizes have to be provided by the . While this ensures
the necessary data privacy, this algorithm has certain limitations. Firstly, both the
AES encryption and OPE’s integer mapping increase the document size, resulting
in larger indices. Furthermore, since all ordered types are mapped to integers, data-
specific ES operations such as date aggregations are not supported. Next, due to 122Chapter 8. Secure Metadata Management
the nonlinearity of OPE, relations requiring a fixed distance such as|x−y|<5 are
not possible. Additionally, if different keys are for different numeric attributes,
they cannot be compared anymore. Lastly, in order to only require one key per row,
the same Initialization vector is for all entries.
8.3 Experimental Setup
To determine the general applicability of the designed solutions, they are extensively
benchmarked. In the following, the general experimental setup is described.
8.3.1 Datasets and Queries
Two different datasets are for the benchmarks. The NYC Taxis dataset, on thenyc_taxisRally track as well as a custom -case-driven MRI benchmark
that is developed on extensive requirement engineering with the involved
doctors and researchers.
NYC Taxis
TheNYC Taxisdataset contains all rides that have been performed in yellow taxis in
2015. It is comprised of 165 million entries, resulting in an uncompressed size of 74.3
GiB, which makes it the largest corpus of all tracks provided by Elastic. Moreover,
its Rally track is one of the canonical benchmarks commonly for benchmarking
ES, making the results comparable to previous work. Furthermore, while most Rally
tracks are for tail latency performance regression analysis, NYC taxis are de-
signed for maximum throughput, making it a realistic case for computing.
Custom MRI
For the data model of the MRI benchmark the model as it is defined in 6.4.1
is and extended with a few extra defined keys thebrain age. Since only a
limited of scans can be done within the scope of the project, an artificial data
set needs to be created to simulate a large cohort. For this, the corresponding values
and their distribution are determined and modeled. For instance, thebrain ageof
a patient is modeled as a normal distribution around the patient’s age with a
variance of 5 years. Other examples are theKSpaceresolution, the magnetic field
strength, the Larmor frequency, or the vendor-dependent measurement protocols,
which are all chosen from a range of valid values of the existing scanners. Compared
to the previous NYC taxis documents, these documents are rather large, with at least
61 different keys, although the overall corpus with 1 million documents is rather
small, but realistic considering that each document represents an MRI scan.
8.4 Results
The individual benchmark runs are done in a grid search manner, scaling the cluster
size by increasing the node count from three, over seven to 15, and for each cluster
size increasing the of ingestion or query workers from 1,2,4,8 to 16. This
exhaustive results in a large dataset of which only a limited subset can
be presented and discussed to highlight interesting caveats. The benchmarks are 8.4. Results123
performed on Emmy
2
, double socket nodes with 2 Xeon Platinum 9242 CLX-
AP, with 368 GiB of available RAM. The storage was a Lustre consisting of
2 ES14KX with 500 12TB SAS HDD each and an SFA7700X with 16 1TB SAS SSD
connected to 4 metadata servers. The compute nodes are running CentOS 7, and
Singularity 3.11.0 and ES 8.6.0 are . In all casesxpack.securityis disabled,
there is one shard per ES instance without any replica, and before each individual
benchmark run, the cache is cleared, and the caching of the index within ES is dis-
abled.
8.4.1 Rally Benchmarks
The first benchmarks are executed with Rally on a single node cluster the NYC
taxis corpus. On all three setups, i.e., the baseline, the GoCryptFS, and the ABE
scenario the same throughput of three operations is recorded, which corresponds
to a total maximum of 30,000 documents per second for a simplematch_allquery,
which has a filter that is always true, i.e., just returns any document. The observed
problem is, that Rally throttles itself when the latency starts to increase too much,
or even timeouts occur. This limitation could not be circumvented by configuring
Rally differently. However, it can be assumed that -world cases utilize less
sensitive client/ workloads. To also simulate these and see, what an ES cluster
is really capable of through putting, a benchmark is needed.
8.4.2 Throughput Benchmarking
In this , the proposed throughput benchmarking tool is presented.
Ingestion
To benchmark the performance and scalability of the ingestion process the two cor-
pora, i.e., NYC taxis and the custom MRI dataset, are once fully ingested. To facilitate
the different numbers of workers, or ingestors, the corpus is split into equally
long parts and each worker is solely responsible for ingesting one unique part of the
corpus. Each of these calls ingests 10,000 documents at once, for which the time is
measured and stored within an array in each worker process. When the entire cor-
pus is successfully ingested, all unique timing arrays from all workers are streamed
into a single result file.
Analyzing Single RunsA single benchmark run is evaluated by summing up all
individual timings of a single worker. This yields the total waiting time of a worker
for an ES response. The measured times include the network latency, as well as the
time ES requires to calculate the response. From these timings, the mean and stan-
dard deviation of all workers are calculated. The results are shown for two examples
in 8.1 for a three-node ES cluster with varying workers per node for both the
MRI and the NYC datasets an ethernet interface. Overall, the ingestion time is
reduced from(49±1)s to(14±1)s when going from a single worker per node to
16 workers per node for the MRI case, while only showing a diminishing return
when going from four workers per node to 16. Similarly, the ingestion time of the
NYC benchmark is reduced from(3270±44)s to(590±42)s, however exhibiting a
larger benefit of more workers considering the runtime drop from(1238±46)s for
2
A German 6 PFLOP CPU- system. 124Chapter 8. Secure Metadata Management
four workers to(592±42)s for 16 workers. The resulting scaling factors are 3.6 for
the MRI dataset and 5.5 for the NYC dataset when scaling from 1 worker to 16.
(A) MRI Dataset(B) NYC Dataset
FIGURE8.1: Geometric mean of the ingestion times on a 3 node clus-
ter via ethernet without encryption.
Analyzing Different Cluster SizesTo determine the influence of the cluster size,
the chosen network interconnect, i.e., OPA or Ethernet, and the cost of the encryp-
tion, the single run analysis is done for all possible configurations. This gives a
total of 180 independent measurements, as exemplified in 8.1. To ease the
representation, the of considered measurements is reduced to 36 by only
considering the fastest run depending on the of workers. This would mean,
that for the measurements shown in 8.1 only the runs with 16 workers are
.
The results are shown in 8.2. The first observation is that in all cases the
Ethernet performance is much better than the OPA via Internet Protocol over
Infiniband (IPoIB). The second observation is that generally, GoCryptFS exhibits the
worst performance. This is expected since every operation has to go through the
additional FUSE layer and needs to be transparently encrypted. This is different
from the baseline scenario, where no encryption is employed, and the ABE scenario,
where the encryption is not covered by the measured round-trip time since the en-
cryption is done by the client. This is chosen to measure the scalability of
the server-side NoSQL database setup. If the encryption had been included in the
measurements presented in 8.2, the embarrassingly parallel properties of the
encryption of the individual documents would have overshadowed the effects of the
server-side setup. Therefore, the ABE encryption costs are analyzed in isolation in 8.4.2. The increased ingestion time of ABE compared to the baseline can be
explained by the overall larger dataset, where the one million documents in the MRI case have a cumulated size of 1.5 GB whereas the ABE-MRI dataset has a size of
4.3 GB. Similarly, the size of the approximately 168 million documents for the NYC
taxis data set is 75 GB for the unencrypted dataset and 184 GB for the encrypted.
The resulting speedups are largest for the MRI case and the Ethernet interface
ranging between five to three times and for the OPA interface between two and
three. For the larger NYC dataset speedups for the Ethernet interface are in the range
of one to three and for the OPA interface very homogenously three. Considering that
the observed scalability is much better for the MRI data set than for the larger NYC 8.4. Results125
dataset, it could be that caching effects become more dominating in the MRI dataset.
(A) MRI Dataset(B) NYC Dataset
FIGURE8.2: Ingestion time for varying cluster sizes for Ethernet and
Omnipath interconnects. The scaling factor is calculated the
ethernet- baseline measurement.
Measuring the ABE Encryption Cost
In order to measure the encryption cost of the ABE method, 10 times a random 10,000
document subset is picked from the NYC taxis and the custom MRI corpus. Then the
encryption operation is measured end-to-end within the encryption itself,
therefore excluding other effects the I/O overhead when reading the data from
the filesystem. From these measurements, the expectation value and the standard
deviation are calculated. This yields:
• NYC: (0.456±0.001) s per document
• MRI: (0.079±0.002) s per document
These results are extraordinarily slow because a very inefficient OPE implementa-
tion is , i.e.,pyope
3
, which implements the encryption as a recursive in
Pythonwithout further optimizations. This is also the reason, why an NYC docu-
ment takes so much longer than an MRI document. Since in the NYC dataset a large
amount of numbers are , the ABE algorithm has to be more often than in
the MRI dataset, where more is , which is more efficiently processed with
an AES256 encryption.
Match All Queries
To establish a baseline, theMatch_Allquery is . This query just matches all
documents. Thus, no complicated logic has to be done by ES and it can just stream
the requested documents as soon as it gets the request for it. However, other effects
can outweigh this simple model. For instance, ES dynamically allocates threads
to execute and manage requests. On an unoptimized index with a high segment the overhead of creating too many tasks to be executed on these segments
can diminish the advantage of applying no logic within the filtering of documents.
3
https://github.com/tonyo/pyope 126Chapter 8. Secure Metadata Management
(A) One worker per node with a return size of 10
documents per response with a 5ms steps.
(B) 16 workers per node with a return size of 10k
documents per response with 100ms steps.
FIGURE8.3: Histogram of the of requests with regard to the
full round-trip latency for the 7-node ethernet cluster the NYC
dataset.
Thus, theMatch_Allquery is not necessarily the very baseline query, but it is useful
for either server configuration fine-tuning or network overhead optimizations.
Since the query benchmarker records for each single request the latency as well
as the of returned documents, one can do a fine-grained analysis of the dis-
tribution of the measured latencies. These are shown for two differentMatch_All
workloads in 8.3. One can see in both cases that the distributions of the mea-
sured latencies for the individual requests follow some kind of super-exponential
decay. However, for the less intensive workload with only one worker per node,
shown in 8.3a, it peaks strongly around 50 ms and only has very few tail la-
tencies around 500 ms whereas the more intense benchmark configuration with 112
workers in total and a response size of 10,000 documents peaks around 80 ms and
shows a constant super-exponential decay to 5 s. These two measurements demon-
strate how the ES cluster continuously gives in to an increasing load, which results
in larger latencies.
To further analyze how these distributions are measured, an exact timeline is
measured and shown in 8.4 and 8.5 for the measurement of the seven-
node cluster for the NYC baseline measurement. One can see that in 8.4a for
the request size of 10 documents and one worker per node a striping pattern is visi-
ble during the entire benchmark run. These could be caused due to beating patterns
within the request frequency. Increasing the response size from 10 to 10,000 docu-
ments as shown in 8.4b leads to two distinct bands. This can be explained that
here some workers are randomly assigned to the localhost. The reduced network
overhead then leads to lower latencies. Due to the one thousand times increased re-
sponse size, this network overhead becomes increasingly relevant and measurable.
When further increasing the workers to four per node as shown in 8.5a, one
can see the bands become blurred until they are indistinguishable in 8.5b.
Increasing the of workers leads to a much larger dispersion and an overall
increased latency.
Since always a super-exponential decay is observed in 8.3 with a mixed
dispersion over time, the geometric mean is to average over an entire measure-
ment to obtain the average latency for a single request. The mixed dispersion should
emphasize, that depending on the exact query and load beating patterns, bands, or 8.4. Results127
(A) Return size of 10 documents per response.(B) Return size of 10k documents per response.
FIGURE8.4: Timeline of the individual request latency for the seven-
node NYC baseline benchmark with one worker per node.
(A) Four workers per node.(B) 16 workers per node.
FIGURE8.5: Timeline of the individual request latency for the seven-
node NYC baseline benchmark with 10k documents response size.
a homogenously decreasing dispersion can be observed. All of these different dis-
tributions would require different modeling, making comparisons between them
nearly impossible. Therefore, the geometric mean is chosen to be a simple, easily
comprehensible but fair measure for the general tendency for the measured laten-
cies, while somehow acknowledging the probably rather multiplicative nature of
the data than an additive. Dividing the of documents per request by this
averaged latency yields the averaged documents per second, as can be seen in Fig-
ure 8.6b for the seven-node cluster running the NYC baseline benchmark. One can
see, that by increasing the of documents per response the overall through-
put increases as well. Similarly, the overall throughput is increased by utilizing more
workers to increase the of parallel requests. In order to determine the en-
cryption cost and the advantages of larger clusters, similar to the analysis of the
ingestion, only the best-performing values are taken into account for each cluster
configuration. Through this , the individual optimum can be , thus
optimally representing a specific server configuration while still reducing the 360 in-
dividual results to only 18 which can be more easily comprehended. That means for
the measurements in 8.6b only the one with 16 workers per node is selected.
The result is shown in 8.6a. One can see that with increasing cluster sizes
the throughput decreases. The reason for this behavior can be that in order to create 128Chapter 8. Secure Metadata Management
(A) Geometric mean of the documents per second
for the best-performing worker configuration for
a match_all query.
(B) Throughput for a seven-node cluster baseline
for the NYC dataset.
FIGURE8.6: Geometric means of different Match All query measure-
ments.
Baseline [s]GoCryptFS [s]
Cluster-
Size
EthernetOPAEthernetOPA
gmean
1st99th
gmean
1st99th
gmean
1st99th
gmean
1st99th
37.46.865.58.57.957.77.16.919.27.16.916.3
72.92.93.73.53.43.62.92.93.13.02.93.6
151.61.61.71.61.61.81.41.41.51.61.61.8
Speedup4.64.65.14.4
TABLE8.1: Geometric means and the first and 99th percentile of the
latencies of a histogram aggregation.
the response, the requested ES node needs to communicate with all the other nodes
which are also holding an active data shard within the index. Since this benchmark
is configured in such a way, that every ES node is an active data node, every node
has to communicate with every other node. This adds a lot of overhead but yields
no parallelization advantage since the request does not contain much logic that has
to be processed.
NYC Aggregations
Analyzing theMatch_Allqueries has demonstrated potential drawbacks to the per-
formance when scaling out. However, it can be expected that positive effects can
be observed when the workload required on the node to fulfill the request is in-
creased. Such a more compute-bound query is a histogram aggregation within the
NYC data set, where first all trips with a distance of greater than zero and smaller
than 50 (miles) are selected and then aggregated into histograms according to their
trip distance and cost. The result is shown in Table 8.1. One can see, that for the
baseline as well as for the GoCryptFS scenario the latency drops from around 8 s per
request for a three-node ES cluster, to 3 s for a seven-node cluster and to 1.6 s for a
15-node cluster. Not only can almost linear speedups be observed, but particularly
the dispersion decreases drastically, since the first percentile, the geometric mean,
and the 99th percentile are much closer for larger clusters. The speedup shown in
Table 8.1 refers to the speedup of the geometric mean of the three-node cluster to the
15-node cluster. Only this value is discussed, to motivate the average speedup that
can be expected, when scaling the cluster size for this workload. A speedup value of
five when scaling from three to 15 nodes corresponds to linear scalability. Therefore, 8.4. Results129
for the baseline scenario, 92% efficiency is obtained when scaling out, while for the
GoCryptFS OPA setup 88% efficiency is achieved. The observed performance ben-
efit of GoCryptFS could be due to additional page-caching introduced by the FUSE
layer or it could be due to measurement uncertainties since the variation would also
allow the actual performance of GoCryptFS to be slightly lower than the baseline.
Therefore, the additional encryption cost when doing compute-intensive operations
is negligible.
MRI Custom Benchmark on previously made experience with the standard NYC dataset, the presented
benchmarking tool is to run a custom MRI benchmark, which model is on the in 6.4.1 developed data model with a few extra attributes thebrain
age. For this, the following set of queries is defined together with researchers who
want to the data catalog in the future:
•Body Part:Atermquery for a body part, in this case, the head. Corresponds
to "return all MRI scans of the head".
•Systemvendor:Amatch_phrasequery for the Systemvendor, e.g., Siemens,
Philips, or General Electric. Corresponds to "return all MRI scans from a spe-
cific system vendor".
•Age:Arangequery to select an age range of patients. Corresponds to "return
all MRI scans of patients with an age between 50 and 60 years".
•Body Part and Resolution:Boolean-chainedtermqueries selecting a body part
and two resolution parameters. Corresponds to "return all MRI scans of the
head with a resolution, i.e.,KspacePhaseEncof 256 times 256".
•Brain-Age:Afilterwhich is on a script included in the query which
determines if the brain age is larger than 5 years than the patient age.
•Vendor and Body Part:Boolean-chainedtermandmatchqueries to query a
body part and then match it against a specific system vendor and protocol.
Corresponds to "return all MRI scans of heads done on either a Siemens scan-
ner with an MP2RAGE or a General Electric scanner with a BRAVO sequence".
•Age-Weight Distribution:A chainedmatchquery for the patient gender, fol-
lowed by arangequery of the age with an ending histogram aggregation. Cor-
responds to "select all men with an age lower than 70 and do a histogram of
the patient weight with a binning size of 5".
To further determine the ideal setup, the same grid search as for the NYC dataset
is done. In Table 8.2 the maximum throughput for documents per second is de-
termined for each cluster size and query. To simplify the comparison between the
queries, only the geometric mean is provided as a measure of general tendency, and
the standard deviation is provided to provide some kind of measure of dispersion.
This is done, since the additive nature allows for a very compact, easily understand-
able, and comparable notation, as it is shown in Table 8.2. That this statistical anal-
ysis provides only a limited representation is extensively discussed in 8.4.2.
However, for an overview, the proposed, compact notation seems more useful.
For this, each run is individually analyzed similarly as shown in 8.6b to
determine the ideal setup for each cluster size and query. One can see in Table 8.2 130Chapter 8. Secure Metadata Management
Setup
Cluster-
Size
Body
Part
System-
vendor
Age
Body Part
Resolution
Brain-Age
Vendor+
Body P.
Age-W.
Distr.
3 Node190k±15k180k±13k190k±15k124k±6k94k±5k96k±5k4k±160
Baseline7 Node212k±10k244k±4k103k±5k114k±6k101k±4k166k±4k12k±700
15 Node39k±9k37k±4k49k±4k48k±34k37k±4k72k±5k5k±2k
3 Node180k±13k157k±10k176k±13k119k±4k170k±13k153k±5k5k±100
GoCryptFS7 Node71k±9k100k±12k51k±4k89k±7k50k±5k103k±4k6k±480
15 Node29k±6k33k±3k49k±3k41k±19k39k±3k60k±4k4k±670
3 Node120k±15k110k±11k160k±6k90k±80k-140k±7k-
ABE7 Node80k±20k80k±7k80k±7k110k±20k-95k±7k-
15 Node6k±8k10k±8k12k±8k11k±9k-12k±8k-
TABLE8.2: Documents per second for the MRI case with the Eth-
ernet interface.
Setup
Cluster-
Size
Body
Part
System
vendor
Age
Body Part
Resolution
Brain-Age
Vendor and
Body Part
Age-Weight
Distribution
30.6±0.10.7±0.20.6±0.10.02±0.020.6±0.10.7±0.10.020±0.005
Baseline70.7±0.10.7±0.10.7±0.10.02±0.020.7±0.10.5±0.10.013±0.003
150.9±0.30.9±0.20.7±0.20.03±0.050.9±0.20.5±0.10.009±0.004
30.6±0.10.6±0.10.6±0.10.02±0.010.6±0.10.5±0.10.022±0.004
GoCryptFS70.7±0.10.7±0.10.7±0.10.02±0.020.7±0.10.5±0.10.013±0.003
151.0±0.30.9±0.30.8±0.20.03±0.050.9±0.30.5±0.10.010±0.005
31±0.21.2±0.31.2±0.30.04±0.001-1.1±0.2-
ABE71.4±0.61.4±0.61.4±0.60.04±0.001-1.2±0.2-
152.5±3.72.6±3.61.8±2.50.05±0.09-1.2±1.3-
TABLE8.3: Latencies for the MRI custom case with the Ethernet
interface in seconds.
that for the baseline scenario, a seven-node cluster performs best for most cases, with
only the Age and the Body Part and Resolution query peaking for the three-node
cluster. For the Body Part, Body Part and Resolution, and the Brain-Age queries, the
results lie within the error, so no direct conclusion is possible.
While for the GoCryptFS the three-node cluster performs best, except for the
Age-Weight Distribution query which peaks for the seven-node cluster. Generally,
all queries performed slightly worse in the GoCryptFS setup compared to the base-
line, for individual performance drops ranging from a 5% to 13%, when comparing
the two three-node setups. The observation, that the Brain-Age and the Vendor and
Body Part are faster on the GoCryptFS setup is not clear beyond any doubt but could
be explained by a temporary noisy-neighbor effect. The very limited effect of scaling
from three to seven nodes can be explained by the smaller dataset size compared to
the NYC corpus. Here, only a million documents, each representing a single MRI
scan, are , while the NYC taxis dataset has more than 165 million documents.
Therefore, the additional network overhead of the intra-ES cluster communication
and thread management can not be outweighed by large parallelization gains.
To further analyze the advantages or disadvantages of scaling out, the latencies
for single requests are analyzed in Table 8.3. One can see that for all queries except
the Age-Weight Distribution the latencies increase or stay at least constant for larger
cluster sizes. The Age-Weight Distribution seems to profit from a larger paralleliza-
tion for the decrease of a single request latency. The reason, that the overall latency
of the Age-Weight Distribution is very small compared to the others, excluding the
Body Part and Resolution query, is that generally 10.000 documents are returned per
response. Thus the overall payload of the response of the Age-Weight Distribution
is three orders of magnitude smaller compared to the others, drastically reducing
the network overhead which is included in all presented benchmarks. 8.5. Evaluation131
8.5 Evaluation
Two different encryption techniques are presented which can be to provide
secure metadata indexing functionalities for sensitive information. Both of these
techniques can be within the modular design of the data lake to provide the
required metadata management. While ABE is implemented purely on the client
side, it can not only be in high-throughput scenarios on systems but also
within (public) cloud environments, while the described GoCryptFS- method the SecureHPC platform cannot be deployed outside of this scope. However,
while the presented ABE method currently has a few shortcomings when equally
spaced distances are required, the GoCryptFS method provides the full capabilities
of the underlying database. One interesting result from the benchmark the
synthetic MRI dataset is that under the condition of 1 million documents, i.e., MRI
scans, higher throughput cannot simply be achieved by scaling out. Therefore, it
needs to be carefully verified, that the recorded throughputs and latencies are suf-
ficient. If not, further node- optimizations should be done, which can also be
tested the presented benchmark. Within the "Big Data MRI"-project, only less
than 200 scans have been processed the developed data lake. For this case
the achieved performance is sufficient and considering the timings of the synthetic
benchmark no further actions seem necessary, since both the throughput and laten-
cies are sufficient to still cover this case.
8.6 Summary
In conclusion, a workflow to deploy on-demand, -owned ES clusters for sen-
sitive data on systems is presented, which allows for dynamically scalable,
workload-defined cluster sizes. Two different encryption techniques are
implemented and are systematically benchmarked with a novel tool that can be to create very aggressive throughput-oriented workloads, as well as latency-
sensitive benchmarks. The importance of accurate data and workload modeling is
demonstrated, showing that for simple queries even on large datasets consisting
of more than 165 million documents, larger ES cluster can decrease performance
due to additional network and thread management overhead, while more compute-
intensive operations can largely benefit from larger clusters, as is demonstrated for
the histogram aggregations. The presented method, ranging from the data and
workload modeling to the analysis of the benchmark is a blueprint that can be sim-
ilarly applied for other cases as well. The interpretation of the gathered results
relies on the determined requirements of the users. Combining an ES cluster with
one of the proposed encryption techniques can then be as a secure and scalable
backend for a data catalog. By generalizing the modeled workload, one can provide
a first template for more abstract and case-specific data catalog functionalities.
To integrate the proposed workflows into the overarching data lake architecture,
one needs to include a reference to the index within the (F)DO record, including
machine-actionable instructions on how to get to the metadata and the corre-
sponding keys. 133
Chapter 9
Conclusion and Future Work
In this chapter, a short summary is provided in 9.1, highlighting the main
contributions of this work. Afterward, an outlook for future work is provided in 9.2, alongside a discussion of corresponding improvements of the presented
work.
9.1 Conclusion
In the following, the main contributions of this thesis are highlighted by summa-
rizing how the individual contributions answer the initial research questions intro-
duced in 1.2. For this, first, the more detailed sub-questions are addressed,
before everything is set into perspective when focusing on the overarching goal of
this thesis.
9.1.1 Data Lake Architecture
What data lake architecture can be to organize a flat-hierarchical data lake
with respect to data maturity and offered functionality that is built in a modu-
larized manner scalable components?
Within the initial requirement analysis in Chapter 2 a set of requirements are identi-
fied. Two of these key aspects are generality and modularity, which should ensure
that specific systems can be included in the future to provide functionalities
when extending beyond the original scope or that components can be exchanged
due to technological progress. In order to enable drop-in replacements of backend
services, which the users would not necessarily recognize, one needs to abstract the
complexity of these utilized systems. Within an extensive literature survey, different
data lake architectures are discussed in 3.3, where the current state-of-the-art
of abstracting on systems is discovered and the resulting disadvantages a high complexity for users and admins are motivated. Therefore, a novel data lake
design is proposed which uses (F)DOs to organize the otherwise flat namespace.
The type of the (F)DOs can be to infer an order indicating the data maturity,
or in the case of tasks, the functionality. The proposed data lake design is therefore
one concrete implementation of the class of hybrid architectures [167]. Hereby the
(F)DOs provide a homogeneous interface to users by the data lake frontend or as
(F)DO records of a reference service. to the underlying services is managed
by functions which are hiding the complexity of the actual implementations from
the end users. 134Chapter 9. Conclusion and Future Work
9.1.2 Scientific Support
How can this data lake architecture be made FAIR by design, allowing for the
integration of diverse scientific workflows and enforcing overarching data man-
agement plans across the lake?
Within the gap analysis in 3.7.5, it is identified that there is a general lack of
ecosystem-agnostic compute capabilities in existing data lakes. These purpose-built
data lakes can therefore not easily integrate scientific workflows that rely, for
instance, on systems. Since there are in particular no data lakes available that
support offloading of compute-intensive tasks to systems, the general chal-
lenges of doing data-intensive research on systems are analyzed in Chapter 4.
The proposed governance-centric interaction paradigm relies on the creation of an
experimental description in advance, detailing the input and output data sets, and
the tasks which process them. These constituents of the overarching DMP can be
considered to be functions, which are well-defined tasks, within well-defined en-
vironments. Executing pre-defined functions is also very common within clouds
following the FaaS computing model. Thus, this computing idiom is ideally suited
to provide a homogenous interface to both cloud and infrastructure. For this,
a REST- FaaS interface for systems is developed in Chapter 5.
Since the general computing paradigm revolves around the fundamental idea of
executing functions, the previously proposed DO- data lake architecture can
be extended to FAIR DOs. FDOs are designed in such a way, that they fulfill
the FAIR principles. Their definition extends these of DOs, and also includes the no-
tion of operations. These are functions, that can be executed with the corresponding
FDO as input data. Thus, FDOs as the central building block allows scien-
tists to work FAIR, while the generic, -centric processing model provides a
convenient interface for diverse compute infrastructure. Since a DMP consists of a
structured workflow connecting input/output data to their processes, they can be
defined on the data lake level, to ensure global consistency.
9.1.3 Secure Processing of Sensitive Data
What are the attack vectors to gain to a ’s data while processing them
and how can trusted and secure compute capacity be provided which ensures the
full data sovereignty to the users?
Since the proposed data lake focuses on integrating systems to offload compute-
intensive tasks, only these are focused on providing secure processing capabilities.
This platform should be to process sensitive health data, which is one of the key
requirements gathered in Chapter 2. The gap analysis in 3.7.5 also revealed
a negligence of existing data lakes on this topic. Therefore, a novel service called Se-
cureHPC is proposed in Chapter 7, which is designed around the basic assumption
that users on systems have bare-metal and the available software stack
of these machines has vulnerabilities. Thus, to provide a secure computing platform,
it needs to be isolated from an attacker with administrative privileges. For this, a se-
curity onion is utilized to provide an isolated partition on a shared system,
which fully restricts the lateral movement of an attacker to this partition. to
this partition is only possible a special workflow, which ensures full end-to-
end encryption as well as the legitimacy of the compute request by confirming the
authenticity with a second factor. This workflow can then execute pre-configured
batch jobs, which completely integrates with the execution model of asynchronous 9.2. Future Work135
functions, which can be triggered on systems a common SSH connection,
or the in Chapter 5 proposed HPCSerA service. this FaaS interface, it can be
homogenously integrated into the proposed (F)DO- data lake architecture.
9.1.4 Secure Metadata Handling of Sensitive Data
What are the attack vectors to gain to data stored in a database or search
engine and how can a secure platform be provided that ensures the full data
sovereignty to the users?
A data catalog, containing semantically meaningful information about the data and
their linkage, is a key component of a data lake. Within the proposed (F)DO-
data lake architecture, every (F)DO has to be accompanied by describing metadata,
or a reference to it. To provide a secure and highly scalable data catalog, capable of
indexing sensitive, health-related metadata, two different methods are proposed in
Chapter 8. One uses client-side encryption of the metadata before they are ingested,
thus it can be for data catalogs hosted on untrusted systems. The second option
utilizes the previously developed SecureHPC service as a platform, on which the ES
clusters and the specific operations on them are executed. This relies on server-side
encryption. Both techniques guard against an unsecured REST interface of the ES
clusters, as well as against direct storage by attackers.
9.1.5 Summary
How can a data lake be designed so that it can serve different scientific domains
as the central data repository and processing platform, with a specific focus on
sensitive data?
In conclusion, a generic data lake is proposed that models data as (F)DOs and pro-
vides a homogeneous interface through its reference services, i.e., the data lake front
end. FDOs ensures transparently that scientists work FAIR and introduces
the notion of operations that can be executed on (F)DOs. This interaction paradigm
matches ideally with the FaaS idiom. By providing a FaaS interface for systems,
this method can be to offload compute tasks to cloud or systems and thus
provide ecosystem-agnostic compute capabilities. Additionally, since the complex-
ity of the underlying services is abstracted by the data lake frontend, a modular
design is established which allows to add or switch backend services required for
the integration of scientific workflows. Additionally, this also enables
seamless integration of the secure compute and metadata platforms that are pro-
posed within this thesis. theDecomposition and Encapsulationand theProblem
Generalizationsteps of the employed scientific method, three services are successfully
spun out of the overarching data lake and are available as stand-alone components
outside the scope of the data lake. Their successful deployment is described, includ-
ing their deployment in clinical workflows. Thus, the employed scientific method
has successfully maximized the overall impact as it is illustrated by the different cases outside of the original scope.
9.2 Future Work
The current state of the presented research can be further extended, to either extend
the capabilities of the services, mitigate shortcomings of the current state, or adapt 136Chapter 9. Conclusion and Future Work
these research artifacts to novel technologies. These opportunities are discussed in
the following.
9.2.1 Integration of Data Management and Compute Resources
The proposedGovernance-centricinteraction paradigm depends on a DMP tool that
can process the information flow originating from the processing tasks. This tool
has only been theorized within this work but has not been prototyped, which would
be the first step towards a viable service. Since the DMP tool depends on an ex-
perimental description of the users’ workflow, it is favorable to integrate it into the
compute time proposal, as it is common on larger, i.e., Tier-2 clusters and upwards.
This integration should be done in a meaningful way so that it will not appear as an
additional , and burden the researchers even more, but that it can ease and
streamline the overall process by reducing the work at other places. One such point
could be the proof of scalability. Here, it would be enough if users could demon-
strate that they could utilize at least one node, and achieve any further paralleliza-
tion through data-parallelism. However, simply dropping these sections in favor of
a machine-readable DMP might impede the job of the reviewers evaluating these
proposals for their applicability to get compute time.
9.2.2 HPCSerA
The presented REST service for remote is focused on the general, FaaS- interaction paradigm, alongside an extensive security architecture. Within this
work, it is assumed, that the functions a wants to execute are already preconfig-
ured. Therefore, an efficient way of providing these functions is still required. One
option would be to build up a repository of functions that users can import. This can
work even entirely through the REST API of HPCSerA, where the agent can import
these functions the code-ingestion endpoint, which is secured with a second
factor.
The general idea of the code-ingestion endpoint can also be extended to pro-
vide better ad-hoc processing support. This requires either an extension of the FaaS-
paradigm, to also cover the execution of unconfigured functions, or an extension of
the security architecture if code is dynamically ingested via streams. The first ap-
proach is more in line with the general idea of HPCSerA. An initial could
be to provide functions that can accept code as arguments. One can then make a call for each code snippet that one wants to execute. In the current secu-
rity design, this would require for each execution an acknowledgment by
the within HPCSerA. This workflow needs to be adapted to provide a simi-
lar security level while providing more -friendliness, e.g., by only requiring an
acknowledgment once within a fixed time frame.
In the current setup, the execution of the agent is triggered by a cron job, or a
systemd timer unit. For time-critical executions, this lag is too large. Instead, the
execution of the agent needs to be triggered with the actual call to the API
server. There are generally different ways to do it. For instance, the agent can be long
polling on the endpoint from which it accepts incoming functions. Another option
would be to have a dedicated on the system with an SSH force command,
which is allowed tosudoto other users and only execute the agent there. This is then remotely accessed via the API server to trigger the agent on the system
within the space of the requesting . 9.2. Future Work137
Lastly, data transfers are only handled for small files, while for larger data trans-
fers more specialized services S3 are . To increase efficiency and -
friendliness, the boundaries of small and large data transfers need to be system-
atically benchmarked, and a tighter integration of S3 into HPCSerA is envisioned
where ideally users do not need to set their secret and keys but can their
HPCSerA credentials to configure to their S3-bucket.
9.2.3 Data Lake Design
The implementation of the presented data lake design is currently only usable with
the abstraction of DOs in a programmatic way. This means, that there is no web
interface available. Instead, interactions with the data lake and the DOs within are
currently done via a Python- Software Development Kit (SDK). Simply devel-
oping a web UI on top of this SDK is not sufficient, since specifically with graphical
interfaces, domain-specific nuances are a main concern. For this, it is important to
find patterns to standardize the process to map this generic, DO- data lake de-
sign on domain-specific cases and their graphical representations. In addition,
the communication protocol currently should be substituted or extended to the DOIP once it is available to provide true FDOs. This will also ensure full machine
actionability and will provide interoperability with other services and frontends,
which can then provide the -specific web UIs with the required domain-specific
refinements.
While the current design aims to be as flexible, and therewith as powerful as
possible, this also entails that a deep knowledge of the system and its concepts is
required to be able to quickly set up pipelines and reuse existing infrastructure.
Here, further research is necessary to define a restricted subset of functionalities,
which allows untrained users to quickly set up workflows to the data lake with
a basic set of features, and provide this in easily accessible interfaces.
The data lake holds data during the entire lifecycle. This means that it contains
cold, warm, and hot data. Therefore, different storage tiers are required to provide
either the required performance or the durability. During the application within the
"Big Data MRI"-project, this is solved by including an explicit data staging step, i.e.,
copying the data to a fast filesystem before processing it on the system. -
though this works fine for this specific case, in other cases a more dynamic
data from the system to the cold storage tier might be required. This can
lead to a situation where many, even hundreds of compute nodes of the system
need to the same data on the cold storage tier. This will then bottleneck either
on the file system, or network congestion will limit the overall throughput, leaving
the compute nodes idling while waiting for the data. To solve this problem, a more
efficient way of loading data is required, where instead of loading the same datan
times fromnnodes, it is only read one time from the remote storage and then dis-
tributed within the system the high-performance interconnect. One way
to transparently do this is through a FUSE, which uses MPI to synchronize file .
For example, one process could load a file and then broadcast it to all other processes
on all participating nodes. Similarly, one-sided communication would enable
other nodes and processes to fetch missing data from a participating node, in-
stead of loading the data from the remote filesystem. Providing these functionalities
via a FUSE mount allows the application to a common file descriptor without
any knowledge of the underlying filesystem, including any MPI communication. A
prototype of such a FUSE mount is already implemented within this work but is not
explicitly discussed within this thesis. 138Chapter 9. Conclusion and Future Work
Within the presented data lake, encryption is to securely store data and
metadata. However, currently, the key management is entirely up to the and
is not discussed at all. The presented KMS within the secure partition and the
secure metadata handling are only as a communication layer where keys are
only retained for a short period of time. Here, a proper KMS needs to be integrated
on the client side to manage the keys over the entire lifecycle. Here, further research
can be done particularly with regard to a suitable granularity. Encrypting each file
with a unique key is technically possible, however, when a multitude of different
files should be processed within a single job, this introduces a large overhead. On
the other hand, a finer granularity can limit the extent of a security breach and can
be for additional, decentralized control.
9.2.4 SecureHPC
For the IO500 measurements of the multinode setup presented in Table 7.6 theior-
hardoperations have to be excluded, since neithereCryptFSnorGoCryptFShad sup-
port for parallel IO, meaning, that multiple processes from multiple nodes write to
a single file. Following this discovery, this problem is fixed inGoCryptFSby us-
ing non-POSIXOpen File Description Locksusing theF_OFD_SETLKWcommand. These
provide byte-range locks which are then by eachGoCryptFSprocess to lock ac-
cess to one dedicated byte-range within the file. These modifications are successfully
deployed on the clusters of GWDG. However, further research can be done with re-
spect to the overhead cost of this lock. In addition, currently, all processes are
always calculating the full block cipher when writing into overlapping byte ranges.
Here, further optimizations,e.g., by collective I/O can be researched.
The current implementation only allows one group on a single dedicated
server. That is because it can currently not be reliably detected or prevented if a makes an attack, e.g., a malicious change to OS. Therefore, changing the group that can this isolated server requires a reboot of the server to unlock
the second factor again and allow a different group to it. In addition, a
dedicated partition needs to be provided for each group, or the Slurm state has
to be locked to prevent non-eligible jobs from trying to run on this server. These
two limitations can be circumvented in future work by providing additional hard-
ening and a fine-granular isolation of the jobs. One direction to research is to start
jobs within an isolated runtime, instead of running them directly on the host. Such
runtime environments can be lightweight virtual machines likeKata Containers[164]
which provide additional isolation of the processes running within this container
from the host OS compared to other runtimes likeSingularity.
Another to increase the trust of the secure server is to remote attes-
tation of its state between different jobs runtime integrity monitoring. Here, all
files are assigned a hash value, which is evaluated upon the file . By combin-
ing this with a Trusted Platform Module, the measured state defined by the chaining
of all hashes of all accessed files can be securely maintained and is even shielded
from manipulation of a local root. These two measures are completely orthogonal
and can therefore be efficiently combined.
Lastly, TEEs are constantly evolving and might be a suitable alternative in the
future. For example, Intels Trust Domain Extension is not only capable of isolating
an entire VM from the host, instead of only limited commands within the SGX en-
vironment, but it can also provide GPU passthrough. These efforts are however in a
very early state. Currently, it is only possible with NVIDIAs H100 without runtime
encryption for its on-card memory. 9.2. Future Work139
9.2.5 Secure Metadata Management
The developed benchmark and its cases can in further work be to compare
the capabilities and performance of the presented on-demand ES clusters which are
spawned in the context of normal Slurm allocation, with other databases, or filesys-
tem services with search capabilities likepixstor
1
. Here, the presented measurements
can be as a baseline to better assess the performance of such storage systems.
To make this service more accessible for a wide range of users, a basic set of
functions needs to be defined which can be wrapped into a simpler command line
interface. This requires further research of diverse cases that integrate such a
data catalog service and then finding common patterns in their interactions with the
data catalog. Similarly, the interaction of the proposed DMP tool in Chapter 4 with
the data catalog needs to be defined on a technical level to provide a well-defined
interface for these two independent services to work as smoothly in conjunction as
conceptually envisioned.
Lastly, further improvements with the presented ABE method are desirable. For
instance, a highly optimized implementation of the OPE algorithm [26] can
drastically reduce the encryption overhead. Similarly, the applicability of order-
preserving versus order-revealing algorithms can be further analyzed. Lastly, ef-
ficient, client-side algorithms can be researched to substitute for fixed-range queries, histogram aggregations or deviations, as it is presented with respect to the brain
age.
1
https://pixitmedia.com/products/pixstor 141
Publications
• Sven Bingert, Christian Köhler, Hendrik Nolte, Alamgir Waqar “An API to
include resources in workflow systems”. In: INFOCOMP 2021: The
Eleventh International Conference on Advanced Communications and Com-
putation. 2021, . 15–20
• Philipp Wieder and Hendrik Nolte. “Toward data lakes as central building
blocks for data management and analysis”. In: Frontiers in Big Data 5 (2022)
• Hendrik Nolte and Philipp Wieder. “Realising data-centric scientific work-
flows with provenance-capturing on data lakes”. In: Data Intelligence 4.2
(2022), . 426–438
• Mohammed Hossein Biniaz, Sven Bingert, Christian Köhler, Hendrik Nolte,
Julian Kunkel “Secure Authorization for RESTful ”. In: INFO-
COMP 2022, The Twelfth International Conference on Advanced Communica-
tions and Computation. Ed. by Claus-Peter Rückemann. 2022, . 12–17
• Christian Köhler, Mohammed Hossein Biniaz, Sven Bingert, Hendrik Nolte,
Julian Kunkel “Secure Authorization for RESTful with FaaS Sup-
port”. In: International Journal on Advances in Security 3 and 4 (2022). Ed. by
Claus-Peter Rückemann, . 119–131
• Hendrik Nolte, Simon Sarmiento, Tim Ehlers, Julian Kunkel “A Secure Work-
flow for Shared Systems”. In: 22nd International Symposium on Cluster,
Cloud and Internet Computing (CCGrid). 2022.
• Hendrik Nolte and Julian Kunkel. “Governance-Centric Paradigm: Overcom-
ing the Information Gap between Users and Systems by Enforcing Data Man-
agement Plans on -Systems”. In: INFOCOMP 2023, The Thirteenth Inter-
national Conference on Advanced Communications and Computation. Ed. by
Claus-Peter Rückemann. 2023, . 13–20
• Hendrik Nolte, Nicolai Spicher, Andrew Russel, Tim Ehlers, Sebastian Krey,
Dagmar Krefting, Julian Kunkel “Secure : A workflow providing a secure
partition on an system”. In: Future Generation Computer Systems 141
(2023), . 677–691
• Vitali Telezki, Henrik tom Wörden, Florian Spreckelsen; Hendrik Nolte, Julian
Kunkel, Ulrich Parlitz, Stefan Luther, Martin Uecker, Mathias Bähr, “Deploy-
ment of an -Accelerated Research Data Management System: Exemplary
Workflow in HeartAndBrain Study”. In: Workshop Biosignals. 2024.
• Submitted/Accepted: Hendrik Nolte, Lars Quentin, and Julian Kunkel. “Se-
cure Elasticsearch Clusters on Systems for Sensitive Data”. In: HIGH
PERFORMANCE COMPUTING: ISC High Performance 2024 International
Workshops. Springer. 2024 142Chapter 9. Conclusion and Future Work
• Submitted/Accepted: Hendrik Nolte, Philip Langer, Julian Kunkel. "Automa-
tisierte Analysen von MRT-Bildern". In: KI in der Projektwirtschaft Band 2
• Submitted: Hendrik Nolte, Lars Quentin, and Julian Kunkel. "Comparing Dif-
ferent Encryption Workflows for Secure Elasticsearch Clusters on Shared Systems for Sensitive Data". In: Journal of High-Performance Storage 143
Bibliography
[1] Georgii Maksimovich Adelson-Velskii and Evgenii Mikhailovich Landis. “An
algorithm for organization of information ”. In:Doklady Akademii Nauk
. Vol. 146. 2. Russian Academy of Sciences. 1962, . 263–266.
[2] Mohammad Ahmadian . “SecureNoSQL: An for secure search
of encrypted NoSQL databases in the public cloud”. In:International Journal
of Information Management37.2 (2017), . 63–74.
[3] Peter Amstutz . “Common workflow language, v1. 0”. In:Figshare(2016).
[4] Michael Armbrust . “Delta lake: high-performance ACID table storage
over cloud object stores”. In:Proceedings of the VLDB Endowment13.12 (2020),
. 3411–3424.
[5] Michael Armbrust . “Lakehouse: a generation of open platforms that
unify data warehousing and advanced analytics”. In:Proceedings of CIDR.
2021.
[6] Michael Armbrust . “Spark sql: Relational data processing in spark”. In:
Proceedings of the 2015 ACM SIGMOD international conference on management
of data. 2015, . 1383–1394.
[7] Sergei Arnautov . “{SCONE}: Secure linux containers with intel{SGX}”.
In:12th{USENIX}Symposium on Operating Systems Design and Implementation
({OSDI}16). 2016, . 689–703.
[8] Monya Baker. “Reproducibility crisis”. In:Nature533.26 (2016), . 353–66.
[9] Anatoliy Batyuk and Volodymyr Voityshyn. “Apache storm on topol-
ogy for -time processing of streaming data from social networks”. In:2016
IEEE First International Conference on Data Stream Mining & Processing (DSMP).
IEEE. 2016, . 345–349.
[10] Mick Bauer. “Paranoid penguin: an introduction to Novell AppArmor”. In:
Linux Journal2006.148 (2006), p. 13.
[11] Sean Bechhofer . “Research objects: Towards exchange and reuse of digi-
tal knowledge”. In:Nature Precedings(2010), . 1–1.
[12] Amin Beheshti . “Coredb: a data lake service”. In:Proceedings of the 2017
ACM on Conference on Information and Knowledge Management. 2017, . 2451–
2454.
[13] Amin Beheshti . “CoreKG: a knowledge lake service”. In:Proceedings of
the VLDB Endowment11.12 (2018), . 1942–1945.
[14] Seyed-Mehdi-Reza Beheshti, Hamid Reza Motahari-Nezhad, and Boualem
Benatallah. “Temporal provenance model (TPM): model and query language”.
In:arXiv preprint arXiv:1211.5009(2012).
[15] Seyed-Mehdi-Reza Beheshti . “On automating basic data curation tasks”.
In:Proceedings of the 26th International Conference on World Wide Web Compan-
ion. 2017, . 165–169. 144Bibliography
[16] Khalid Belhajjame .Prov-dm: The prov data model. Tech. rep. 2013.
[17] Gordon Bell, Tony Hey, and Alex Szalay. “Beyond the data deluge”. In:Sci-
ence323.5919 (2009), . 1297–1298.
[18] Richard B Berry .The AASM Manual for the Scoring of Sleep and Associ-
ated Events: Rules, Terminology and Technical Specifications. Illinois: American
Academy of Sleep Medicine, 2015.
[19] Anant Bhardwaj . “Datahub: Collaborative data science & dataset version
management at scale”. In:arXiv preprint arXiv:1409.0798(2014).
[20] L. Biel . “ECG analysis: a in human identification”. In:
IEEE Transactions on Instrumentation and Measurement50.3 (2001), . 808–812.
[21] Sven Bingert . “An API to include resources in workflow systems”.
In:INFOCOMP 2021: The Eleventh International Conference on Advanced Com-
munications and Computation. 2021, . 15–20.
[22] Mohammad Hossein Biniaz . “Secure Authorization for RESTful ”. In:INFOCOMP 2022, The Twelfth International Conference on Advanced
Communications and Computation. Ed. by Claus-Peter Rückemann . 2022,
12–17.
[23] Mark S Birrittella . “Intel® omni-path architecture: Enabling scalable,
high performance fabrics”. In:2015 IEEE 23rd Annual Symposium on High-
Performance Interconnects. IEEE. 2015, . 1–9.
[24] MKABV Bittorf . “Impala: A modern, open-source SQL engine for
Hadoop”. In:Proceedings of the 7th biennial conference on innovative data systems
research. 2015, . 1–10.
[25] Kai Tobias Block, Martin Uecker, and Jens Frahm. “Undersampled radial MRI
with multiple coils. Iterative image reconstruction a total variation con-
straint”. In:Magnetic Resonance in Medicine: An Official Journal of the Interna-
tional Society for Magnetic Resonance in Medicine57.6 (2007), . 1086–1098.
[26] Alexandra Boldyreva . “Order-preserving symmetric encryption”. In:Ad-
vances in Cryptology-EUROCRYPT 2009: 28th Annual International Conference
on the Theory and Applications of Cryptographic Techniques, Cologne, Germany,
April 26-30, 2009. Proceedings 28. Springer. 2009, . 224–241.
[27] Karla AV Borges, Alberto HF Laender, and Clodoveu A Davis Jr. “Spatial
data integrity constraints in object oriented geographic data modeling”. In:
Proceedings of the 7th ACM international symposium on Advances in geographic
information systems. 1999, . 1–6.
[28] Dhruba Borthakur. “The hadoop distributed file system: Architecture and
design”. In:Hadoop Project Website11.2007 (2007), p. 21.
[29] PeterBraam. “TheLustrestoragearchitecture”. In:arXivpreprint
arXiv:1903.01955(2019).
[30] Jennifer Buchmüller . “Extending an Open-Source Federated Identity
Management System for Enhanced Security”. In:The International Con-
ference for High Performance Computing, Networking, Storage, and Analysis(2020).
[31] Patrice Calegari, Marc Levrier, and Paweł Balczy ́nski. “Web portals for high-
performance computing: a survey”. In:ACM Transactions on the Web (TWEB)
13.1 (2019), . 1–36. Bibliography145
[32] Capgemini SE and Pivotal Software Inc.The Technology of the Business Data
Lake. 2013.URL:https://www.capgemini.com/wp-content/uploads/2017/
07/pivotal-business-data-lake-technical_brochure_web.pdf(visited
on 09/24/2023).
[33] Steven B Caudill. “The necessity of mining data”. In:Atlantic Economic Journal
16.3 (1988), p. 11.
[34] Fay Chang . “Bigtable: A distributed storage system for structured data”.
In:ACM Transactions on Computer Systems (TOCS)26.2 (2008), . 1–26.
[35] Wo L Chang and Nancy Grady. “Nist big data interoperability framework:
Volume 1, definitions”. In: (2019).
[36] Kyle Chard . “I’ll take that to go: Big data bags and minimal identifiers for
exchange of large, complex datasets”. In:2016 IEEE International Conference
on Big Data (Big Data)(Washington, DC, USA). IEEE, Dec. 2016, . 319–328.
DOI:10.1109/BigData.2016.7840618.URL:https://doi.org/10.1109/
BigData.2016.7840618.
[37] Amit Chavan . “Towards a unified query language for provenance and
versioning”. In:7th USENIX Workshop on the Theory and Practice of Provenance
(TaPP 15). 2015.
[38] Haogang Chen . “Linux kernel vulnerabilities: State-of-the-art defenses
and open problems”. In:Proceedings of the Second Asia-Pacific Workshop on Sys-
tems. 2011, . 1–5.
[39] Lanxiang Chen . “Secure search for encrypted personal health records
from big data NoSQL databases in cloud”. In:Computing102 (2020), . 1521–
1545.
[40] Mohamed Cherradi and Anass EL Haddadi. “Data Lakes: A Survey Paper”.
In:Innovations in Smart Cities Applications Volume 5. Ed. by Mohamed Ben
Ahmed . Cham: Springer International Publishing, 2022, . 823–835.
ISBN: 978-3-030-94191-8.
[41] Mandy Chessell . “Governing and managing big data for analytics and
decision makers”. In:IBM Redguides for Business Leaders(2014).
[42] Fernando Chirigati . “ Reprozip: Computational reproducibility with
ease”. In:Proceedings of the 2016 international conference on management of data.
2016, . 2085–2088.
[43] Shreyas Cholia and Terence Sun. “The NEWT platform: an extensible plugin
framework for creating ReSTful APIs”. In:Concurrency and Computation:
Practice and Experience27.16 (2015), . 4304–4317.
[44] Jason Christopher, Gary Jung, and Christopher Doane. “Making it More Se-
cure: The Technical and Social Challenges of Expanding the Functionality of
an Existing Cluster to Meet University and Federal Data Security Re-
quirements”. In:Proceedings of the Practice and Experience in Advanced Research
Computing on Rise of the Machines (learning). 2019, . 1–5.
[45] Sophie Cockcroft. “ A taxonomy of spatial data integrity constraints ”. In:
GeoInformatica1.4 (1997), . 327–343.
[46] Diana Coman Schmid . “SPHN–The BioMedIT Network: A Secure IT
Platform for Research with Sensitive Human Data”. In:Digital Personalized
Health and Medicine270 (2020), . 1170–1174. 146Bibliography
[47] Peter Corbett . “Overview of the MPI-IO parallel I/O interface”. In:In-
put/Output in Parallel and Distributed Computer Systems(1996), . 127–146.
[48] Julia Couto . “A Mapping Study about Data Lakes: An Improved Defini-
tion and Possible Architectures.” In:SEKE. 2019, . 453–578.
[49] Felipe A. Cruz . “FirecREST: a RESTful API to systems”. In:2020
IEEE/ACM International Workshop on Interoperability of Supercomputing and
CloudTechnologies(SuperCompCloud). 2020, 21–26
.DOI:10.1109/SuperCompCloud51944.2020.00009.
[50] Dong Dai . “Lightweight provenance service for high-performance com-
puting”. In:2017 26th International Conference on Parallel Architectures and Com-
pilation Techniques (PACT). IEEE. 2017, . 117–129.
[51] Koenraad De Smedt, Dimitris Koureas, and Peter Wittenburg. “FAIR digi-
tal objects for science: From data pieces to actionable knowledge units”. In:
Publications8.2 (2020), p. 21.
[52] Jeffrey Dean and Sanjay Ghemawat. “MapReduce: simplified data processing
on large clusters”. In:Communications of the ACM51.1 (2008), . 107–113.
[53] Barry A. Devlin and Paul T. Murphy. “An architecture for a business and
information system”. In:IBM systems Journal27.1 (1988), . 60–80.
[54] Claudia Diamantini . “A metadata model to uniformly handle het-
erogeneous data lake sources”. In:European Conference on Advances in Databases
and Information Systems. Springer. 2018, . 165–177.
[55] Henrik Dibowski . “ Semantic Technologies to Manage a Data Lake:
DataCatalog,ProvenanceandAccessControl.” In:SSWS@ ISWC
. 2020, . 65–80.
[56] J. Dixon.Pentaho, Hadoop, and Data Lakes. 2010.URL:https://jamesdixon.
wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/(visited on
03/10/2021).
[57] James Dixon.Data Lakes Revisited. 2014.URL:https://jamesdixon.wordpress.
com/2014/09/25/data-lakes-revisited/(visited on 09/24/2023).
[58] Chris Dunlap.MUNGE Uid ’N’ Gid Emporium. 2022.URL:https : / / dun .
github.io/munge/(visited on 03/21/2022).
[59] Simon Eismann . “A review of serverless cases and their characteris-
tics”. In:arXiv preprint arXiv:2008.11110(2020).
[60] R Elmasri and SB Navathe.Fundamentals of database systems. 1994.
[61] Ronald Fagin, Amnon Lotem, and Moni Naor. “Optimal aggregation algo-
rithms for middleware”. In:Journal of computer and system sciences66.4 (2003),
. 614–656.
[62] Edgar Gabriel . “Open MPI: Goals, concept, and design of a next gen-
eration MPI implementation”. In:European Parallel Virtual Machine/Message
Passing Interface Users’ Group Meeting. Springer. 2004, . 97–104.
[63] Todd Gamblin . “The Spack package manager: bringing order to software chaos”. In:Proceedings of the International Conference for High Perfor-
mance Computing, Networking, Storage and Analysis. 2015, . 1–12.
[64] Gartner Inc.Gartner Says Beware of the Data Lake Fallacy. 2014.URL:https:
//www.gartner.com/en/newsroom/press-releases/2014-07-28-gartner-
says-beware-of-the-data-lake-fallacy(visited on 09/24/2023). Bibliography147
[65] Corinna Giebler . “ The Data Lake Architecture Framework: A Founda-
tion for Building a Comprehensive Data Lake Architecture”. In:Proceedings
der 19. Fachtagung für Datenbanksysteme für Business, Technologie und Web (BTW
2021). 2021.
[66] Corinna Giebler . “A Zone Reference Model for Enterprise-Grade Data
Lake Management”. In:Proceedings of the 24th IEEE Enterprise Computing Con-
ference (EDOC 2020). 2020.DOI:https://doi.org/10.1109/EDOC49727.
2020.00017.
[67] Corinna Giebler . “Modeling data lakes with data vault: practical expe-
riences, assessment, and lessons learned”. In:International Conference on Con-
ceptual Modeling. Springer. 2019, . 63–77.
[68] GitLab.GitLab CI/CD variables. 2022.URL:https://docs.gitlab.com/ee/
ci/variables/(visited on 03/18/2022).
[69] Matteo Golfarelli, Dario Maio, and Stefano Rizzi. “The dimensional fact mod-
el: A conceptual model for data warehouses”. In:International Journal of Coop-
erative Information Systems7.02n03 (1998), . 215–247.
[70] Alex Gorelik.The enterprise big data lake: Delivering the promise of big data and
data science. O’Reilly Media, 2019.
[71] Krzysztof J Gorgolewski . “The brain imaging data structure, a format for
organizing and describing outputs of neuroimaging experiments”. In:Scien-
tific data3.1 (2016), . 1–9.
[72] Shashank Gugnani, Xiaoyi Lu, and Dhabaleswar K Panda. “Swift-X: Accel-
erating OpenStack swift with RDMA for building an efficient cloud”.
In:2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing (CCGRID). IEEE. 2017, . 238–247.
[73] Rihan Hai, Sandra Geisler, and Christoph Quix. “Constance: An intelligent
data lake system”. In:Proceedings of the 2016 international conference on man-
agement of data. 2016, . 2097–2100.
[74] Rihan Hai, Christoph Quix, and Matthias Jarke. “Data lake concept and sys-
tems: a survey”. In:arXiv preprint arXiv:2106.09592(2021).
[75] Rihan Hai, Christoph Quix, and Chen Zhou. “Query rewriting for hetero-
geneous data lakes”. In:Advances in Databases and Information Systems: 22nd
European Conference, ADBIS 2018, Budapest, Hungary, September 2–5, 2018, Pro-
ceedings 22. Springer. 2018, . 35–49.
[76] Michael Austin Halcrow. “eCryptfs: An enterprise-class encrypted filesystem
for linux”. In:Proceedings of the 2005 Linux Symposium. Vol. 1. 2005, . 201–
218.
[77] Alon Halevy . “Goods: Organizing google’s datasets”. In:Proceedings of
the 2016 International Conference on Management of Data. 2016, . 795–806.
[78] Alon Y Halevy . “Managing Google’s data lake: an overview of the Goods
system.” In:IEEE Data Eng. Bull.39.3 (2016), . 5–14.
[79] Dick Hardt.The OAuth 2.0 Authorization Framework. RFC 6749. Oct. 2012.DOI:
10.17487/RFC6749.URL:https://www.rfc- editor.org/info/rfc6749
(visited on 03/21/2022).
[80] Reihaneh H Hariri, Erik M Fredericks, and Kate M Bowers. “Uncertainty in
big data analytics: survey, opportunities, and challenges”. In:Journal of Big
Data6.1 (2019), . 1–16. 148Bibliography
[81] Olaf Hartig and Jun Zhao. “Publishing and consuming provenance metadata
on the web of linked data”. In:International Provenance and Annotation Work-
shop. Springer. 2010, . 78–90.
[82] Zirije Hasani, Margita Kon-Popovska, and Goran Velinov. “Lambda architec-
ture for time big data analytic”. In:ICT Innovations(2014), . 133–143.
[83] Michael Hausenblas and Jacques Nadeau. “Apache drill: interactive ad-hoc
analysis at scale”. In:Big data1.2 (2013), . 100–104.
[84] Leonie Henschel . “Fastsurfer-a fast and accurate deep learning neuroimaging pipeline”. In:NeuroImage219 (2020), p. 117012.
[85] Frank Herold, Sven Breuner, and Jan Heichler.An introduction to BeeGFS.
2014.
[86] Alan R. Hevner . “Design Science in Information Systems Research”. In:
MIS Quarterly28.1 (2004), . 75–105.ISSN: 02767783.URL:http://www.
jstor.org/stable/25148625(visited on 12/29/2023).
[87] David E Hudak . “Open OnDemand: Transforming computational sci-
ence through omnidisciplinary software cyberinfrastructure”. In:Proceedings
of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. 2016,
. 1–7.
[88] Bill Inmon.Data Lake Architecture: Designing the Data Lake and avoiding the
garbage dump. Technics publications, 2016.
[89] W. H. Inmon. “The Data Warehouse and Data Mining”. In:Commun. ACM
39.11 (1996), 49–50.ISSN: 0001-0782.DOI:10 . 1145 / 240455 . 240470.URL:
https://doi.org/10.1145/240455.240470.
[90] William H Inmon.Building the data warehouse. John Wiley & Sons, 2005.
[91] Zachary G Ives and Yi Zhang. “Dataset relationship management”. In:Pro-
ceedings of Conference on Innovative Database Systems Research (CIDR 19). 2019.
[92] Patrick Jattke . “BLACKSMITH: Scalable Rowhammering in the Frequen-
cy Domain”. In:IEEE Symposium on Security and Privacy (SP)(2022).
[93] Aude Jegou . “BIDS Manager-Pipeline: A framework for multi-subject
analysisinelectrophysiology”. In:NeuroscienceInformatics2.2
(2022), p. 100072.
[94] Robert Kahn, Robert Wilensky, . “A framework for distributed digital
object services”. In:International Journal on Digital Libraries6.2 (2006), . 115–
123.
[95] David Karns, Katy Protin, and Justin Wolf.iSSH v. Auditd: Intrusion Detection
in High Performance Computing. Tech. rep. Los Alamos National Lab.(LANL),
Los Alamos, NM (United States), 2012.
[96] Pradeeban Kathiravelu and Ashish Sharma. “A dynamic data warehousing
platform for creating and accessing biomedical data lakes”. In:Data Manage-
ment and Analytics for Medicine and Healthcare: Second International Workshop,
DMAH 2016, Held at VLDB 2016, Delhi, India, September 9, 2016, Revised
Selected Papers 2. Springer. 2017, . 101–120.
[97] Florian Kerschbaum. “Frequency-hiding order-preserving encryption”. In:
Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communi-
cations Security. 2015, . 656–667. Bibliography149
[98] Pwint Phyu Khine and Zhao Shun Wang. “Data lake: a ideology in big
data era”. In:ITM web of conferences. Vol. 17. EDP Sciences. 2018, p. 03025.
[99] G. Klosh . “The SIESTA project polygraphic and clinical database”. In:
IEEE Engineering in Medicine and Biology Magazine20.3 (2001), . 51–57.DOI:
10.1109/51.932725.
[100] Vasili Korol . “Introducing VIKING: A novel online platform for multi-
scale modeling”. In:ACS omega5.2 (2019), . 1254–1260.
[101] Jay Kreps, Neha Narkhede, Jun Rao, . “Kafka: A distributed messaging
system for log processing”. In:Proceedings of the NetDB. Vol. 11. 2011. Athens,
Greece. 2011, . 1–7.
[102] Thomas S Kuhn.The structure of scientific revolutions. Vol. 111. Chicago Uni-
versity of Chicago Press, 1970.
[103] J Kunkel . “Establishing the io-500 benchmark”. In:White Paper(2016).
[104] John Kunze .The bagit file packaging format (v1. 0). Tech. rep. 2018.
[105] Gregory M Kurtzer, Vanessa Sochat, and Michael W Bauer. “Singularity: Sci-
entific containers for mobility of compute”. In:PloS one12.5 (2017).
[106] Christian Köhler . “Secure Authorization for RESTful with
FaaS Support”. In: 3 and 4 (2022). Ed. by Claus-Peter Rückemann, . 119–
131.
[107] Dayeol Lee . “Keystone: An open framework for architecting trusted ex-
ecution environments”. In:Proceedings of the Fifteenth European Conference on
Computer Systems. 2020, . 1–16.
[108] Jae-Kook Lee, Sung-Jun Kim, and Taeyoung Hong. “Brute-force Attacks Anal-
ysis against SSH in Multi- Service Environment”. In:Indian Journal
of Science and Technology9.24 (2016), . 1–4.
[109] Kevin Lewi and David J Wu. “Order-revealing encryption: construc-
tions, applications, and lower bounds ”. In:Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security. 2016, . 1167–
1178.
[110] Jingmin Li. “Design of -time data analysis system on Impala”. In:
2014 IEEE Workshop on Advanced Research and Technology in Industry Applica-
tions (WARTIA). IEEE. 2014, . 934–936.
[111] Dan Lindstedt and Kent Graziano.Super charge your data warehouse: invaluable
data modeling rules to implement your data vault. CreateSpace, 2011.
[112] Marilex Rea Llave. “Data lakes in business intelligence: reporting from the
trenches”. In:Procedia computer science138 (2018), . 516–524.
[113] Jay Lofstead, Joshua Baker, and Andrew Younge. “Data pallets: containeriz-
ing storage for reproducibility and traceability”. In:High Performance Com-
puting: ISC High Performance 2019 International Workshops, Frankfurt, Germany,
June 16-20, 2019, Revised Selected Papers 34. Springer. 2019, . 36–45.
[114] Michael Lustig, David Donoho, and John M Pauly. “Sparse MRI: The appli-
cation of compressed sensing for rapid MR imaging”. In:Magnetic Resonance
in Medicine: An Official Journal of the International Society for Magnetic Resonance
in Medicine58.6 (2007), . 1182–1195. 150Bibliography
[115] Antonio Maccioni and Riccardo Torlone. “Crossing the finish line faster when
paddling the data lake with kayak”. In:Proceedings of the VLDB Endowment
10.12 (2017), . 1853–1856.
[116] Antonio Maccioni and Riccardo Torlone. “KAYAK: a framework for just-in-
time data preparation in a data lake”. In:International Conference on Advanced
Information Systems Engineering. Springer. 2018, . 474–489.
[117] Cedrine Madera and Anne Laurent. “The next information architecture evo-
lution: the data lake wave”. In:Proceedings of the 8th international conference on
management of digital ecosystems. 2016, . 174–180.
[118] Mark Madsen. “How to Build an enterprise data lake: important considera-
tions before jumping in”. In:Third Nature Inc(2015), . 13–17.
[119] Daniel S Marcus . “The Extensible Neuroimaging Archive Toolkit: an in-
formatics platform for managing, exploring, and sharing neuroimaging data”.
In:Neuroinformatics5 (2007), . 11–33.
[120] Vivien Marx. “The big challenges of big data”. In:Nature498.7453 (2013),
. 255–260.
[121] Christian Mathis. “Data lakes”. In:Datenbank-Spektrum17.3 (2017), . 289–
293.
[122] Frank McKeen . “Intel® software guard extensions (intel® sgx) support
for dynamic memory management inside an enclave”. In:Proceedings of the
Hardware and Architectural Support for Security and Privacy 2016. 2016, . 1–9.
[123] Robert McLay . “Best practices for the deployment and management of
production clusters”. In:SC’11: Proceedings of 2011 International Confer-
ence for High Performance Computing, Networking, Storage and Analysis. IEEE.
2011, . 1–11.
[124] Xiangrui Meng . “Mllib: Machine learning in apache spark”. In:The jour-
nal of machine learning research17.1 (2016), . 1235–1241.
[125] R Menolascino . “A realistic UMTS planning exercise”. In:Proc. 3 ACTS
Mobile Communications Summit 98. 1998.
[126] Hui Miao, Amit Chavan, and Amol Deshpande. “Provdb: Lifecycle manage-
ment of collaborative analysis workflows”. In:Proceedings of the 2nd Workshop
on Human-in-the-Loop Data Analytics. 2017, . 1–6.
[127] Hui Miao and Amol Deshpande. “ProvDB: Provenance-enabled Lifecycle
Management of Collaborative Data Analysis Workflows.” In:IEEE Data Eng.
Bull.41.4 (2018), . 26–38.
[128] George A Miller. “WordNet: a lexical database for English”. In:Communica-
tions of the ACM38.11 (1995), . 39–41.
[129] Steven P Miller . “Kerberos authentication and authorization system”. In:
In Project Athena Technical Plan. Citeseer. 1988.
[130] Paolo Missier, Khalid Belhajjame, and James Cheney. “The W3C PROV fam-
ily of specifications for modelling provenance metadata”. In:Proceedings of the
16th International Conference on Extending Database Technology. 2013, . 773–
776.
[131] Paolo Missier . “Linking multiple workflow provenance traces for inter-
operable collaborative science”. In:The 5th Workshop on Workflows in Support
of Large-Scale Science. IEEE. 2010, . 1–8. Bibliography151
[132] Abidalrahman Moh’d, Yaser Jararweh, and Lo’ai Tawalbeh. “AES-512: 512-
bit Advanced Encryption Standard algorithm design and evaluation”. In:
2011 7th International Conference on Information Assurance and Security (IAS).
IEEE. 2011, . 292–297.
[133] Luc Moreau . “The open provenance model core specification (v1. 1)”. In:
Future generation computer systems27.6 (2011), . 743–756.
[134] Kiran-Kumar Muniswamy-Reddy . “Provenance-aware storage systems.”
In:Usenix annual technical conference, general track. 2006, . 43–56.
[135] Amr A Munshi and Yasser Abdel-Rady I Mohamed. “Data lake lambda archi-
tecture for smart grids big data analytics”. In:IEEE Access6 (2018), . 40463–
40471.
[136] Athira Nambiar and Divyansh Mundra. “An Overview of Data Warehouse
and Data Lake in Modern Enterprise Data Management”. In:Big Data and
Cognitive Computing6.4 (2022), p. 132.
[137] Fatemeh Nargesian . “Data lake management: challenges and opportu-
nities”. In:Proceedings of the VLDB Endowment12.12 (2019), . 1986–1989.
[138] Roberto Navigli and Simone Paolo Ponzetto. “BabelNet: The automatic con-
struction, evaluation and application of a wide-coverage multilingual seman-
tic network”. In:Artificial intelligence193 (2012), . 217–250.
[139] Iuri D Nogueira, Maram Romdhane, and Jérôme Darmont. “Modeling data
lake metadata with a data vault ”. In:Proceedings of the 22nd International Data-
base Engineering & Applications Symposium. 2018, . 253–261.
[140] Hendrik Nolte and Julian Kunkel. “Governance-Centric Paradigm: Overcom-
ing the Information Gap between Users and Systems by Enforcing Data Man-
agement Plans on -Systems”. In:INFOCOMP 2023, The Thirteenth In-
ternational Conference on Advanced Communications and Computation. Ed. by
Claus-Peter Rückemann. 2023, . 13–20.
[141] Hendrik Nolte, Philip Langer, and Julian Kunkel.KI in der Projektwirtschaft
Band 2. Status: Submitted/Accepted. UVK Verlag, 2024.
[142] Hendrik Nolte, Lars Quentin, and Julian Kunkel. “Comparing Different En-
cryption Workflows for Secure Elasticsearch Clusters on Shared Sys-
tems for Sensitive Data”. In:Journal of High-Performance Storage. Status: Sub-
mitted.
[143] Hendrik Nolte, Lars Quentin, and Julian Kunkel. “Secure Elasticsearch Clus-
ters on Systems for Sensitive Data”. In:HIGH PERFORMANCE COM-
PUTING: ISC High Performance 2024 International Workshops. Status: Submit-
ted/Accepted. Springer. 2024.
[144] Hendrik Nolte and Philipp Wieder. “Realising data-centric scientific work-
flows with provenance-capturing on data lakes ”. In:Data Intelligence4.2
(2022), . 426–438.
[145] Hendrik Nolte . “A Secure Workflow for Shared Systems”. In:22nd
International Symposium on Cluster, Cloud and Internet Computing (CCGrid).
2022.
[146] Hendrik Nolte . “Secure : A workflow providing a secure parti-
tion on an system”. In:Future Generation Computer Systems141 (2023),
. 677–691. 152Bibliography
[147] Daniel de Oliveira . “An adaptive parallel execution strategy for cloud- scientific workflows”. In:Concurrency and Computation: Practice and Ex-
perience24.13 (2012), . 1531–1550.
[148] OpenAPI Initiative.OpenAPI Specification v3.0.0.2017.URL:https://spec.
openapis.org/oas/v3.0.0(visited on 03/21/2022).
[149] OpenFaaS.Invocations. 2022.
URL:https://docs.openfaas.com/architecture/invocations/(visited on
12/13/2022).
[150] Andrew Oram.Managing the Data Lake: Moving to Big Data Analysis. O’Reilly
Media, 2015.
[151] Arvind Panwar . “A Blockchain Framework to Secure Personal Health
Record (PHR) in IBM Cloud- Data Lake”. In:Computational Intelligence
and Neuroscience2022 (2022).
[152] Thorsten Papenbrock . “Data profiling with metanome”. In:Proceedings
of the VLDB Endowment8.12 (2015), . 1860–1863.
[153] P Patel, G Wood, and A Diaz. “Data lake governance best practices”. In:The
DZone Guide to Big Data-Data Science & Advanced Analytics4 (2017), . 6–7.
[154] Cesare Pautasso and Gustavo Alonso. “Parallel computing patterns for grid
workflows”. In:2006 Workshop on Workflows in Support of Large-Scale Science.
IEEE. 2006, . 1–10.
[155] Brian Pawlowski . “The NFS version 4 protocol”. In:In Proceedings of the
2nd International System Administration and Networking Conference (SANE 2000.
Citeseer. 2000.
[156] Ken Peffers . “A design science research methodology for information
systems research”. In:Journal of management information systems(2007), . 45–
77.
[157] Pedro F Pérez-Arteaga . “Cost comparison of lambda architecture im-
plementations for transportation analytics public cloud software as a
service”. In:Special Session on Software Engineering for Service and Cloud Com-
puting(2018), . 855–862.
[158] Pierre Peterlongo . “Lossless filter for finding long multiple approxi-
mate repetitions a data structure, the bi-factor array”. In:Interna-
tional Symposium on String Processing and Information Retrieval. Springer. 2005,
. 179–190.
[159] Gregory F Pfister. “An introduction to the infiniband architecture”. In:High
performance mass storage and parallel I/O42.617-632 (2001), p. 10.
[160] Karl Popper.The logic of scientific discovery. Routledge, 2005.
[161] Klaas P Pruessmann . “SENSE: sensitivity encoding for fast MRI”. In:
Magnetic Resonance in Medicine: An Official Journal of the International Society
for Magnetic Resonance in Medicine42.5 (1999), . 952–962.
[162] Christoph Quix, Rihan Hai, and Ivan Vatov. “GEMMS: A Generic and Ex-
tensible Metadata Management System for Data Lakes.” In:CAiSE forum.
Vol. 129. 2016.
[163] Raghu Ramakrishnan . “Azure data lake store: a hyperscale distributed
file service for big data analytics”. In:Proceedings of the 2017 ACM International
Conference on Management of Data. 2017, . 51–63. Bibliography153
[164] Alessandro Randazzo and Ilenia Tinnirello. “Kata containers: An emerging
architecture for enabling mec services in fast and secure way”. In:2019 Sixth
International Conference on Internet of Things: Systems, Management and Security
(IOTSMS). IEEE. 2019, . 209–214.
[165] Franck Ravat and Yan Zhao. “Data lakes: Trends and perspectives”. In:Inter-
national Conference on Database and Expert Systems Applications. Springer. 2019,
. 304–313.
[166] Ronald L Rivest, Adi Shamir, and Leonard Adleman. “A method for obtain-
ing digital signatures and public-key cryptosystems”. In:Communications of
the ACM21.2 (1978), . 120–126.
[167] Pegdwendé Sawadogo and Jérôme Darmont. “On data lake architectures and
metadata management”. In:Journal of Intelligent Information Systems56 (2021),
. 97–120.
[168] Pegdwendé N Sawadogo . “Metadata systems for data lakes: models and
features”. In:European conference on advances in databases and information sys-
tems. Springer. 2019, . 440–451.
[169] SchedMD.Slurm REST API. 2022.URL:https://slurm.schedmd.com/rest.
html(visited on 03/18/2022).
[170] Michel Scheerman . “Secure platform for processing sensitive data on
shared systems”. In:arXiv preprint arXiv:2103.14679(2021).
[171] Frank B Schmuck and Roger L Haskin. “GPFS: A Shared-Disk File System for
Large Computing Clusters.” In:FAST. Vol. 2. 19. 2002, . 231–244.
[172] Etienne Scholly . “Coining goldMEDAL: a contribution to data lake
generic metadata modeling”. In:arXiv preprint arXiv:2103.13155(2021).
[173] Erik Schultes and Peter Wittenburg. “FAIR Principles and Digital Objects:
Accelerating convergence on a data infrastructure”. In:Data Analytics and
Management in Data Intensive Domains: 20th International Conference, DAM-
DID/RCDL 2018, Moscow, Russia, October 9–12, 2018, Revised Selected Papers
20. Springer. 2019, . 3–16.
[174] Philip Schwan . “Lustre: Building a file system for 1000-node clusters”.
In:Proceedings of the 2003 Linux symposium. Vol. 2003. 2003, . 380–386.
[175] Ulrich Schwardmann. “Digital objects–FAIR digital objects: Which services
are required?” In:Data Science Journal19.1 (2020).
[176] Omar Sefraoui, Mohammed Aissaoui, Mohsine Eleuldj, . “OpenStack: to-
ward an open-source solution for cloud computing”. In:International Journal
of Computer Applications55.3 (2012), . 38–42.
[177] Raghav Sethi . “Presto: SQL on everything”. In:2019 IEEE 35th Interna-
tional Conference on Data Engineering (ICDE). IEEE. 2019, . 1802–1813.
[178] Ben Sharma.Architecting data lakes: data management architectures for advanced
business cases. O’Reilly Media, 2018.
[179] Shekhar Shashi and Chawla Sanjay. “Spatial databases: A tour”. In:Upper
Saddle River, Jersey7458 (2003).
[180] Smadar Shilo, Hagai Rossman, and Eran Segal. “Axes of a revolution: chal-
lenges and promises of big data in healthcare”. In:Nature medicine26.1 (2020),
. 29–38. 154Bibliography
[181] David W Shimabukuro . “Effect of a machine learning- severe sep-
sis prediction algorithm on patient survival and hospital length of stay: a
randomised clinical trial”. In:BMJ open respiratory research4.1 (2017).
[182] Arun Kumar Singh and Samidha Dwivedi Sharma. “High Performance Com-
puting () Data Center for Information as a Service (IaaS) Security Check-
list: Cloud Data Governance.” In:Webology16.2 (2019), . 83–96.
[183] Amit Singhal. “Introducing the knowledge graph: things, not strings”. In:
Official google blog5 (2012), p. 16.
[184] Tyler J Skluzacek, Kyle Chard, and Ian Foster. “Klimatic: a virtual data lake
for harvesting and distribution of geospatial data”. In:2016 1st Joint Interna-
tional Workshop on Parallel Data Storage and data Intensive Scalable Computing
Systems (PDSW-DISCS). IEEE. 2016, . 31–36.
[185] Stephen Smalley, Chris Vance, and Wayne Salamon. “Implementing SELinux
as a Linux security module”. In:NAI Labs Report1.43 (2001), p. 139.
[186] Austin Smith . “Exploring Untrusted Distributed Storage for High Per-
formance Computing”. In:Proceedings of the Practice and Experience in Ad-
vanced Research Computing on Rise of the Machines (learning). 2019, . 1–6.
[187] Jens B Stephansen . “Neural network analysis of sleep stages enables
efficient diagnosis of narcolepsy”. In:Nature Communications9.5229 (2018).
DOI:10.1038/s41467-018-07229-3.
[188] Cathie Sudlow . “UK biobank: an open resource for identifying
the causes of a wide range of complex diseases of middle and old age”. In:
PLoS medicine12.3 (2015), e1001779.
[189] Snezhana Sulova . “The Usage of Data Lake for Business Intelligence
Data Analysis”. In:International Conference Information and communication tech-
nologies in business and education. Vol. 18. 2019, . 135–144.
[190] Isuru Suriarachchi and Beth Plale. “Crossing analytics systems: a case for in-
tegrated provenance in data lakes”. In:2016 IEEE 12th International Conference
on e-Science (e-Science). IEEE. 2016, . 349–354.
[191] Isuru Suriarachchi and Beth Plale. “Provenance as essential infrastructure for
data lakes”. In:International Provenance and Annotation Workshop. Springer.
2016, . 178–182.
[192] Isuru Suriarachchi, Quan Zhou, and Beth Plale. “Komadu: A capture and vi-
sualization system for scientific data provenance”. In:Journal of Open Research
Software3.1 (2015).
[193] Vaclav Svaton . “-as-a-Service via HEAppE Platform”. In:Conference
on Complex, Intelligent, and Software Intensive Systems. Springer. 2019, . 280–
293.
[194] V. Telezki . “Deployment of an -Accelerated Research Data Manage-
ment System: Exemplary Workflow in HeartAndBrain Study”. In:Workshop
Biosignals. 2024.DOI:https://doi.org/10.47952/gro-publ-204.
[195] Ashish Thusoo . “Hive-a petabyte scale data warehouse hadoop”.
In:2010 IEEE 26th international conference on data engineering (ICDE 2010).
IEEE. 2010, . 996–1005.
[196] Ashish Thusoo . “Hive: a warehousing solution over a map-reduce frame-
work”. In:Proceedings of the VLDB Endowment2.2 (2009), . 1626–1629. Bibliography155
[197] Chia-Che Tsai, Donald E Porter, and Mona Vij. “Graphene-sgx: A practical
library{OS}for unmodified applications on{SGX}”. In:2017{USENIX}
Annual Technical Conference ({USENIX}{ATC}17). 2017, . 645–658.
[198] M Uecker and M Lustig.BART toolbox for computational magnetic resonance
imaging, .2016.DOI:10.5281/zenodo.592960.
[199] Martin Uecker . “ESPIRiT—an eigenvalue to autocalibrating
parallel MRI: where SENSE meets GRAPPA ”. In:Magnetic resonance in medi-
cine71.3 (2014), . 990–1001.
[200] Massimo Villari . “AllJoyn Lambda: An architecture for the management
of smart environments in IoT”. In:2014 International Conference on Smart Com-
puting Workshops. IEEE. 2014, . 9–14.
[201] Deepak Vohra. “Apache parquet”. In:Practical Hadoop Ecosystem. Springer,
2016, . 325–335.
[202] Denny Vrandeˇci ́c. “Wikidata: A platform for collaborative data collec-
tion”. In:Proceedings of the 21st international conference on world wide web. 2012,
. 1063–1064.
[203] Coral Walker and Hassan Alrehamy. “Personal data lake with data gravity
pull”. In:2015 IEEE Fifth International Conference on Big Data and Cloud Com-
puting. IEEE. 2015, . 160–167.
[204] Li Wang . “Optimizing eCryptfs for better performance and security”. In:
Linux Symposium. Citeseer. 2012, p. 137.
[205] Min Wang, Jiankun Hu, and Hussein A. Abbass. “BrainPrint: EEG biomet-
ric identification on analyzing brain connectivity graphs”. In:Pattern
Recognition105 (2020), p. 107381.ISSN: 0031-3203.
[206] James Warren and Nathan Marz.Big Data: Principles and best practices of scal-
able realtime data systems. Simon and Schuster, 2015.
[207] Sage A Weil . “Ceph: A scalable, high-performance distributed file sys-
tem”. In:Proceedings of the 7th symposium on Operating systems design and im-
plementation. 2006, . 307–320.
[208] Philipp Wieder and Hendrik Nolte. “Toward data lakes as central building
blocks for data management and analysis”. In:Frontiers in big Data5 (2022),
p. 945720.
[209] M Wilkinson, M Dumontier, and Aalbersberg. “The FAIR Guiding Principles
for scientific data management and stewardship”. In:Scientific Data 3(2016).
URL:https://doi.org/10.1038/sdata.2016.18.
[210] Ming-Chuan Wu and Alejandro P Buchmann. “Research issues in data ware-
housing”. In:Datenbanksysteme in Büro, Technik und Wissenschaft. Springer.
1997, . 61–82.
[211] Chao-Tung Yang . “The implementation of data storage and analytics
platform for big data lake of electricity usage with spark”. In:The Journal of
Supercomputing77.6 (2021), . 5934–5959.
[212] Tatu Ylonen. “SSH - Secure Login Connections Over the Internet”. In:Pro-
ceedings of the 6th USENIX Security Symposium (USENIX Security 96). San Jose,
CA: USENIX Association, July 1996, . 37–42.URL:https://www.usenix.
org/conference/6th-usenix-security-symposium/ssh-secure-login-
connections-over-internet. 156Bibliography
[213] Zhihao Yuan . “Utilizing provenance in reusable research objects”. In:In-
formatics. Vol. 5. 1. Multidisciplinary Digital Publishing Institute. 2018, p. 14.
[214] Z. Wang . “RS-YABI: A workflow system for Remote Sensing Processing
in AusCover”. In:Proceedings of the 19th International Congress on Modelling
and Simulation. MODSIM 2011 - 19th International Congress on Modelling
and Simulation - Sustaining Our Future: Understanding and Living with Un-
certainty. 2011, 1167–1173.
[215] Matei Zaharia . “Apache spark: a unified engine for big data processing”.
In:Communications of the ACM59.11 (2016), . 56–65.
[216] Matei Zaharia . “Spark: Cluster computing with working sets”. In:2nd
USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 10). 2010.
[217] X Zhang . “Process and outcome for international reliability in sleep scor-
ing”. In:Sleep and Breathing19 (1 2015), . 191–5.
[218] Yi Zhang and Zachary G Ives. “Juneau: data lake management for Jupyter”.
In:Proceedings of the VLDB Endowment12.12 (2019).
[219] Paul Zikopoulos.Big data beyond the hype: A guide to conversations for today’s
data center. McGraw-Hill Education, 2015. 157
Declaration on the of AI
In this thesis, I have ChatGPT or another AI as follows:
□not at all
□during brainstorming
□when creating the outline
□to write individual passages, altogether to the extent of ____% of the entire □for the development of software source texts
□for optimizing or restructuring software source texts
□for proofreading or optimizing
■further, namely: Development of individual functions within the analysis
and plotting of theSecure Metadata Managementpresented in Chapter 8.
I hereby declare that I have stated all uses completely. Missing or incorrect infor-
mation will be considered as an attempt to cheat.
Date and Signature
Hendrik Nolte Big Data Management Challenges
Šuman, Sabrina; Poščić, Patrizia; Gligora Marković, Maja
Source / Izvornik: International journal of advanced trends in computer science and engineering, 2020, 9, 717 - 723
Journal article, Published version
Rad u časopisu, Objavljena verzija rada (izdavačev PDF)
https://doi.org/10.30534/ijatcse/2020/102912020
Permanent link / Trajna poveznica:https://urn.nsk.hr/urn:nbn:hr:184:555933
Rights / Prava:Attribution 4.0 International / Imenovanje 4.0 međunarodna
Download date / Datum preuzimanja:2025-06-06
Repository / Repozitorij:
Repository of the University of Rijeka, Faculty of Medicine - FMRI Repository Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 717  ABSTRACT The emergence of data types in big data era implicates the need to analyse and exploit them to gain valuable business insight. Traditional platforms cannot fully meet the analytical needs of the company if support for an unstructured data type is needed. This paper gives an overview and synthesis of areas related to big data technologies, with a series of guidelines for adopting the appropriate software, storage structure, and efficient deployment for big data management. A broad data management context is presented through a conceptual model of business performance management in a modern data management era. Key words: Big data, Big data management, Hadoop, Big data processing, Data lake 1. INTRODUCTION The ICT field is extremely dynamic, with the frequent emergence of technologies. In the last decade there have been major changes in data management with the emergence of types and sources of data summed up in the big data concept. Big data are complex, layered, large amount of data, and big data technologies are usually considered in comparison to standard 3V: Volume, Variety (of many different data types from different sources), Velocity, and additional 2V: Veracity (Data Quality) and Value (Value for Business). Big data can also be structured, but they are primarily semi-structured and unstructured data types. It is primarily the size and data diversity that creates analytics approaches [1]. It imposes special ways of retrieving, transforming and preparing, storing and analysing [2] [3]. Utilizing big data technology has unlimited potential for improving both personal life and business competitiveness [4] [5]. At the same time, a large amount of data types make it difficult to find the right values from data [6], and data management in big data is extremely complex. The value of big data lies in the way that information obtained through analytical processing are (for example to reduce costs, reduce time in business processes and respond to queries, faster and better development of a product, eliminate failures, errors and failures, better customer relationships, risk assessment and making better quality decisions). Problems related to big data are most commonly related to storage, processing, and management in general [7][8]. There are also issues related to data ownership, privacy and security (see for security issues and algorithms in [9]), quality (a large amount of data from different sources should be readily available for analysis in a short time) and timeliness (a large amount of data, longer analysis, streaming issues analyses) [10]. In big data, there are also storage structures (e.g. data lakes, distributed file systems, non-relational databases, etc.), needs for competencies of existing experts and profiles of experts (big data analyst, big data engineer, big data architect and so on) [3]. In order to maintain and increase the company's ability to technology for successful decision-making, it is necessary to build an innovative management platform with all data types, methods of storage, processing and application of intelligent methods (knowledge and information retrieval, pattern recognition, machine learning, optimization methods, etc.) [11]. As Russom's research [12] shows, employees are aware of changes and issues in big data, and 82% of them (225 of them) believe that data in their company evolves, in terms of diversity in structure, type, sources, the way they are managed and how they are in business (20% claim to be drastic, while 62% claim to be moderate). In the same survey, one of the major issues highlighted the incompatibility of large data types and structures with relational databases (68% of respondents). This requires a revision of the data management strategy and good information regarding the potentials and disadvantages of big data technology, big data tools, platforms and ways of implementation. The purpose of this paper is to provide arguments and guidelines for the adoption of an appropriate big data management strategy, selection of software tools, storage structure and efficient deployment, through an overview of the field of big data technologies. The aim of the paper is to provide a synthetic Big Data Management Challenges Sabrina Šuman 1
, Patrizia Poščić 2
, Maja Gligora Marković 3 1 Polytechnic of Rijeka, Business department, Croatia, ssuman@veleri.hr 2 University of Rijeka, Department of Informatics, Croatia, patrizia@inf.uniri.hr 3 University of Rijeka, Faculty of Medicine, Department of Medical Informatics, Croatia, majagm@medri.uniri.hr ISSN 2278-3091 Volume 9, No.1, January – February 2020 International Journal of Advanced Trends in Computer Science and Engineering Available Online at http://www.warse.org/IJATCSE/static/pdf/file/ijatcse102912020.pdf https://doi.org/10.30534/ijatcse/2020/102912020 Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 718 overview of the area relevant to the establishment of modern data architecture. After the introduction, a review of concepts important for understanding the wider area of modern data architecture is given - big data technology from the aspect of data types, storage types, analytical processing, and overall big data management strategy. The concept of warehousing modernization, the Hadoop ecosystem and the data lake were discussed in particular. In the results and discussion , examples of modern data architecture are given as well as the synthetic representation of the processing phase of the big data with the description of each phase with the examples of tools in phases. The motivation for this research stems from the current issues of managing forms of data and concerns all data management phases: from data sources, through purification, analysis, visualization and storage. Also, companies are faced with the problem of choosing the right solutions to manage their data and reviewing possible solutions with a series of guidelines can be beneficial. 2. DATA MANAGEMENT OVERVIEW IN BIG DATA ERA In this , we list some traditional and modern elements of data management such as data warehouses, Hadoop framework, Data lake, Spark and Map Reduce 2.1. Data warehouses role in a big data era Data warehouses are created due to the need to integrate the contents of different databases and other data sources over time and these data effectively to perform analytical processes. It is a centralized, cleaned and integrated organization of different source data. Today, the following questions can be asked: can data warehouses meet the data management needs in big data? Is data warehouse needed in big data era? Are solutions related to big data management a replacement for data warehouses? First of all, a big data solution is a technology that involves storage and management of big data, and a data warehouse is architecture. Today, there are cases where some companies may have a big data solution as well as a data warehouse, cases with no big data management solution, but with a data warehouse, and a scenario where the company does not have a data warehouse but has a big data management solution. Each of the scenarios depending on the business activity and business model of the company can or does not have to be successful. While the company needs reliable, consolidated, relevant data for decision-making and management at all levels, it needs also a data warehouse [13]. Yet, the traditional data warehouse provides the basis for reporting and analytics of structured data but probably does not represent the most cost-effective way to store all kinds of data [14]. Data warehouse concept in big data should evolve because it does not solve all issues related to analytics and decision support. One of the changes in business requirements is the need for analysing unstructured and semi-structured data, performing streaming analytics, network analytics, and other phenomena and needs related to big data. The data are generated in huge quantities, completely incompatible with the relational model, often with lacking ownership, so it is difficult to manage and store them in a rigid structure. Often, a lot of resources were spent on building a warehouse without thinking about data and analytical needs. Also, many implementations were unsuccessful, mostly as a result of bad, rigid designs. More modern and agile design and implementation techniques show greater success, customer satisfaction, and greater return on investment. Such techniques are successful because they allow for flexible upgrades and changes that result from changes in business requirements. In a response to the need to analyse types of data in a big data era, many innovative tools and techniques have been developed to store and process this data. The basic concept in developing these tools and techniques was that each company accessed the data in a customized, personalized way that meets the specifics and requirements of the company. Thus, the evolution of data warehousing is needed to adapt and coexist with other analytical solutions that include the of data types and data sources. This does not mean that there will be no need to store and manage structured data in relational structures, but that companies will different forms of storage, management, and data processing. big data and managing them is a completely separate need and additional business potential that cannot replace the need for warehouses and warehousing [13]. 2.2. Hadoop framework The Hadoop framework is the technology that is becoming the standard when it comes to storing and processing large amounts of different data (big data). The Hadoop platform is allows performing the tasks on multiple computers (Cluster). It is optimized for processing large amounts of data. Hadoop architecture consists of: • Hadoop Common Package – contains Java Archive Files (JARs) and scripts required to run and manage all Hadoop modules; • MapReduce processing mechanism - a system on YARN for parallel processing of large data sets; • Hadoop distributed file system (HDFS) - a distributed, scalable and portable file system which stores large amounts of data (GB and TB) on multiple computers; • YARN - a central platform for managing operations, security and data through Hadoop clusters [15], [16]. Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 719 Hadoop is a platform that offers a solution for many limitations of past technologies, such as storage constraints and large volume data processing capabilities [17]. It supports multiple data formats – i.e. structured, semi-structured and unstructured data. It is the open source software, with low implementation costs, and low learning curve. NoSQL solutions, such as Apache Cassandra, helps eliminate performance issues, costs, and the availability of big data applications. To manage such data, it is convenient to Apache Hive, data storage software built on Hadoop. It allows reading, writing and managing large databases stored in distributed storage with SQL support [3]. The Hadoop ecosystem provides a variety of open source and commercial technologies for processing fast, interactive BI queries, data retrieval and research, and sophisticated analytical processes such as testing predictive models [18]. Today, technologies enable users to run queries and analysis within the Hadoop cluster and/or clouds without having to move data to a data warehouse, data marts, or stand-alone BI server [18]. The Hadoop cluster consists of many parallel machines where large data sets are stored and processed. Client computers send tasks to this cloud of computers and get results. Storage and data processing take place within this "cloud" machine. Different users can send computerized tasks to Hadoop from individual clients (their own machines at remote locations from the Hadoop cluster). The linear scalability offered by Hadoop clusters with flexible cloud scalability storage can enable organizations to be agile and flexible in expanding computer power in response to immediate BI analytic needs. Companies can also improve security management - security procedures that are set on sources, as the preparation and processing takes place where the data is located [18]. 2.3. MapReduce MapReduce is the script frame for applications handling huge amounts of both structured and unstructured data stored in the Hadoop Distributed File System (HDFS). Each MapReduce works in two phases: the map phase which maps the input data to key/value sets and reduce phase which takes key/value pairs and provides a desired output on applying its own algorithms [19]. MapReduce algorithm can be written in many languages (Java, C ++, or Python) and its tasks are easy to run. MapReduce can handle petabytes of data from HDFS on one cluster at maximum processing speeds. MapReduce takes care of the failures and allows retrieving the redundant copy of the processing data. It moves the processing to data in HDFS, and not vice versa. Processing tasks may appear on a physical node where data is found, significantly contributing to Hadoop's processing speed [20]. 2.4. Spark Apache Spark is a distributed or clustered open source computing platform and a big data processing framework. Apache Spark is a framework for fast processing of a large amount of data that can be generally for all types of data processing (batch processing, interactive analysis, stream processing, machine learning and graph computing) [21]. Provides a fast-paced computing environment for general purpose and sophisticated analytical capabilities that enable the development of analytic applications written in Java, Scala, Python or R [21]. Spark can run independently, on the Hadoop cluster or in the Mesos environment. Spark can connect at run time on HDFS, Amazon S3, Cassandra, and Apache HBase. Apache Spark stands out at its speed due to the ability to perform in-memory processing. Spark is more effective for disk processing applications. If linear data processing of large data sets is , the advantage should be given to Hadoop MapReduce while Spark provides fast performance, -time analytics, graphing, machine learning and other. In many cases, Spark can surpass Hadoop MapReduce. Also, Spark is fully compatible with the Hadoop Ecosystem [22]. It is suitable to be by a data scientist or a statistician without or limited knowledge of cluster computing. It is usually enabled by interactive shells similar to those such as MATLAB or R. It is particularly suitable for interactive data mining of large data sets in clusters [23]. 2.5. Data lake It is the central repository for all organization data (without predefined data schema). The goal is to collect all the data before they are potentially lost. It provides data consolidation and a highly customizable analytical . The lake consists of a distributed, scalable file system (HDFS or Amazon S3), and one or more dedicated processing and query tools such as Apache Spark, Drill, Impala or Presto [24]. Upgrading the existing data warehouse with a data lake (creating hybrid architecture) is a positive change for businesses. Advantages of increasing data warehouse include:  Great savings in storage costs - scaling architectures (e.g. Hadoop, AWS S3) can store non-processed data in any format at a much lower cost than data warehouses.  Significantly accelerating processing - flexible data lake architecture enables faster data loading and parallel processing, resulting in faster instant analytical insight.  Maximizing efficiency - spending less time on low-value business activities (ETL, for example) and making better of resources for strategic goals of high business value.  Getting more valuable business insights, faster and of a larger amount of data (lower storage costs allow you to store more data, leading to more accurate trends, better forecasts, etc.) [25]. Comparison of the classical data warehouse and data lake, on research data from [25][26] is given in Table 1. Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 720 Table 1: Comparison of a classic data warehouse and a data lake. Data warehouse comparison of Data lake structured, processed, data preparation requires IT department assistance, administrator permissions etc. data and data preparation structured, unstructured, in its source (raw) form, provide self-service ad-hoc transformation without administrator permissions the data were processed before being entered in the data warehouse processing the data is in the original format and is processed as needed on large databases, very expensive for large amounts of data storage designed for a large amount of data at a low cost fixed configuration flexibility flexible, possible reconfiguration as needed mature and steady security in development business professionals for whom it is intended? data scientists, data scientists, data engineers moderate scaling but with high-cost scaling large scaling at a low cost efficiently utilizes storage and processing capabilities but with high cost cost / efficiency efficiently utilizes storage and processing capabilities at a low cost easily manage the quality and security of data data management requires an to create metadata for raising quality, security and privacy Russom’s research [12] shows that out of 225 respondents, 90% of them know the concept of data lakes, 24% think that Hadoop data lake has a really high impact on the success of data management strategies in their company, 32% consider the impact is moderate, while 44% of respondents do not even consider this question at all. The users consider the following items (in terms of consequences) as the ones that would benefit the most of data lake implementation (Table 2). Table 2 data lake's implementation positive impact [12] Advanced Analytics (data mining, Machine Learning, Complex SQL) 49% Data Exploration and Knowledge Discovery 49% Sources of big data for analytics 45% Data warehouse widening 39% Data retention for storage 36% Reducing Cost of Data Storage 34% A possibility of the data also for nontechnical types 24% To accept unstructured data 21% As barriers to the implementation of Hadoop data lakes, users have often reported a series of reasons related to data management, security and lack of knowledge and skills related to Hadoop and big data technology. 2.6. Other elements of a modern data architecture Analytics / Sandbox Environments - This environment almost completely contradicts the easy-to-manage BI / DW environment of a predictable workload that supports classical managerial reporting of a type "what's happened" with business questions. It represents a research environment that have very unpredictable burden and usage patterns. In this environment data scientists should have the freedom of experiment with data types from different sources, methods of data transformations, and analytical models to get valuable insights from data and build predictable business models. It's too manageable and allows data scientists to any tools that prefer research, analysis, and analytical modelling [27]. Data Lab - Big data and advanced analytics require different technologies and approaches. The analysis may require data that are not available in the warehouse. Models can be CPU-intensive and create problems for other applications at the same time. There may be conflicts between a warehouse administrator who wants a carefully controlled environment and analysts, especially data scientists who want maximum flexibility. That is why data labs can be created where users can make changes, unlike in original storage tables. The Data Warehouse Administrator creates certain lab owners for specific areas, such as marketing and sales. Each lab owner identifies people who have to a data lab. The Data Store Administrator in collaboration with the users determines how much the workspace is allocated to each lab and sets the expiration date [27]. 3. RESULTS AND DISCUSSION on changes and challenges reported in previous in area related to modern data management, a visualised and summarized overview and a more detailed insight into the modern data architecture is given in 1. It starts with the general data management cycle (with some modern architecture elements added). Then the possible benefits from Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 721 introducing modern data platforms’ elements, a of possible strategies and implementation solutions are discussed. At the end of this chapter, detailed big data processing phases are described in detail, along with descriptions of basic processes, and a description of software support, needed for their realization. 3.1. Data management context in big data era In order to show a wider context related to data management in big data era, a synthetic graphical overview of the performance management cycle of an enterprise is given. A conceptual model, i.e. a framework ( 1), has been developed that specifies the components, that is, the areas that participate in the efficient business of the company. Model is separated into three interconnected areas or subsystems. A subsystem representing internal or external data and/or systems generating such data. A subsystem for "preparing" and storing data (cleaning, consolidating, structuring, aggregating, storing) to serve the company's analytical needs as efficiently as possible, and an analytic subsystem where data is "exploited" for the implementation of previously defined goals. In the last subsystem, there are a of activities in which a whole spectrum of different tools is to extract the potential of different types of data. The need for continuity of running such performance management cycles is stressed, so that the company moderates goals, KPIs, and selects the tools that deliver the best results. Storage methods and analytical tools and methods are divided into traditional BIs (storage in data warehouses and analytical processes mostly over structured data) and on big data typical storage and analytical processes (data lakes and sandboxes). For each analytical category there is a full range of techniques and tools [28] [29]. 1: Business Performance Management Cycle 3.2. Synthesis of big data processing phases Big data Processing can be seen as six dependent phases. First, data are generated in different applications and systems (internal and external data in different formats and structures). The second phase includes all steps to download generated data from different sources (web scraping, web crawling, APIs ...). The third phase involves cleaning and converting data from multiple sources processing. Today, data management systems are expected to be able to process data in -time (streaming data) and batch-aggregate processing. This means that the dynamics must also adjust the processes of preparation, cleaning, and other data transformation actions. The fourth phase is storage - usually, all data types are permanently stored in some type of the file system or database, or they combine local storage with cloud storage services. In the fifth phase, different tools, methods and techniques, for analysing and data are to obtain information important for supporting business activities and making decisions. While the data warehouse is to process and analyse structured data, the Hadoop cluster is for processing and transforming unstructured and semi-structured into structured data. For analytical processes, the Hadoop ecosystem has multiple extensions for queries, data processing, storage in NoSQL databases (e.g. HBase), data warehouses (e.g. Hive) and advanced data mining and machine learning algorithms. The Sixth Phase - the information and results obtained from analysis phase should be visually presented, assigned, distributed and presented to its users in the final phase [30]. All the phases are also synthesized visually ( on data from [31], [32] and [33]) on a 2. Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 722 2: Synthesis of the big data processing phases 4. CONCLUSION Traditional data management architectures cannot meet the current needs of companies for integrating and analysing a wide range of data types generated from a variety of sources. The modern data platform enables analytical processing of both historical data and in time, and for the structured, semi-structured and unstructured data, clouded or locally stored. The big data technologies are complementary to existing data management technologies and serve to manage, process, and analyse types and forms of data that are not supported in standard BI / DW systems. Therefore, data management platforms complement and optimize the existing ones. This paper presents the context of data management in big data era, provides a review of technologies, concepts, platforms in modern data management architecture. Companies should realize the benefits and problems of big data technology, but above all, should understand their needs to successfully implement those technologies that help them achieve successful business goals. Whereas much of the big data is stored the Hadoop File System (HDFS) and on distributed computing platforms that support Hadoop clusters, most companies need to be thoroughly informed about the technologies within the Hadoop ecosystems related to all the above-mentioned big data processing phases. In terms of assistance and aid during that process, an overview of some of the possible solutions is provided in 2 and it provides a general and broad overview of the big data technology. In order to create an optimal strategy and appropriate software selection, a skilled team is needed with a spectrum of knowledge and competencies in data management, a modern data architecture, big data technologies, and also business domain experts. The following research activities are directed toward identifying employees' profiles and knowledge, competencies and capabilities requirements. Subsequent research activities will aim at adjusting the study programs in higher education structures in order to meet the companies' demands for management and decision making in large data and IoT era. REFERENCES 1. I. A. Atoum and N. A. -Jarallah, Big data analytics for value- care: Challenges and opportunities, Int. J. Adv. Trends Comput. Sci. Eng., vol. 8, no. 6, . 3012–3016, 2019. https://doi.org/10.30534/ijatcse/2019/55862019 2. I. P. Popchev and D. A. Orozova, Towards Big Data Analytics in the e-Learning Space, Cybern. Inf. Technol., vol. 19, no. 3, . 16–24, 2019. 3. J. Campos, P. Sharma, U. Gorostegui Gabiria, E. Jantunen, and D. Baglee, A big data analytical architecture for the Asset Management, Procedia CIRP 64, . 369 – 374, 2017. https://doi.org/10.1016/j.procir.2017.03.019 4. J. Morris, Top 10 categories for Big Data sources and mining technologies, 2012. [Online]. Available: https://www.zdnet.com/article/top-10-categories-for-big
-data-sources-and-mining-technologies/. 5. V. Mayer-Schönberger and K. Cukier, Big data: A revolution that will transform how we live, work, and think. York: Houghton Mifflin Harcourt, 2013. 6. B. Butler, Cloud Cronicles, 2015. [Online]. Available: http://www.networkworld.com/article/2973963/big-data
-business-intelligence/5-problems-with-big-data.html. 7. D. Ahamad, M. Akhtar, and S. A. Hameed, A review and analysis of big data and mapreduce, Int. J. Adv. Trends Comput. Sci. Eng., vol. 8, no. 1, . 1–3, 2019. 8. C. Ji, Y. Li, W. Qiu, U. Awada, and K. Li, Big data processing in cloud computing environments, in International Symposium on Pervasive Systems, Algorithms and Networks, 2012. Sabrina Šuman ., International Journal of Advanced Trends in Computer Science and Engineering, 9(1), January – February 2020, 717 – 723 723 9. P. Amarendra Reddy and O. Ramesh, Security mechanisms leveraged to overcome the effects of big data characteristics, Int. J. Adv. Trends Comput. Sci. Eng., vol. 8, no. 2, . 312–316, 2019. 10. A. -Drees, R. Bin-Hezam, R. -Muwayshir, and W. Haddoush, Unified Retrieval Model of Big Data, in Advances in Big Data Proceedings of the 2nd INNS Conference on Big Data, October 23–25, 2016. 11. D. Zhu, Y. Zhang, X. Wang, and E. ., Research on the methodology of technology innovation management with big data, Sci. Sci. Manag. S. T., vol. 4, . 172–180, 2013. 12. P. Russom, Data Lakes Purposes, Practices, Patterns, and Platforms, 2017. 13. B. Inmon, Big Data Implementation vs. Data Warehousing, 2013. [Online]. Available: http://www.b-eye-network.com/view/17017. 14. C. Russell, Database Development with IBM Hybrid Data Architecture, 2017. [Online]. Available: http://www.ibm.com/developerworks/. 15. D. Marjanović, Hadoop i analitika u realnom vremenu, 2017. [Online]. Available: http://www.datascience.rs/Hadoop-i-analitika-u-realno-
vremenu/. 16. V. Dagade, M. Lagali, S. Avadhani, and P. Kalekar, Big Data Weather Analytics Hadoop, Int. J. Emerg. Technol. Comput. Sci. Electron., vol. 14, no. 2, 2015. 17. N. Garg, S. Singla, and S. Jangra, Challenges and Techniques for Testing of Big Data, Procedia Comput. Sci., vol. 85, . 940–948, 2016. 18. D. Stodder, Strategies for Visual Big Data Analytics, How organizations can apply modern data platform technologies and practices to support analytics innovation, 2017. 19. S. Alapati, Expert Hadoop Administration Managing Tuning, and Securing Spark,Yarn and HDFS. Addison- Wesley, 24-25, 2017. 20. MapReduce, 2019. [Online]. Available: https://www.ibm.com/analytics/hadoop/mapreduce. 21. R. Alapati, S., Expert hadoop administration managing, tuning, and securing Spark, Yarn,and HDFS. Addison- Wesley, 149-151,2017. 22. A. Bekker, Spark vs. Hadoop MapReduce: Which big data framework to choose, 2017. [Online]. Available: https://www.scnsoft.com/blog/spark-vs-hadoop-mapred
uce. 23. T. Oktay and A. Sayar, Analyzing Big Security Logs in Cluster with Apache Spark, in Advances in Big Data, Advances in Intelligent Systems and Computing, Angelov ., Ed. 2016. https://doi.org/10.1007/978-3-319-47898-2_14 24. J. Caserta and E. Cordo, Data Warehousing in the Era of Big Data, 2016. [Online]. Available: http://www.dbta.com/BigDataQuarterly/Articles/Data-
Warehousing-in-the-Era-of-Big-Data-108590.aspx. 25. B. Sharma, Architecting Data Lakes: Data Management Architectures for Advanced Business Cases. O’Reilly Media, 2018. 26. M. Knight, Data Warehouse vs. Data Lake Technology: Different Approaches to Managing Data, 2017. [Online]. Available: https://www.dataversity.net/data-warehouse-vs-data-lak
e-technology-different-approaches-managing-data/. 27. J. Watson,H., Data Lakes, Data Labs, and Sandboxes, Bus. Intell. J., vol. 20, no. 1, 2015. 28. S. Šuman and I. Pogarčić, Development of ERP and other large business systems in the context of trends and Technologies, in 27th Daaam International Symposium On Intelligent Manufacturing And Automation, 2016, . 319–327. 29. S. Šuman, Sustavi poslovne inteligencije - teorija i riješeni primjeri. Rijeka: Veleučilište U Rijeci, 2017. 30. Heilig,L. and S. Voß, Managing Cloud- Big Data Platforms: A Reference Architecture and Cost Perspective, in Big Data Management, B. García Márquez,F.,P., Lev, Ed. Springer International Publishing AG, p.29, 2017. 31. W. El Kaim, Big Data Architecture, 2016. [Online]. Available:https://www.slideshare.net/welkaim/big-data-
architecture-part-2. 32. K. Singh, Top 10 Big Data Tools in 2019, 2019. [Online]. Available: https://dimensionless.in/ top-10-big-data-tools-in-2019/. 33. G. Ginde, R. Aedula, S. Saha, A. Mathur, S. Roy Dey, S. Sampatrao, G., and D. Sagar, Big Data Acquisition, Preparation, and Analysis Apache Software Foundation Tools, in Big Data Analytics Tools and Technology for Effective Planning, D. Somani, A.K., Ganesh, C., Ed. Boca Raton: CRC Press Taylor & Francis Group, 2018. https://doi.org/10.1201/b21822-9 This item is the archived peer-reviewed author-version of:
Keeping the data lake in form : proximity mining for pre-filtering schema matching
Reference:
Alserafi Ayman, Abello Alberto, Romero Oscar, Calders Toon.- Keeping the data lake in form : proximity mining for pre-filtering schema matching
ACM transactions on information systems / Association for Computing Machinery - ISSN 1046-8188 - 38:3(2020), 26 Full (Publisher's DOI): https://doi.org/10.1145/3388870 To cite this reference: https://hdl.handle.net/10067/1742940151162165141
Institutional repository IRUA Keeping the Data Lake in Form: Proximity
Mining for Pre-filtering Schema Matching
Ayman Alserafi
1,2
, Alberto Abell ́o
1
, Oscar Romero
1
, and Toon Calders
3
1
Universitat Polit`ecnica de Catalunya - BarcelonaTech, Barcelona, Catalunya, Spain
{alserafi,aabello,oromero}@essi.upc.edu
2
Universit ́e Libre de Bruxelles (ULB), Brussels, Belgium
3
Universiteit Antwerpen (UAntwerp), Antwerp, Belgium
toon.calders@uantwerp.be
Abstract.Data Lakes (DLs) are large repositories of raw datasets from
disparate sources. As more datasets are ingested into a DL, there isan
increasing need for efficient techniques to profile them and to detect the
relationships among their schemata, commonly known asholistic schema
matching. Schema matching detects similarity between the information
stored in the datasets to support information discovery and retrieval.
Currently, this is computationally expensive with the volumeof state-of-
the-art DLs. To handle this challenge, we propose a novel early-pruning to improve efficiency, where we collect different types ofcontent
metadataandschema metadataabout the datasets, and then this
metadata in early-pruning steps to pre-filter theschema matchingcom-
parisons. This involves computing proximities between datasets on their metadata, discovering their relationships on overall prox-
imities and proposing similar dataset pairs for schema matching. We
improve the effectiveness of this task by introducing a supervised mining for effectively detecting similar datasets which are proposed
for further schema matching. We conduct extensive experiments on a
-world DL which proves the success of our in effectively
detecting similar datasets for schema matching, with recall rates of more
than 85% and efficiency improvements above 70%. We empirically show
the computational cost saving in space and time by applying our ap-
proach in comparison to instance- schema matching techniques.
Keywords:Data Lake Governance, Holistic Schema Matching, Content Meta-
data Management, Early-pruning, Dataset Similarity Mining
1 Introduction
Today, it is more and more common for data scientists to Data Lakes (DLs)
to store heterogeneous datasets coming from different sources in theirraw format
[35]. Such data repositories support the era of data analytics wheredatasets
are ingested in large amounts and are required to be analysed just-in-time [20].
However, it is a challenge for data wranglers [13,16,35] preparing the datasets
The final publication is available at ACM via http://dx.doi.org/10.1145/3388870 2A. Alserafi .
for analysis to understand their structure andcommonalitiesfor DL governance
purposes [4]. They must extract those datasets which have related data to be together in data analysis tasks [20,23]. This is commonly referred toas
schema matching, where the aim is to find connection strengths between similar
concepts from different pairs of datasets [8,10,21,27]. The large-scale application
of such a task to big data repositories DLs is referred to asholistic schema
matching[5,23,27,29], where the goal is to match multiple datasets together
considering them all in the matching task.
We focus on DLs having datasets storing data in flat tabular formats. Flat
datasets are organised as attributes and instances, such as tabular data, comma
separated values (CSV) files, hypertext markup language (HTML) tables, etc.
(see 3). It is a challenge with such DLs to efficiently process the datasets
to detect their common features, as schema matching tasks are generally ex-
pensive (involving huge amounts of string comparisons and mathematical cal-
culations) [7,8,17]. In this paper, we propose novel techniques to reduce those
comparisons usingpre-filteringtechniques that generate less comparisons. To
illustrate this, consider the different types of comparisons in Fig.1. Tradition-
ally, schema matching makes many comparisons of data values of instances from
different datasets (see the instance- matching box). With the rise of DLs,
previous research [5,12,7] recommended early-pruning steps to facilitate
the task usingschema matching pre-filtering. Here, only datasets detected to be
of relevantsimilarityare recommended for further fine-grained schema matching
tasks, and dissimilar ones are filtered out from further comparisons. Fig. 1 re-
flects this stratified that filters by means of extracted metadata at the
dataset and level before going for expensive instance-basedapproaches.
For example, consider a DL with 1000 datasets, with 15 attributes and 1000 in-
stances each. Since schema matching techniques generate
n∗(n−1)
2
comparisons,
it would result in 499500 comparisons at the dataset level, 113 million at the level and about 500 billion at the instance level. Clearly, fine-grained
comparisons do not scale and pre-filtering is necessary.
We propose a pre-filtering on different levels of granularity, at
which we collect metadata data profiling techniques. Our collects
metadata at two different levels: at the dataset and level. This metadata
is then in a supervised learning model to estimateproximityamong pairs
of datasets. Such proximity is then for pruning out pairsless likelyto be
related. This is illustrated in Fig. 1, where our goal is to filter candidate pairs
of datasets before conducting computationally intensive instance- schema
matching techniques. The scope of this paper is the early-pruningat the top
two granularity tiers. We refer the interested reader to previousresearch about
classical instance- schema matching which is outside the scope of this paper
[7,8,21,30,31].
It is a challenge to compute dataset similarities for pre-filtering tasks due to
the difficulty of finding adequate similarity metrics and features touse [5]. This
paper is an extension of our previous work [5], and it presents a novel proximity
4
4
In this paper, we proximity and similarity interchangeably. Proximity Mining for Pre-filtering Schema Matching3
Dataset-level matching
-level matching
Instance- matching
Content-
metadata
(Relationships)
Filtering
pairs to
generate less
comparisons
More
comparisons
needed
DS-Prox
(Dataset Proximity mining)
Dataset
Meta-
features
-Prox
( Proximity mining) Meta-
features
Schema matching
(string/semantic similarity) Values
Goal
Cost
Fig. 1: The stratified holistic schema matching at different levels of
granularity.
mining (see 4) between datasets. The similarity functions are on automatically extracted metadata and can be effectively for pre-
filtering in a stratified (see the DS-Prox and -Prox boxes
attached to the matching steps in Fig. 1). To our knowledge, no other uses automatically extracted metadata for this purpose.
To show the feasibility of our , we assess its performance by it
on a -world DL. Our was able to filter the of fine-grained
comparisons by about 75% while maintaining a recall rate of at least 85% after
filtration. Our early-pruning also saves computational costs interms
of space and time requirements by at least 2 orders of magnitude compared to
instance- matching.
Contributions.We present an for pre-filtering schema matching
tasks. We propose techniques for detecting similar schemata onmeta-
data at different levels of granularity. This supports in early-pruning of the
raw-data instance- schema matching tasks. We present an expanded and
cross-validated experiment for the DS-Prox technique from our previous work
[5] and comparisons against combining it with our proposed -level
proximity metrics to find the most appropriate metrics to assign similarities
between pairs of datasets. We demonstrate a detailed analysis of the different
proximity metrics on different types of meta-features (name- and
content-). Our improvements outperform our previous work in terms of
effectiveness measures recall and lift-scores.
The paper is organised as follows: 2 presents the related work, 3 introduces the main concepts in our research, 4 presents our
proximity mining for early-pruning tasks of holistic schema
matching, 5 presents our experimental evaluation, and finally,we con-
clude in 6. 4A. Alserafi .
2 Related Work
State-of-the-art schema matching techniques either schema-level metadata
(mainly names and data types of schema components) [10,11,25,27] or instance-
level values of data objects [8,10,12,14,21,25,33]. Some others a hybrid ap-
proach utilising schema metadata and data instances [7,28]. At the schema-level,
these techniques usually the syntactic similarity of the names of the schema
components for the matching task. At the instance-level, values are usually com-
pared Jaccard similarity of intersecting exact values [23]. This can also be
achieved by first matching duplicate instances and finding the correspondences
between their schema components [8]. Further, these algorithms canbe domain-
specific or generic [21].
Schema matching is a computationally intensive task that requires large
amounts of comparisons [4,5,7] because they typically generate a Cartesian prod-
uct between the values to be compared. Moreover, other approaches alsoexploit
the semantic or linguistic similarity of values, which requires further computa-
tions to translate data values (finding synonyms and hypernyms) or to map them
to ontologies [21].
The current focus of the schema matching research community is toimple-
ment efficient holistic schema matching that improves performanceby reducing
the of actual comparisons to conduct [23]. To handle this challenge,
multiple techniques were proposed. For example, several approachesuse cluster-
ing techniques as a pre-filter of datasets to match [3,6,27]. Only datasets falling
in the same cluster are matched, or datasets from one cluster are only matched
against representative datasets from other clusters. This is similarto the concept
of “blocking” for record linkage [32], where items having equal data values in
some or all of their attributes are placed in the same bucket for comparison. The
work in [32] focuses on matching instances of data (rows in tabular data) rather
than attributes in schemata (columns in tabular data). We propose in this paper
a supervised learning technique that can classify dataset pairs (schemata) for a
decision whether they are related (should be compared) or not related,rather
than unsupervised techniques blocking and clustering. Inaddition, we tackle
the similar schemata search problem (i.e., schema matching) ratherthan similar
records search (i.e., record linkage or entity resolution).
In [27], they cluster the schemata and attributes of datasets on TF-IDF
similarity scores of their textual descriptions. In [3], they exploit the structural
properties of semi-structured XML documents, i.e., data elements embeddings
and hierarchies, to cluster the datasets before applying instance- schema
matching. In [6], they the textual descriptions and keywords of the datasets
to cluster them TF-IDF and WordNet. In this paper, we do not con-
sider textual descriptions of datasets, which could be misleading ormissing, but
rely on metadata that can be automatically extracted from any dataset. Meta-
data describing datasets, their schemata, and the information stored in them
can be collected data profiling techniques [1,4,18,20,24]. Different types of
metadata can also describe the information inside datasets at differentlevels of Proximity Mining for Pre-filtering Schema Matching5
granularity, e.g., overall dataset level [5] or descriptionslike we propose
in this paper.
The pre-filtering of dataset pairs which are less-likely to have interrelated
data before performing schema-matching is called early-pruning [4,12,7], and it
was implemented in previous research on semi-structured datasets, XML, by
finding the similarity of hierarchical structures between named data objects [3].
Other works have investigated schema matching with semi-structured datasets XML [21,25] and JSON [14]. In the web of data, previous research [26]
investigated recommendation of RDF datasets in the semantic web usingpre-
defined annotations such as thesameAsproperty. In this paper, we consider
flat datasets without such a hierarchy of embedded data objects and without
pre-defined semantic linkages.
To facilitate the early-pruning tasks for schema matching, we can apply the
same approaches and concepts from collaborative filtering and adapt them to
the holistic schema matching problem [2,15]. The goal is to profilinginfor-
mation for comparison and recommendation, which was applied to multimedia
in [2] and semi-structured documents in [14]. Content- metadata was also to predict schema labels [11]. They minimum and maximum values for
numeric attributes, and exact values for nominal attributes, including the format
of values. We propose to apply similar techniques but at the dataset granularity
level. Accordingly, we adapt such techniques and find the appropriate similarity
metrics for tabular datasets.
Another line of research aims at optimising the schema matching process by computational improvements [12,28]. This can be done partitioning
techniques that parallelise the schema matching comparison task [28].Another uses efficient string matching comparisons. Such techniques are very
useful in the case when schema components are properly named in the datasets.
However, such techniques fail when the components are not properly named (e.g.,
internal conventionalism, sequential ID numbering of the components). In
[12], they introduce intelligent indexing techniques on value- signa-
tures.
Schema matching can also be automated data mining techniques [10,11,14].
In [10], they hybrid name- and value- classification models to
match dataset attributes to a mediated integration schema. Their is
focused on one-to-one mediation between two schemata, while our approachtar-
gets all datasets in a DL by holistic schema matching coarser meta-features.
In [11], they content- meta-features in multi-value classification mod-
els to match schema labels of attributes across datasets. Decision trees were
also to profile semi-structured documents before schema matching [14]. In
this paper, we also data mining classification models, however the goal dif-
fers from [10], [11] and [14] as it tackles the early-pruning and pre-filtering task
rather than instance- schema matching.
We summarise the state-of-the-art in Table 1. This table gives a compari-
son of the most relevant techniques discussed with our onthe
main features discussed in this . As a result, we can see thatwe propose 6A. Alserafi .
an not only on string matching, but also on content metadata
matching involving statistics and profiles of data stored in the datasets. Con-
tent metadata are matched on approximate similarities and not justexact
value-matches at the instance-level [20,23]. We focus on proposing novel early-
pruning techniques that supervised learning to pre-filterirrelevant dataset
pairs and to detect likely-to-be similar pairs. Finally, the table shows that our
technique makes a novel contribution to the schema matching pre-filtering prob-
lem that is not achieved by other state-of-the-art techniques.
Table 1: Schema matching techniques state-of-the-art comparison
COMA++
[28]
PARIS [33]LOD Data
Linking [6]
XML
Semantic- Matching
[17]
Ontology
Clustering
[3]
Proximity
Mining
[this
paper]
Type of DataTabular,
semi-
structured,
Semantic
OWL
RDFSemantic
RDF
Semi-
structured
XML
Semi-
structured,
Semantic
OWL
Tabular
Instance-basedXXXX××
Metadata usedAttribute-
level schema
names
×Ontology
mappings,
RDF schema
names,
Textual
descriptions
-
level schema
names and
structural
metadata
-
level struc-
tural meta-
data
Dataset-
level content
and name,
-
level content
and name
Data Mining ××Clustering×ClusteringSupervised
learning
Approximate
Matching
××X×XX
3 Preliminaries
Att1Att2Att3
Value 1a 0.25 Value 3a Value 1b 55.6Value 3b Value 1c 27.9Value 3c Value 1d 73.1Value 3d Attributes
Instances
Fig. 2: A flat dataset with attributes and instances. Proximity Mining for Pre-filtering Schema Matching7
We consider DLs with datasets having tabular schemas that are structured
as attributes and instances Fig. 2. We formally define a datasetDas a set
of instancesD={I
1
, I
2
, ...I
n
}. Each dataset has a schemaS={A
1
, A
2
, ...A
m
},
where each attributeA
i
has adata typeand describes a single property of the in-
stances in the dataset. We focus on two types of attributes:numeric attributes
(consisting of numbers) andnominal attributes(consisting of discrete cat-
egorical values). We differentiate between those two types of attributes, similar
to previous research [10,11,25], because we collect different profiling meta-
data for them. The resulting statistics collected are calledcontent meta-features,
and are as follows:
– Nominal attributes:frequency distributions of their distinct values.
– Numeric attributes:aggregated statistical value distributions mean,
min, max, and standard deviations.
For pairs of datasets and attributes, we compute the functions in Table 2
and describe them in the rest of this .
Table 2: Schema matching pre-filtering functions
RelationshipFunction TypeOutputObject TypeDescription
Rel(A
i
, A
j
)BinaryZ∈{0,1} pairRelated attributes storing data
about the same -life concept
which contain overlapping informa-
tion in their values. 1 means posi-
tively related and 0 means not.
Sim(A
i
, A
j
)ContinuousR∈[0,1] pairA valueRto measure the similarity in the range [0,1].
Rel(D
y
, D
z
)BinaryZ∈{0,1}Dataset pairRelated datasets which contain in-
formation about the same -life
object. 1 means positively related
and 0 means not.
Sim(D
y
, D
z
)ContinuousR∈[0,1]Dataset pairA valueRto measure the dataset
similarity in the range [0,1].
We aim at finding the relationship between a pair of datasetsRel(D
y
, D
z
).
We determine such relationship directly dataset-level meta-features and
by computing the relationships between their pairs of attributes (A
i
, A
j
), being
A
i
fromD
y
andA
j
fromD
z
, as could be seen in Fig. 3. The shows an
overview of our proposed proximity mining .
The goals of the is to efficient and effective techniques to
accurately predictRel(D
y
, D
z
) for the pre-filtering task. As could be seen in
Fig. 3, this can be done metadata and similarity collected at the dataset
level (right-side) or by the levelSim(A
i
, A
j
) to predict it (left-
side). The shows the steps required for combining -level similar-
ity with the dataset-level similarity to predictRel(D
y
, D
z
). Here, we only Sim(A
i
, A
j
) as an auxiliary step that supports us in the main task of predicting
Rel(D
y
, D
z
). This is possible because the metadata are of finer gran-
ularity which can be aggregated to a single similarity score at the dataset-level 8A. Alserafi .
-level
Proximity
Dataset-level
Proximity Content
Meta-features
Attributes Names
Dataset Content
Meta-features
Datasets Names
Levenshtein
distance
P
m
computation
Sim(A
i
,A
j
)
Agg
Levenshtein
distance
P
m
computation
M
cls-ds
M
cls-num-attr /
M
cls-nom-attr
Sim(D
y
,D
z
)
Rel(A
i
,A
j
)
Rel(D
y
,D
z
)
Fig. 3: The dependencies of components in the metadata- proximity mining for pre-filtering schema matching.
with an aggregation functionAgglike averagingSim(A
i
, A
j
) scores. When pre-
dictingRel(D
y
, D
z
), typically the dataset pair will have information contained in
some of their attributes which are partially overlapping, satisfyingRel(A
i
, A
j
),
where∃A
i
∈D
y
∧A
j
∈D
z
=⇒Rel(A
i
, A
j
) = 1. An example would be a
pair of datasets describing different human diseases, (e.g., diabetes and hyper-
tension). The datasets will have similar attributes (partially) overlapping their
information the patient’s age, gender, and some common lab tests likeblood
samples.
Theintermediate outputleading toRel(A
i
, A
j
) andRel(D
y
, D
z
) in our pro-
posed proximity- , seen in Fig. 3, is a similarity score consisting
of a in the range of [0,1], which we indicate usingSim(A
i
, A
j
) and
Sim(D
y
, D
z
) respectively.
The similarity scores are computed on proximity models we construct ensemble supervised learning techniques [34], which we denote asM
cls−ds
for models handling dataset-level metadata andM
cls−num−attr
orM
cls−nom−attr
for models handling -level metadata (depending on the type,
numerical or nominal respectively). The models take as input thedistancebe-
tween the meta-features describing content of each object pair, whether dataset
pair or pair forSim(D
y
, D
z
) andSim(A
i
, A
j
) respectively, and we call
the distance in a specific meta-feature ‘m’a proximity metricwhich is denoted
asP
D
m
(D
y
, D
z
) for dataset pairs orP
A
m
(A
i
, A
j
) for pairs. The names of Proximity Mining for Pre-filtering Schema Matching9
objects can also be compared Levenshtein string distance comparison [22]
to generate a distance score. The output from the models is a score we compute the positive class distribution (see 4.2).
We convert the intermediate similarity scores to thefinal outputconsisting
of a boolean value for the binary relationships Equations (1) and (2).The
simscore computed for each relationship type is checked against a minimum
threshold in the range of [0,1] to indicate whether the pair involved is overall
related ‘1’ or unrelated ‘0’, and therefore whether they should be proposed for
expensive schema matching or otherwise filtered out. cut-off thresholds
of similarity rankings for the collaborative filtering task and schema matching
is a common practice [12,15,20]. We can different thresholds ‘c
d
’ and ‘c
a
’
for each of the relationship evaluated at the dataset level and level
respectively. This means that we only consider a pairsimilarif their similarity
score is greater than the threshold as in Equations (1) and (2).
Rel(D
y
, D
z
) =
{
1, Sim(D
y
, D
z
)> c
d
0,otherwise
(1)Rel(A
i
, A
j
) =
{
1, Sim(A
i
, A
j
)> c
a
0,otherwise
(2)
To summarise Fig. 3, the hierarchy to compute the final output is:Relis onSimsimilarity scores, which in turn are onP
m
proximity metrics
of meta-features. To convert fromP
m
toSimwe an ensemble supervised
modelM
cls
which takes theP
m
proximity metrics as input. The outputSim
is compared against a minimum threshold, and those passing the threshold are
positive cases forRelto be considered for further detailed schema matching.
D
2
: census_dataD
3
: health_data
A6: type {f,m} A11: gender {female,male} A7: age { 0<A2<100} A13: age { 30<A3<60} A8: race {01,02,03,04} A12: Ethnicity {AS,AF,ER,LT} A9: Household { 0<A4<16} A14: Temp { 35<A4<42} A10: income { 50k<A5<300k} A15: H_rate{ 40<A5<160} Rel(D
2
,D
3
) = 1
D
1
: 1992_city_data A1: salary {25k<A1<600k} A2: age { 20<A2<97} A3: family_Size{ 2<A3<11} A4: identity {w,m,t}
A5: house_type {h,t,v,s,p,l} . . .
Rel(D
1
,D
2
) = 1
. . .
. . .
Rel(A
6
,A
11
) = 1
Rel(A
1
,A
10
) = 1
Fig. 4: Final output of our consisting of similarity relationshipsbetween
two pairs of datasets.
Examples. Consider the relationships between the three datasets in Fig. 4
which presents thefinal outputof our . Each dataset has a set of at-
tributes. An arrow links attributes having similar data profiles. We label this
as aRel(A
i
, A
j
) = 1. For example, attributes ‘A6’ and ‘A11’ fromD
2
andD
3
are nominal attributes with two unique values which we included as ameta-
feature called‘ of unique values’. The proximity metric is the distance
(difference) in the meta-feature of of unique values, whichin this case 10A. Alserafi .
P
A
m
= 0, because they are identical (i.e., 2−2 = 0), thus making the pair similar (in this case, by their of distinct values). Ifwe consider this
proximity metric of ‘ of unique values’ alongside other collected content- meta-features anensemble supervised learning modelM
cls
, we can
compute aSim(A
6
, A
11
) score on the positive-class distribution (see Sec-
tion 4.2). This can lead toSim(A
6
, A
11
) = 0.95 and if we a threshold of
c
a
= 0.75 then the final output forRel(A
6
, A
11
) = 1. A numeric ‘A7’ inD
2
holds similar data as attributes ‘A13’ and ‘A14’ fromD
3
, as ex-
pressed by the intersecting numeric ranges. For such numeric attributes we can
consider a meta-feature ‘mean value’. On the other hand, attributes ‘A1’
and ‘A7’ have different data profiles (different numeric ranges) and therefore are
not labelled with an arrow and do not satisfy theRel(A
1
, A
7
) relationship, as
they will have large differences in their meta-features, leading to high proximity
metric and a low similarity scores. In those examples, we collect level
meta-features from the datasets (in this case, the of distinct values for
nominal attributes and means for numeric attributes) to assess the similarity
between attributes of a given pair of datasets. In our , we computethe
similarity between attributesSim(A
i
, A
j
) proximity metrics
in the range of [0,1] and we it to predictRel(D
y
, D
z
) instead of the
binary output ofRel(A
i
, A
j
). We shouldaggregatethe individual pairs’
similarities with an aggregation functionaggto obtain a single value proximity
metric for the overall dataset level similarity. We discuss this in the description
of our in 4.
Furthermore, we extract higher-granularity dataset level meta-features (e.g.,
‘ of attributes per type’) from the datasets for thetask of di-
rectly computing theSim(D
y
, D
z
) similarity relationships. For example,Rel(D
2
, D
3
)
returns ‘1’ in the case we usec
a
= 0.67 because they have 2 nominal and 3 nu-
meric attributes each, so overall they can haveSim(D
2
, D
3
) = 0.7 passing the
minimum threshold. onRel(D
2
, D
3
) =‘1’, our indicates that
these two datasets are possibly related and should be considered for further
scrutinising by schema matching.
4 : Metadata- Proximity Mining for
Pre-filtering Schema Matching
Our goal is to effectively apply early-pruning for holistic schema matching in
a DL setting. Such pre-filtering is on novel proximity mining techniques.
Those techniques evaluate similarity among pairs of datasets automatically
extracted meta-features and utilising a data-mining ensemble supervised model
to select highly similar pairs. We apply this the stratified (Fig.
1). An overview of the is summarised in Fig. 3, which shows the steps
required to compute the schema matching pre-filtering functions from Table 2.
We explain how we build and apply those models in this .
In the remaining subsections, we describe the details of our ap-
proach as follows:profile the datasets to extract the meta-features and pair- Proximity Mining for Pre-filtering Schema Matching11
wise proximity metrics inSubsection 4.1, supervised proximity mining to
build the ensemble models forRel(A
i
, A
j
) andRel(D
y
, D
z
) inSubsection 4.2,
then apply the models on pairs of attributes and datasets in the DL to com-
pute theirSim(A
i
, A
j
) andSim(D
y
, D
z
) scores inSubsection 4.2, and finally theSim(D
y
, D
z
) to predictRel(D
y
, D
z
) for pairs of datasets and applying
the pre-filtering step for schema matching inSubsection 4.3.
4.1 Proximity Metrics: Meta-features distances
Our gathers metadata at two levels of granularity: at theI. dataset
levelandII. level. Further, at each of these levels, we gatherA.
content-basedmeta-features with profiling statistics andB. name-
meta-features with the naming of datasets and their attributes. The name-
techniques are the most commonly metadata in previous research [14,30,31].
We propose other content- meta-features at the two levels of granularity
as follows:
–Dataset level (DS-Prox):We collect overall meta-features summarising
the dataset content: overall statistics concerning all the attributes collec-
tively, the types found and the overall of instances. The
meta-features are described in our previous work [5], which includes a
detailed list of meta-features that proved to be effective in predicting related
datasets for schema matching pre-filtering, e.g., of instances, of attributes per type, dimensionality, of missing values,
etc.
– level (-Prox):The set of meta-features for both
types of attributes, nominal and numeric, is described in Table 3. Foreach
attributeA
i
in datasetD, we profile it on its type by computing the
appropriate features.
Table 3: level content meta-features TypeMeta-featureDescription
AlldistinctvaluescntThe of distinct values
AlldistinctvaluespctThe percentage of the distinct values from of instances
AllmissingvaluespctThe percentage of missing values from of instances
NominalvalsizeavgThe average of strings in values from the NominalvalsizeminThe minimum of strings in values from the NominalvalsizemaxThe maximum of strings in values from the NominalvalsizestdThe standard deviation of of strings in values from the NominalvalpctmedianThe median percentage of instances per each value of the NominalvalpctminThe minimum percentage of instances per each value of the NominalvalpctmaxThe maximum percentage of instances per each value of the NominalvalpctstdThe standard deviation of the percentage of instances per each value of the NumericmeanThe mean numeric value of the NumericstdThe standard deviation of the numeric value of the NumericminvalThe minimum numeric value of the NumericmaxvalThe maximum numeric value of the NumericrangevalThe numeric range of the values of the NumericcoofvarThe numeric coefficient of variance of the Equation 3 shows the proximity metric computed for a pair of attributes (or
datasets), denoted asO
i
,O
j
. the meta-features described, we compute the 12A. Alserafi .
z-score distance for each meta-featurem. The result is a ,P
m
. The
z-score is a normalisation where we the mean ‘μ’ and standard deviation ‘σ’
of each meta-feature considering its value from all datasets in the DL. A value
of 0 is the most similar, while larger negative or positive means more
different. The z-score is to standardise the comparisons of attributes in a
holistic manner that considers all datasets and attributes in the DL. Most pairs
of attributes and dataset will have a value falling in the range of [−3,3].
P
m
=zscore
distance(O
i
, O
j
) =
∣
∣
∣
∣
m(O
i
)−μ
σ
−
m(O
j
)−μ
σ
∣
∣
∣
∣
(3)
For the name- metadata we compute the proximity metricP
m
with a
Levenshtein string comparison [22], as in Equation 4.
P
m
=levenshtein
distance(name(O
i
), name(O
j
))(4)
4.2 Supervised Proximity Mining
After the different proximity metrics of meta-features are generated by profil-
ing the datasets, arepresentative sample(an adequate sample size should be
similar to the sample in our experiments in 5) of dataset and pairs should be selected by a human annotator and should be labelled whether
they satisfyRel(D
y
, A
z
) andRel(A
i
, A
j
) respectively. The dataset and pairs with their proximity metrics and labels are fed to a supervised learning
algorithm to create a proximity scoring model. We propose supervised ensem-
ble models on different dataset level and level proximity metrics
for computing overall similarity between pairs of datasets. The models decide
on the of attributes to consider in order to evaluate a pair of datasets
as ‘related’ by different aggregation functionsaggfor the level
metrics, giving different weights to a different of linkages of
different similarity ranks. This will be explained in detail in this .
Our builds supervised ensemble modelsM
cls−ds
forRel(D
y
, D
z
),
andM
cls−nom−attr
&M
cls−num−attr
forRel(A
i
, A
j
) whether the type
is nominal or numerical respectively. Model- learning for pre-filtering has
been applied before in the collaborative filtering field [2]. In such scenarios, item
pairs are recommended or filtered out model- learning algorithms
where a learnt model is to assign the similarities and rankings ofitem pairs on previously annotated examples. We give details of how we learn the
models and how we them in our in the subsections below.
Building the models from annotated samplesAn overview process for
building the supervised models in our can be seen in Fig. 5. In the
buildphase, we take the pairs of datasets and profile them by computing their
dataset level and level meta-features, followed by computing the prox-
imity metrics for those extracted features. We take different dataset pair samples
for building the level models and the dataset level models as seen in
the split into samples OML01 and OML02 (how to build such samples is given in Proximity Mining for Pre-filtering Schema Matching13
Pairs
of DS
Data
Lake
Profile -
level Meta-
features
Build Dataset-level Classification Model
M
cls_ds
DS Pairs
Training
Sample
Aggregate -
level proximity to
dataset-level
Agg Pairs
Training
Sample
Sample
OML01
Sample
OML02
Profile -
level Meta-
features
Apply -level
Proximity Model
Sim(A
i
,A
j
)
Profile Dataset-level
Meta-features
-level
profiling
Dataset-level
profiling
Build -level Classification Models
M
cls_num_attr
M
cls_nom_attr
Evaluate
-level
Classifiers
Rel(A
i
,A
j
)
Evaluate
Dataset-level
Classifier
Rel(D
y
,D
z
)
Compute
-level
meta-feature
proximity metrics
P
m
Compute
-level
meta-feature
proximity metrics
P
m
Compute Dataset-
level meta-feature
proximity metrics
P
m
Fig. 5: An overview of the process tobuildthe supervised ensemble models in
our proposed datasets proximity mining previously manually
annotated dataset pairs. 5.1). The pairs in sample OML01 should be already annotated to indicate
whether their attributes match (i.e.,Rel(A
i
, A
j
) for level models) or
whether the datasets are relevant for schema matching or not in sample OML02
(i.e.,Rel(D
y
, D
z
) for dataset level models). Initially, we start with the level supervised learning procedure as it is only an auxiliary subcomponent for the dataset level, where an aggregation step is to compute dataset level
proximities. First, we divide the pairs into training and test sets, we train a su-
pervised learning ensemble model for each type (nominaland numeric
types) the training sample, and we test the performance of themodel on
the test set (evaluation distinguished by dotted lines and circlesin the ).
We conduct this test to guarantee that the models generated are accuratein
detectingRel(A
i
, A
j
). Similarly, we do the same with the dataset level super-
vised models which generateRel(D
y
, D
z
). We the dataset level proximity
metrics and the level aggregated proximity metrics together to train
a supervised model a training sub-sample of dataset pairs fromOML02.
Finally, we evaluate the generated dataset level supervised modelsto guarantee
their accuracy in detectingRel(D
y
, D
z
).
Supervised learning.To build the models, we classical supervised
learning to create the proximity models. The meta-features are as input to
the models as seen in Fig. 6, where an object could be an for -
level models or a dataset for dataset-level models. First, for each object we
extract its meta-features (i.e., ‘m1’, ‘m2’, ...). Then, for each object, we gener-
ate all pairs with each of the other objects and compute the proximity metrics
between their meta-features either Equation 3 for content- meta-
features or Equation 4 for the name- comparison. We then take a sample
of pairs of objects which are analysed by a data analyst; a human-annotator
who manually decides whether the pairs of objects satisfy (assign ‘1’)or not
(assign ‘0’) theRelproperties (see 3). This can be achieved by simply
labelling the objects with their respective subject-areas and those falling under
the same one are annotated as positively matching ‘1’, otherwise all othersare 14A. Alserafi .
OB 1
Object
OB 1
Object
OB 2
Object
OB n
...
Extract Meta-Features
Profile 1Profile 2Profile n
m1
m2
...
10
320
m1
m2
...
22
1200
m1
m2
...
20
550
Annotate Pairs
Rel(1,2) = '1'
OB n
OB n
OB 2
OB 2
OB 1
OB 1
Rel(1,n) = '0'
Rel(2,n) = '1'
Compute distances
per pair
m1 = 0.55
m2 = 0.73
OB n
OB n
OB 2
OB 2
OB 1
...
m1 = 0.50
m2 = 0.42
...
m1 = 0.09
m2 = 0.54
...
Generate Training Set
Tuples
Run Ensemble Learning
Algorithm
OB 1
OB 1
OB 2
OB 2
OB n
OB n
Rel(ob1,ob2)
Classifier
0.55
0.50
0.09
m1 m2
0.73
0.42
0.54
...
...
...
...
Rel
+
-
+
M
cls
Extract Meta-Features
Extract Meta-Features
Fig. 6: Proximity Mining: supervised machine learning for predicting related data
objects.
labelled with ‘0’ (see 5.1). We then supervised learning techniques
and 10-fold cross-validation over the proximity metrics to create twotypes of
independent models which can classify pairs of datasets or pairs of attributes
according toRel(D
y
, D
z
) andRel(A
i
, A
j
) respectively. This is the final output
consisting of the two auxiliary supervised modelsM
cls−nom−attr
,M
cls−num−attr
forRel(A
i
, A
j
) and the main dataset level modelM
cls−ds
forRel(D
y
, D
z
). The
positive-class distribution from the generated models is toscorenew pairs
of objects (unseen in the training process) with a similarity scoreSim(D
y
, D
z
)
usingM
cls−ds
, andSim(A
i
, A
j
) usingM
cls−nom−attr
orM
cls−num−attr
.
We a random forest ensemble algorithm [9,34] to train the supervised
models in predicting related and dataset pairs as it is one ofthe most
successful supervised learning techniques. The algorithm generates a similarity
score on the positive-class distribution (i.e., the predicted probability of
the positive-class on weighted averages of votes for the positive class from
all the sub-models in the ensemble model) to generate a score in [0,1] forSim.
For example, if Random Forest generates 1000 decision trees, and for a pair of
datasets [D
y
, D
z
] we get 900 trees vote positive forRel(D
y
, D
z
) then we get
900
1000
= 0.9 forSim(D
y
, D
z
) score.
We feed the supervised learning algorithm the normalised proximity metrics
of the meta-features for pairs of datasets [D
y
, D
z
]. For level meta-
features, we feed theM
cls−ds
model with all the different aggregations of the
meta-features after computing their normalised proximity metrics (i.e., after
applying Equation 7, which we describe later in this ). Proximity Mining for Pre-filtering Schema Matching15
ALGORITHM 1: Level Top-Similarity Matching
Input:Sets of the meta-features of each typeAtt
nominal
andAtt
numeric
containing
the proximity metrics of the meta-features of each in the pair{A
i
, A
j
}for
each dataset in the pair{D
y
, D
z
}, the modelM
cls−nom−attr
for nominal attributes,
the modelM
cls−num−attr
for numeric attributes, an aggregation functionAggfor
aggregating links to compute dataset level proximity
Output:The partially ordered setSPof proximity metricsP
D
m
(D
y
, D
z
) for each pair of
{D
y
, D
z
}
SP
dataset
←∅;
SP ←∅;
SP
top ←∅;
foreach{D
y
, D
z
}⊂DLandy6=zdo
foreach{A
i
, A
j
}⊂Att
nominal
andA
i
∈D
y
andA
j
∈D
z
do
Sim(A
i
, A
j
) =M
cls−nom−attr
(A
i
, A
j
);
SP ←SP ∪{[A
i
, A
j
, Sim(A
i
, A
j
)]};
end
foreach{A
i
, A
j
}⊂Att
numeric
andA
i
∈D
y
andA
j
∈D
z
do
Sim(A
i
, A
j
) =M
cls−num−attr
(A
i
, A
j
);
SP ←SP ∪{[A
i
, A
j
, Sim(A
i
, A
j
)]};
end
\\Iterate on the set of pairsSP to find top matching pairs
whilemore pairs{A
i
, A
j
}can be pickeddo
Pick pair{A
i
, A
j
}fromSP that maximisesSim(A
i
, A
j
) whereA
i
andA
j
were not picked before;
SP
top ←SP
topattribute
∪{[A
i
, A
j
, Sim(A
i
, A
j
)]};
end
P
D
m
(D
y
, D
z
)←Agg(SP
top );
SP
dataset
←SP
dataset
∪{[D
y
, D
z
, P
D
m
(D
y
, D
z
)]};
end
-level proximity.To compute the overall proximity of datasets their level meta-features we Algorithm 1, whichfirst com-
pares the proximity metrics from the meta-features of each of a specific
type against all other attributes of the same type in the other dataset M
cls−nom−attr
for nominal attributes andM
cls−num−attr
for numeric attributes.
The algorithm then finds top matching pairs where we match each to the most similar in the other dataset agreedy ap-
proach[19]. For each pair of datasets, we match each only once (we
do not allow many-to-many matching). We rank pairs by proximity
top-to-least, then we assign matching pairs on the top of the list where each did not appear in a previous higher ranking pair (i.e., both attributes
need to be unmatched by any higher ranking pair in the list, otherwise the -
gorithm skips to the next pair until all the list of pairs is traversed). Finally, in
order to summarise the linkages to predict the overall proximity of the
dataset pairs, we compute anaggregationof the top-matching linkages
found between a pair of datasets anfunctionAggto convert the multi-
pleSim(A
i
, A
j
) scores to a single proximity metricP
D
m
for their dataset pair.
We different types of level aggregation functions. Those functions
assign different weights ‘W’ (which is an indicator of relevance, a bigger weight
means more relevant) to the links to consider. The different aggrega-
tions should have the goal of giving less weight to links which could be
considered asnoise; i.e., those pairs which are too strongly correlated without 16A. Alserafi .
any meaning (e.g., discrete ID numbers) or those pairs with too low
proximity to be significant.
Thus, the top-matching pairs of attributes are sorted by proximity weights
and are fed to the aggregation which allocates a weight between [0,1]
for aggregation in the summation of weights. The total sum of weights should
add up to 1.0. The different aggregations we are as follows:
–Minimum:we allocate all the weight (i.e.,W= 1.0) to the single pair link with the minimum similarity, and we consider this as the overall
proximity between the dataset pair. Therefore, all top-matching pair links need to have a high similarity score to result into a highproximity
for a dataset pair.
–Maximum:we allocate all the weight (i.e.,W= 1.0) to the single pair link with the maximum similarity, and we consider this as the overall
proximity between the dataset pair. Therefore, only one top-matching at-
tribute pair link needs to have a high similarity score to result into a high
proximity for a dataset pair.
–Euclidean:a Euclidean aggregation of the similaritiesSimof all matching
pairs of attributes without any weighting as in Equation 5. Here we consider
all the pair links in the aggregation and we assign equal weights to
all the links.
P
D
m
=
√
√
√
√
n
∑
i=1,j=2
[Sim(A
i
, A
j
)]
2
(5)
–Average:a standard averaging aggregation of the pairs of attributes without
any weighting, where all links are equally weighted in the average.
–Weighted :a normal distribution to assign different prox-
imity weightsWfor all linkages found, and then summing up all
the weighted similarities as the overall proximity as in Equation 6.
P
D
m
=
n
∑
i=1,j=2
[W
i
∗Sim(A
i
, A
j
)](6)
This is visualised in Fig. 7. Here the weight 0.0≤W≤1.0 for each top-
matching linkage is assigned on ordering the linkages top-
to-least in terms of their similarity scores, and the weight allocatedvaries
according to a normal distribution. We different p-parameters (proba-
bility of success) of{0.1,0.25,0.5,0.75,0.9}, where a parameter of 0.5 leads
to a standard normal distribution of weights allocated for the sorted pairs
of attributes. A lower parameter value leads to skewness to the left, allo-
cating more weight to highly related pairs, and a higher parameter leads to
skewness to the right, allocating higher weights to pairs with lower ranked
relationships. This means that with lowerpwe expect similar datasets to
have a few very similar attributes and a higherpvalue means we expect
most of the attributes to be strongly similar. Proximity Mining for Pre-filtering Schema Matching17
Assigned Weight
Rank of Top Matching Pairs
p = 0.75
p = 0.50
p = 0.25
p = 0.90
p = 0.10
Fig. 7: Different normal distributions for assigning weights to rankedattribute
linkages.
As can be seen from their descriptions, each aggregation leads to a different
meaning of similarity on the of linkages to consider and
which linkages are considered more important (having higher weights
assigned). All the dataset proximity metrics generated by the different aggrega-
tions listed above are finally normalised Equation 7. The proximity metric
P
D
m
for two datasetsD
y
andD
z
is computed by multiplying the of match-
ing attributes found (N), and divided by the minimum of attributes of
both datasets (M in(|Attr
y
|,|Attr
z
|)). This is done to prevent an inaccurate sim-
ilarity score for two datasets having few very similar attributes of asingle type,
and many other attributes of different types. For example, if datasetD
1
has
1 nominal and 10 numeric attributes andD
2
just has 8 nominal at-
tributes, then if the single nominal inD
1
is highly similar to a nominal inD
2
(e.g.,Sim(A
1
, A
2
) = 0.9) then the overall outcome without nor-
malisation will be a high proximity metric between both datasets although they
have many disjoint types. The resulting proximity metric after nor-
malisation for the datasets would be calculated as follows:P′
D
m
= 0.9∗
1
9
= 0.1,
so overall they will have a low proximity compensating for all the unmatched
attributes without corresponding types.
P′
D
m
=P
D
m
∗
N
Min(|Attr
y
|,|Attr
z
|)
(7)
Applying the models on the DLIn the second phase, after building the
ensemble models, we apply them to each pair of previously unseen datasets
to achieve a measure of the similarity score. When applying the models, we
compute for each pair of datasets the similarity score ofSim(D
y
, D
z
) and for
each pairSim(A
i
, A
j
) the supervised models extracted in the
previous phase. TheSimscore is the positive-class distribution value generated
by each ensemble model [34]. For the level scoring, we complete the
proximity mining task by aggregating thesimscores between pairs of datasets 18A. Alserafi .
(as seen in the last steps of Algorithm 1). To compare dataset pairs, we Algorithm 2, and theM
cls−ds
model generated by the previous build phase.
ALGORITHM 2:Dataset level Matching
Input:Dataset-level proximity metrics for each pair of datasets{D
y
, D
z
}, the modelM
cls−ds
Output:The setSPof similarity score [D
y
, D
z
, Sim(D
y
, D
z
)] for each pair of{D
y
, D
z
}
SP←∅;
foreach{D
y
, D
z
}⊂DLandi6=jdo
Sim(D
y
, D
z
) =M
cls−ds
(D
y
, D
z
);
SP←SP∪{[D
y
, D
z
, Sim(D
y
, D
z
)]};
end
Pairs
of DS
Data
Lake
Dataset-level Classification Model
M
cls_ds
Assign Proximity classifier's score
Sim(D
y
,D
z
)
Aggregate -
level proximity to
dataset-level
Agg
DS Pairs
Testing
Sample
Filter negative pairs of datasets
-level Classification Model
M
cls_num_attr
M
cls_nom_attr
NO
YES
Check pass
threshold C
d
?
Evaluate Dataset-level
Pruning
Rel(D
y
,D
z
)
Pass positive pairs
of datasets
Sample
OML02
Apply -level
Proximity Model
Compute Dataset level
meta-feature proximity
metrics
P
m
-level
profiling
Dataset-level
profiling
Compute level
meta-feature proximity
metrics
P
m
Fig. 8: An overview of the process toapplythe learnt supervised models in our for pre-filtering previously unseen dataset pairs independent of the
build process.
In theapplyphase visualised in Fig. 8, we take the pairs of datasets from
sample OML02 which have not been in the build phase and we compute
the proximity metrics for the dataset level and level meta-features.
First, we profile the level meta-features from this dataset pairs
sample. Then, we apply the level supervised models resulting from the
previous sub-process to score the pairs similarities from the different
dataset pairs. Then, we aggregate the resulting pairs similarities to
the dataset level the aggregation functions. Once we have the dataset level
proximity metrics generated from dataset level and levelmeta-features,
we feed them all to the dataset level supervised models from the build phase to
unseen testing set pairs, not in the training of the models, which assigns a
proximity score to the pairs. If a pair exceeds a certain proximity threshold, we
consider that pair as a positive match to propose for further schema matching,
otherwise the pair is considered as a negative match and is pruned out from Proximity Mining for Pre-filtering Schema Matching19
further schema matching tasks (we evaluate this by pruning effectiveness metrics
in 5.2). This is described in the next subsection.
4.3 Pre-filtering Dataset Pairs for Schema Matching
For the final step of pre-filtering pairs of datasets before applying detailed
instance- schema matching, we check whether the pairs of datasets are
overall related or not, and therefore whether they should be filteredout or pro-
posed for expensive schema matching. We analyses the finalSim(D
y
, D
z
) score
generated by the modelM
cls−ds
for each dataset pair in the DL to decide whether
they satisfy theRel(D
y
, D
z
) or not. We consider the relationship of each dataset
with each of the other datasets existing in the DL. Each dataset must passthe
similarity thresholdc
d
with each individual dataset to be proposed for detailed
schema matching (as in Equation 1).
If we choose a high cut-off threshold we restrict the supervised model to re-
turn less pairs of high proximity, leading to lower recall but also less comparisons,
thus helping to reduce the computational time at the expense of possibly miss-
ing some misclassified pairs. Alternatively, if we choose a lower cut-off threshold,
we relax our model to return pairs of lower proximity. This leads tomore pairs
(i.e., more work for further schema matching tasks) yielding positive matches
and higher recall of positive cases, but, with more pairs marked incorrectly as
matching. We propose how to select an appropriate threshold that optimises this
trade-off empirically in 5.
The complexity of our is quadratic in the of objects (at-
tributes or datasets) compared, and therefore runs in polynomial time,however,
it applies the cheapest computational steps for early-pruning (justcomputing
distances in Equations 3 and 4 and applying the model to score each pair). This
way, we save unnecessary expensive schema matching processing per each value
instance of the attributes in later steps, reducing the computational workload
at the detailed granularity schema matching level by pre-filteringthe matching
tasks. We demonstrate this empirically in 5.5.
5 Experimental Evaluation
In this , we present the experiments which evaluate our by a prototype implementation. We evaluate the following components of our in predictingRel(D
y
, D
z
) for pre-filtering schema matching:
– Proximity metrics:we evaluate the different individual dataset level and
aggregated level meta-features.
– Supervised models:we also evaluate the ensemble supervised models,
which consume the proximity metrics, in the pre-filtering task.
In addition, we evaluate the sub-components of our which in-
clude the level modelsM
cls−nom−attr
andM
cls−num−attr
in predicting 20A. Alserafi .
Rel(A
i
, A
j
). We test the level model in experiment 1, the dataset level
pruning effectiveness against the ground-truth in experiment 2, andthe compu-
tational performance in experiment 3.
In experiments 2 and 3, we compare the performance of our proposed prox-
imity mining models against traditional instance- schema matching tech-
niques. Those are the most expensive techniques which compare values from
instances in the datasets to for computing schema similarity. We benchmark
our results against a na ̈ıve averaging of similarity from a prototype
called Probabilistic Alignment of Relations, Instances, and Schema (PARIS),
which is one of the most cited schema matching tools [33]. PARIS was found
to be best performing with large datasets when compared against other tools
[19] and does not need collection of extra metadata (see Table 1). PARIS does
exact value-string matching on value-frequency inverse functionality [33].
We implement a prototype [4] which compares pairs of attributes from differ-
ent datasets PARIS and generates an overall score forSim(D
y
, D
z
) by
averagingSim(A
i
, A
j
) generated by PARIS from the top-matching -
pairs (similar to Algorithm 1, where PARIS replaces the supervised models). It
converts tabular datasets to RDF triples, and executes a probabilistic match-
ing algorithm for identifying overlapping instances and attributes. We selected
PARIS because of its simplicity and ease of integration with Java- APIs
and its high performance in previous research [19]. We parametrised thepro-
totype with the top performing settings from experiments in [4] sampling 700
instances per dataset, 10 iterations comparisons, with identity and shingling
value strings matching. This will be a baseline pre-filtering heuristic we shall compare against in the experiments.
The rest of this describes the datasets in the experiments, the
evaluation metrics and the different experiments implemented. We present
the results from our experiments and discuss their implications.
5.1 Datasets
We the OpenML DL
5
in our experiments [36], which has more than 20,000
datasets intended for analytics from different subject areas. OpenMLis a web- data repository that allows data scientists to upload different datasets,
which can be in data mining experiments. OpenML stores datasets in the
ARFF tabular format which consist of diverse raw data loaded without any
specific integration schema. This allows us to evaluate our in a -life
setting where datasets come from heterogeneous domains.
We two subsets of manually annotated datasets from OpenML as our
ground-truth (gold standard) for our experiments. Those two subsets have been
generated two different independent processes, and therefore provide in-
dependently generated ground truths that do not overlap. As the research com-
munity is lacking appropriate benchmarking gold standards for approximate
(non-equijoins) dataset and similarity search [23], we published those
5
https://www.openml.org Proximity Mining for Pre-filtering Schema Matching21
datasets online to support in future benchmarking tasks
6
. The experimental
datasets are described as follows:
– OML01 - The level annotated 15 DS:consists of 15 datasets
from different domains as described in Table 4. The total of at-
tributes is 126 (61 nominal and 65 numeric), and the average of
attributes per dataset is 8. There is a total of 3468 pairs of attributes to be
matched (1575 nominal pairs and 1892 numeric pairs). All the pairs of at-
tributes in this subset were manually scrutinised by 5 annotatorsconsisting
of post-graduates with an average age of 28, where 4 are pharmacists and 1
is a computer scientist. They checked the attributes in the datasets and an-
notated all the pairs of attributes from different datasets with relateddata,
Rel(A
i
, A
j
). It took on average 3 hours by each annotator to complete the
task. Annotators assign a single value from{0,1}, where ‘1’ means a related
, and the majority vote is taken for each pair, where the average
Kappa coefficient for the inter-rater agreement is 0.59, the maximum is 0.80
and the minimum 0.37. Annotators were given the following to judge if the pair is related: name, OpenML dataset description, top
10 values, and the mean and standard deviation for numeric attributes. We
didn’t give instructions on how to the provided information to judge,
but we described that “related attributes should store data related to sim-
ilar -world properties, e.g., car prices, specific body size measurements height, etc., and should contain similar data”. Examples of the anno-
tations can be seen in Table 5. There are only 56 positively matching pairs
(19 nominal and 37 numeric). This subset is in training the level models for computing the similarity between attributes from different
datasets and predicting related attributes,Rel(A
i
, A
j
).
Table 4: Description of the OML01 datasets
DomainDatasets IDsDatasets
Vehicles21,455,967,1092car,cars,cars,Crash
Business223,549,841Stock,strikes,stock
Sports214baskball
Health13,15,37breast-cancer,breast-w,diabetes
Others48,50,61,969tae,tic-tac-toe,Iris,Iris
Table 5: Example Cross-dataset Relationships from OML01
No.Dataset 1Dataset 2Attribute 1Attribute 2Relationship
137 (diabetes)214 (baskball)ageagerelated
2455 (cars)549 (strikes)model.yearyearrelated
3455 (cars)967 (cars)allallduplicate
4455 (cars)1092 (Crash)namemodelrelated
5455 (cars)1092 (Crash)weightWtrelated
6
https://github.com/AymanUPC/all
proxopenml 22A. Alserafi .
– OML02 - The dataset level annotated 203 DS:consists of 203 datasets
different from those in the OML01 subset. To collect this sample, we scraped
the OpenML repository to extract all datasets not included in the OML01
sample and having a description of more than 500 characters. Out of the 514
datasets retrieved, we selected 203 with meaningful descriptions (i.e., exclud-
ing datasets whose descriptions do not allow to interpret the content and to
assign a topic). The datasets have a total of 10,971 attributes (2,834 nomi-
nal, 8,137 numeric). There are 19,931 pairs of datasets with about 35 million pairs to match. According to Algorithm 1, there are 3.7 million
comparisons for nominal attributes (leading to 59,570 top matching pairs)
and 31.5 million numeric pairs (leading to 167,882 top matching
pairs). We try to prevent the value- schema matching on all possible
pairs of values between datasets, where there are 216,330 values which would
lead to 23.4 billion comparisons at the value level. A domain expert with a
background in pharmaceutical studies and one of the authors collaborated
to manually label the datasets
7
. They the textual descriptions of the
datasets to extract their topics, which is common experimental practice in
dataset matching assessment, similar to the experimental setup in[6]. The
annotators sat together in the same room and discussed each dataset with
its description and decided on its appropriate -life subject-area (e.g., car
engines, computer hardware, etc.). To group similar datasets in the same
subject-area grouping, annotators had to discuss and agree together on a
single annotation to give to a dataset. This was done by discussing the
specific -world concept which the dataset describes, e.g., “animal pro-
files”, “motion sensing”, etc. The annotators were only allowed to scrutinise
the textual descriptions of the datasets and did not receive the underlying
data stored in their attributes to prevent any bias towards our proposed
algorithms. It took the annotators about 15 hours in total to annotate the
datasets. Pairs of datasets falling under the same subject-area were positively
annotated forRel(D
y
, D
z
). The sample consists of 543 positive pairs from
the 20,503 total of pairs. The details of the sample is summarised
in Table 6, which lists the of datasets, the of topics, top
topics by the of datasets, and the of related pairs. Some of
the pairs from the sample can be seen in Table 7. We can see, for example,
that dataset with ID 23 should match all datasets falling under the topic of
‘census data’ dataset 179. Both datasets have data about citizens from
a population census. In row 4, we can see an example of duplicated datasets
having highly intersecting data in their attributes. Duplicatepairs those
in row 4 have the same of instances, but described with different of attributes, which are overlapping. We consider all duplicate pairs
of datasets as related pairs. We aim to detect and recommend such kind
7
Those dataset annotations were reviewed by 5 independent judges, and the results
of this validation are published online at:
https://github.com/AymanUPC/all
proxopenml/blob/master/OML02/oml02revalidationresults.pdf Proximity Mining for Pre-filtering Schema Matching23
of similar dataset pairs as those in Table 7 for schema matching our
proximity mining .
Table 6: Description of the OML02 datasets
DatasetsTopicsTop TopicsRel(D
y
,D
z
)
20374computer software defects (16), health
measurements (13), digit handwriting
recognition (12), robot motion sensing
(11), plant and fungi measurements (9),
citizens census data (8), diseases (8)
543
Table 7: An example of pairs of datasets from the OML02 sample from OpenML
No.DID 1Dataset 1DID 2Dataset 2TopicRelationship
123cmc179adultCensus Datarelated
214mfeat-fourier1038ginaagnosticDigit Handwriting Recognitionrelated
355hepatitis171primary-tumorDiseaserelated
4189kin8nm308puma32HRobot Motion Sensingduplicate
5.2 Evaluation Metrics
We different evaluation metrics to assess the effectiveness of our .
We the traditional recommendation and information retrieval evaluation
metrics similar to other research [15,22], including precision, recall and ROC
measurements. For the supervised models, we traditional data mining clas-
sification effectiveness metrics [34]. We evaluate the computational costs of our vs. traditional schema matching for baseline comparison. Those metrics
are categorised per the experiment types and granularities:
–Classification effectiveness
•Granularity: levelRel(A
i
, A
j
) and Dataset levelRel(D
y
, D
z
)
•Models evaluated:M
cls−nom−attr
,M
cls−num−attr
,M
cls−ds
•Classification measures: Classification accuracy, Recall, Precision, ROC,
Kappa
–Pre-filtering (pruning) effectiveness
•Granularity: Dataset levelRel(D
y
, D
z
)
•Model evaluated:M
cls−ds
,P ARIS
•Retrieval measures: Recall, Precision, Efficiency Gain, Lift Score
–Computational performance
•Granularity: levelRel(A
i
, A
j
) and Dataset levelRel(D
y
, D
z
)
•Model evaluated:M
cls−nom−attr
,M
cls−num−attr
,M
cls−ds
,P ARIS
•Computational measures: computational processing time (milliseconds),
metadata size (megabytes) 24A. Alserafi .
Table 8: The significance of the
Kappa statistic
KappaSignificance
<0Disagreement
0.0 - 0.10No significance
0.0 - 0.20Slight
0.21 - 0.40Fair
0.41 - 0.60Moderate
0.61 - 0.80High
0.81 - 1.0Excellent
Table 9: The significance of the ROC
statistic
ROCSignificance
<0.5Disagreement
0.5 - 0.6No significance
0.6 - 0.7Slight
0.7 - 0.8Moderate
0.8 - 0.9High
0.9 - 1.0Excellent
For the classification effectiveness measures, the classification accuracy is
given in our results as a percentage. The recall and precision rate are also per-
centages. For the ROC (area under the curve) and Kappa statistic, they are
a value between 0 and 1, where the value significance is evaluated inour
results according to Tables 8-9.
For the pruning effectiveness measures, we evaluate our the
measurements described in Equations (8),(9), (10) and (11). Here, TP means
true-positives which are the pairs of datasets correctly classified by the models.
FN are false negatives, FP are false-positives, TN are true-negatives,and N
indicates the total of possible pairs of datasets (which is a sumof all pairs
TP + FP + TN + FN). The efficiency gain measures the amount of reduction in
work required, in terms of of pairs of datasets eliminated by ourmodels.
The lift score measures the capability of the model in filtering out more pairs
than randomly removing pairs for the recall rate achieved. A higher amount is
better, where a value of 3.0 would mean that the model is capable of retrieving
3 times more positive pairs than the expected amount of positive pairs from a
random sample without the model.
recall=
T P
T P+F N
(8)precision=
T P
T P+F P
(9)
efficiency-gain=
T N+F N
N
(10)
lift-score=
recall
(1.0−efficiency-gain)
(11)
5.3 Experiment 1: -level Models
Our goal in this experiment is to evaluate the supervised models webuild for
detecting the relationship between related attributesRel(A
i
, A
j
) level content meta-features as follows:
– Dataset: OML01
– Evaluation metrics: Classification effectiveness
– Relationship evaluated:Rel(A
i
, A
j
)
– Input: the level meta-features matching for pairs of attributes.
– Output: a supervised model to predict related attributes per type.
– Goal: select the most appropriate models for predicting related attributes
by evaluating their effectiveness for each type (nominal or numerical). Proximity Mining for Pre-filtering Schema Matching25
– Description: we take two subsets of pairs and their meta-features,
depending on the type: nominal attributes and numeric attributes.
The subsets are annotated by a human to decide whetherRel(A
i
, A
j
) is 1
or 0. We build a proximity model a supervised learning algorithm.
Experimental Setupwe evaluate the model the leave-one-out (where we exclude one pair from training in each run, and it to test the
output model, therefore having a cross-validation where the of folds is
equal to the of pairs). As the of positive pairs to negative pairs
are imbalanced, we create a balanced training set for each type, nominal ornu-
meric, which consists of all the positive pairs of matches and an equal of negative unmatching pairs. To make the training set representative
of all the different negative cases, we cluster the negative cases the Expec-
tation Maximisation (EM) algorithm [34] and we select a representative sample
of negative cases from each cluster.
ResultsThe level models were evaluated for both nominal pairs and numeric pairs. We evaluate theM
cls−nom−attr
andM
cls−num−attr
models which assign theSim(A
i
, A
j
) for pairs. As could be seen in Ta-
ble 10, we created two supervised models; one for each type of pairs.
Both models achieved excellent ROC performance and highly significant results
on the Kappa statistic (see Tables 8-9 results significance). The models had
good accuracy, recall, and precision rates. This is important because the dataset
pairs pre-filtering step depends on this proximity step, so we have to
achieve a good performance at this level to minimise accumulation of errors for
the following tasks.
Table 10: Performance evaluation of pairs proximity models
ModelROCKappaAccuracyPositive RecallPositive Precision
Nominal0.9570.6582.5%89.5%77.3%
Numeric0.9150.784.8%89.2%80.5%
5.4 Experiment 2: Dataset-level Models
In this experiment, we evaluate the effectiveness of the dataset level models in
pre-filtering dataset pairs for further schema matching. Our goal is toevaluate
how good is our in retrieving related datasetsRel(D
y
, D
z
) and filter-
ing out unrelated datasets from the schema matching process. We evaluate the
effectiveness of correctly proposing related datasets for schema matching the different types of models we describe later in this . The evaluation is
as follows:
– Dataset: OML02 26A. Alserafi .
– Evaluation metrics: Classification effectiveness, Pre-filtering effectiveness
– Relationship evaluated:Rel(D
y
, D
z
)
– Input: Pairs of datasets with different types of meta-features matching
[dataset level names, dataset level content meta-features, level
names, level meta-features, all meta-features].
– Output: a supervised modelM
cls−ds
to predict related datasets on
proximity mining and PARIS baseline.
– Goal: select the best proximity model to predict related datasets andthe
proximity thresholdc
d
to with that model.
– Description: we take annotated pairs of datasets and their meta-features’
normalised metrics. The pairs are annotated by a human annotator to decide
whetherRel(D
y
, D
z
) is 1 or 0. We build a proximity model a supervised
learning algorithm.
We create different types of dataset pairs ensemble models to scoreSim(D
y
, D
z
)
by different combination of meta-feature types. We create different models
depending on the meta-feature type(s) as input (namely those are dataset
content meta-features, dataset name similarity, content meta-features,
and name similarity). We can combine the meta-feature types to
build the model or each type separately to lead to the following model types
depending on which meta-features are :
– DS-Prox-Content:uses the dataset level content meta-features, without
considering the dataset name distance.
– DS-Prox-Name:uses the dataset name Levenshtein distance as the only
predictor of dataset pairs similarity.
– -Prox-Content:uses the level content meta-features,
not considering the name meta-features.
– -Prox-Name:uses the level name meta-features only,
not considering the content meta-features.
– Name-Prox:uses dataset level and level name- meta-features
only.
– Content-Prox:uses dataset level and level content- meta-
features only.
– All-Prox:uses all the dataset level and level meta-features, in-
cluding both name- and content- meta-features.
We differentiate between the meta-feature types in our experiments so we can
test if a specific subset of meta-features is better in predictingRel(D
y
, D
z
) or
whether it is necessary to all of them together to build an effective proximity
mining model for the pre-filtering task. We also investigate if there is a difference
in performance with regards to the types of meta-features extracted:classical
name- meta-features vs. the newly proposed content- meta-features,
and whether both types together in combination leads to better results.
We also separate the types so we can distinguish if purely content- meta-
features can be as an alternative to name- meta-features, especially in
the case when the datasets and their attributes are not properly named.The most Proximity Mining for Pre-filtering Schema Matching27
comprehensive of all models is theAll-P roxmodel which uses all the possible
meta-features we collect from the data profiling step. TheDS-P rox-N ameis
the most generic of all models as it just considers a single meta-feature at the
most abstract-level, therefore it will be as our baseline for our performance
comparisons in the experiments.
Experimental SetupTo evaluate our and models, we consider in our
experiments a 10-fold cross-validation experimental setup different subsets
of datasets from a -world DL. The purpose of a cross-validation setup isto
select the best supervised model for the pre-filtering task by evaluating the
different models on test-sets separate from the training-sets, which is commonly in recommendation assessment experiments [2,15] and schema matching
tasks [10]. Such an experimental setup increases the validity and generalisability
of our experiments and . This is only achieved if the training set is
representative of the cases found in the -life population.
We make sure that the folds do not include intersecting dataset pairs. We
create a balanced training set with all the positive cases and an equal of
negative cases similar to experiment 1. As the of negative cases is much
higher in the OML02 subset too, we also follow the clustering of negative cases to select a representative sample from each cluster (see 5.3).
We iterate 10 times an alternating fold as the test set, and the remain-
ing folds as the training set. We evaluate the models in accurately predicting
Rel(D
y
, D
z
) with 9 different cut-off thresholds in [0.1-0.9] for ‘c
d
’ from Equation
(1) in order to cover a wide range of values. Finally, we evaluate the performance
of each model by averaging the evaluation metrics from all 10 iterations. We also
compute standard deviations in performance between different folds to evaluate
the stability and consistency of the models evaluated. For the PARIS baseline
implementation, it does not need to train any models, so we simply run it on all
pairs of datasets and compare its results to our .
Results Classification effectiveness.First, we created the models for the
dataset pairs which assignSim(D
y
, D
z
) and check if they satisfyRel(D
y
, D
z
)
by passing the minimum threshold. We evaluated the classification effectiveness
measures for each proximity model after the 10-fold cross-validation. The re-
sults are summarised in Figures 9-11. The figures show a plot of results (from
10 folds) and interquartile ranges of accuracy, kappa statistic and ROC statistic
for each model type. The distribution between folds can also be seento assess
the stability of the models. For our comparison, we the name- models,
which are common in previous research, as our baseline comparison. As could
be seen, all models were stable with very close values for the different evaluation
metrics, indicating the versatility of our . However, stillthe All-Prox
and -Prox models consistently had slightly better stability (lower devi-
ations) than Name-Prox and other models. It can be seen from the resultsthat
the All-Prox and -Prox models are consistently performing better than
the name- model in terms of accuracy, ROC and Kappa statistic. This indi- 28A. Alserafi .
cates that our proposed content- and name- combined meta-features
models perform best with schema matching pre-filtering.
Model Type 71 72 73 74 75 76 77 78 79 80 81 82 Classification Accuracy All-Prox -Prox Name-Prox Content-Prox DS-Prox 78.18 75.51 74.29 73.72 73.29 Fig. 9: Classification accuracy from 10-
fold cross-validation of dataset pairs
pre-filtering models.
Model Type 0.44 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.60 0.62 Kappa Statistic All-Prox -Prox Name-Prox Content-Prox DS-Prox 0.56 0.51 0.49 0.47 0.47 Fig. 10: Kappa statistic from 10-fold
cross-validation of dataset pairs pre-
filtering models.
Model Type 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 Roc Score All-Prox -Prox Name-Prox Content-Prox DS-Prox 0.86 0.83 0.81 0.81 0.81 Fig. 11: ROC statistic from 10-fold
cross-validation of dataset pairs pre-
filtering models.
The different models for the schema matching pre-filteringtask achieve
different results because the meta-features in the different models are not
correlated, therefore contain different information about the datasets leading
to the different performance of each model. We evaluated the Spearman rank
correlation [15] between the different types of meta-features, whichis presented
in Table 11. The Spearman rank correlation ranks the dataset pairs according
to the proximity metrics of the meta-features. If the dataset pairshave the same
identical rankings between two different meta-features then we geta perfect
correlation. If the rankings produced in descending order by the two proximity
metrics are different (e.g., a dataset pair can be ranked in the 100th position by
one meta-feature and in the 9th position by the other, which have a difference
of 81 ranks) then we get a lower correlation, with completely uncorrelated meta-
features. We evaluated the average, standard deviation, minimum, and maximum
of the correlation between the meta-features falling under the different types
of meta-features. Recall that each type will have multiple meta-features (see 4.1), content will include all the meta-features in Table 3
with all their different proximity metrics according to the aggregations described
in 4.2. We calculate the correlation between each individual meta-feature
pair and we calculate aggregates per type. As can be seen in Table 11, all the
correlation values are low. Proximity Mining for Pre-filtering Schema Matching29
Table 11: Spearman rank correlation for the different meta-features. We aggre-
gate minimum (Min.), average (Avg.), maximum (Max.), & standard deviation
(Std. Dev.) for different meta-feature types.
Type 1Type 2Min. CorrelationAvg. CorrelationMax. CorrelationStd. Dev. Correlation NameAttribute Content-0.12-0.010.190.04 NameDataset Content0.020.040.100.02 NameDataset Name0.060.070.130.02
Dataset ContentAttribute Content-0.010.090.150.04
Dataset ContentDataset Name-0.020.000.020.02
Dataset NameAttribute Content0.000.010.040.01
0 10 20 30 40 50 60 70 80 90 100 Avg. Recall 0 10 20 30 40 50 60 70 80 90 100 Avg. Efficiency Gain 3.1 2.3 2.9 5.6 1.7 1.9 2.4 4.5 1.6 10.8 3.3 1.3 1.2 4.5 24.6 4.9 5.5 7.6 14.8 1.3 DS-Prox Name-Prox Content-Prox -Prox All-Prox Baseline Fig. 12: Recall against efficiency gain
for the different supervised models.
0 10 20 30 40 50 60 70 80 90 100 Avg. Recall 0 10 20 30 40 50 60 70 80 90 100 Avg. Precision 10.8 5.6 4.5 14.3 24.6 7.6 73.8 3.1 3.4 2.1 1.9 29.8 3.3 14.8 2.4 5.5 1.7 4.9 4.5 1.3 All-Prox -Prox Content-Prox Name-Prox DS-Prox Baseline Fig. 13: Recall against precision for the
different supervised models.
0 10 20 30 40 50 60 70 80 90 100 Avg. Recall 0 10 20 30 40 50 60 70 80 90 100 Avg. Efficiency Gain 1.57 1.84 2.21 2.77 3.19 3.73 4.02 1.21 1.01 1.07 1.11 1.26 1.40 1.64 1.80 2.15 3.37 8.19 1.26 1.69 1.95 2.36 7.06 6.71 1.27 9.38 29.14 38.28 Sheet 4 A tt r i b u te- N a m e - P r o x A tt r i b te - C o n t e n t D S - C o n t e n t D S - N a m e Average of Recall vs. average of Efficiency Gain. Color shows details about Sim Measure Grouped. Shape shows details about Sim Measure Grouped. The marks are labeled by average of Lift Score. Details are shown for Threshold. The data is filtered on Eval Type and Sim Measure. The Eval Type filter keeps
entity. The Sim Measure filter has multiple members selected. Fig. 14: Recall against efficiency gain
for the different metric types.
0 10 20 30 40 50 60 70 80 90 100 Avg. Recall 0 10 20 30 40 50 60 70 80 90 100 Avg. Precision 3.19 4.02 6.57 1.11 1.01 1.21 3.37 8.19 14.26 22.11 6.63 8.81 12.69 15.54 17.06 18.40 10.41 11.68 1.69 14.96 1.95 2.36 2.74 6.71 8.70 23.93 29.14 32.37 32.51 34.23 35.03 38.28 Sheet 5 Name Prox -Content DS-Content
DS-Name
Average of Recall vs. average of Precision. Color shows details about Sim Measure Grouped. Shape shows details about Sim Measure Grouped. The marks are labeled by average of Lift Score. Details are shown for Threshold. The data is filtered on Eval Type and Sim Measure. The Eval Type filter keeps entity. The Sim Measure filter has multiple members selected. Fig. 15: Recall against precision for the
different metric types. 30A. Alserafi .
Pre-filtering effectiveness.We compare the effectiveness of our against the baseline implementation of PARIS. We change the cut-off thresholds
for Equation 1, and we aim to maximize the efficiency-gain while maintaining
the highest recall for all candidate dataset pairs satisfyingRel(D
y
, D
z
). The ef-
fectiveness is also evaluated by lift scores. The results from our and
from the‘baseline’PARIS prototype are presented in Figures 12-15. Figures 12-
13 show the results for the different supervised models and PARIS,and Figures
14-15 show the results of the same evaluation metrics but for the individual meta-
features in our , where we the individual proximity metrics of the
meta-features directly as an indicator ofRel(D
y
, D
z
) without any super-
vised learning models. We evaluate the dataset level meta-featuresfrom 4.1. The graphs show the average performance for all the individual metrics per
specific type. We a different minimum threshold with each proximity model
or meta-feature in Figures 12-15 leading to the different plotted results per model
or meta-feature. The aim of comparing both models and individual metrics is
to be able to detect if the proposed supervised proximity modelsperform any
better than simply single independent metrics for the pre-filtering task.
For each evaluation of the models or the individual metrics, we evaluatethe
efficiency gain against recall first. We set a minimum target recall of 80% and
a minimum target efficiency gain of 60% (i.e., filtering out at least 60% of the
pairs of datasets while still proposing 80% of the true positive pairs), which are
the grey shaded areas in the graphs. The minimum thresholds can be selected
differently according to the requirements of the data analyst. Goodperforming
models or proximity metrics are those that fall in this shaded area. The numbers
annotated to some of the points in the graphs indicate the lift score (higher values
are better). Similarly, we compare the precision against the recall inthe second
graphs for each evaluation (models or metrics). We also annotate some selected
lift scores for some points in the graph.
When comparing our proposed proximity models with the proximity
model (DS-Prox) from our previous work [5], it can be seen that our -
Prox model and the All-Prox model perform consistently better. This is expected
because we are collecting finer granularity metadata to describe the datasets
which makes it easier in the supervised learning task to differentiate between
positive pairs and negative pairs. Although our proposed techniques out-
perform our previous work in the DS-Prox model in terms of recall rates and lift
scores, it comes at the price of a more computationally expensive algorithm (-
gorithm 1). The complexity of the dataset level Algorithm 2 isO([n∗(n−1)]/2)
while the complexity of the level Algorithm 1 isO([n∗(n−1)∗a
2
]/2)
where ‘n’ is the of datasets and ‘a’ is the of attributes in each
dataset (we can the average of attributes per dataset as an approx-
imation for ‘a’ when estimating the of computations required).
If we would compare the content meta-features only model (Content-Prox)
with the name meta-features only model (Name-Prox), we would see thatboth
models perform equally the same in the pre-filtering task, although combining
them in the All-prox model leads to the best results capturing the similarity of Proximity Mining for Pre-filtering Schema Matching31
difficult pairs that can not be retrieved by any single type individually. There-
fore, it is possible to solely depend on content- proximity models as a
replacement of name- proximity models to achieve similar results. This
will be important in the cases of DLs which are not well maintained and do
not have properly named datasets and attributes. We investigate in detail the
performance of the All-Prox proximity model on its true positives, false
positives and false negative pairs in the Appendix
8
, where we present the exact
cases, we discuss the reasons of discrepancies and we give a comparativeanalysis
of the underlying proximity metrics which led to those cases.
For each of the pruning effectiveness evaluation metrics listed above, we com-
pute the average and standard deviation of the measure between the different
folds of evaluation for our . The average is plotted in the graphs in
Figures 12-15, and the standard deviations of each model for the threshold 0.5
(we chose the mean threshold) are given in Table 12. The standard deviation
indicates the stability of our proposed metrics and models with different subsets
of datasets. We aim for a low standard deviation to prove the high adaptability
of our .
Table 12: The standard deviation of each evaluation measure for 10-fold cross-
validation of each dataset pairs pre-filtering model, wherec
d
= 0.5
ProximitySD RecallSD Efficiency GainSD PrecisionSD Lift Score
All-Prox5.40.610.580.32
-Prox6.51.00.70.24
Content-Prox6.81.00.380.35
DS-Prox7.860.850.290.28
Name-Prox6.31.10.470.24
Dataset Pairs Pre-filtering Meta-featuresFirst, we assess if the super-
vised learning models perform better than a simpler onthe
sub-components they are dependant on, which are the individual meta-features in the models. The supervised models multiple features in combination
to score the similarity of pairs of datasets. Here, we assess the individual features
as a baseline to compare against, and whether simply a proximitymetric
of an individual meta-feature without any models can lead to any good result.
We aggregated an average for the pruning evaluation metrics per each type of
meta-feature. The results comparing recall against efficiency gain is given in Fig.
14. In our experiments, no single meta-feature was able to individually predict
related pairs of datasets to achieve optimum recall and efficiency gain, as can
be seen by the lack of any plotted result in the top-right box. As seen in Fig.
15, the pre-filtering task the meta-features can not have a precision better
than 10% for the higher recall rates.
8
The appendix could be found online at https://aymanupc.github.io/all
proxopenml 32A. Alserafi .
We note here that the different types of meta-features are able to model
different information about the datasets and their attributes as seen bythe
low correlations in Table 11. That is the main reason we the combination
of different types of meta-features in our proximity models which are able to
combine the meta-features to give better results.
Dataset Pairs Pre-filtering Efficiency Gain Vs. RecallWe also eval-
uated the different supervised proximity models by testing their pre-filtering
performance with different proximity thresholds. As can be seen inFig. 12,
all of the proximity models were able to optimise recall and efficiency gain to
achieve results in the top-right shaded area, compared to the baselinePARIS
implementation that was not successful. This shows the value of approximate
proximity matching and the supervised models in our compared to
exact instance- string matching in the baseline. The best performing mod-
els were the All-Prox and -Prox models which achievedbetter results
than DS-Prox from our previous work [5] and better results than Name-Prox
which are more common in other previous research. This means that combining
both name- meta-features and content- meta-feature in a supervised
model achieves best results in the schema matching pre-filtering task. For ex-
ample, a good result can be achieved the All-Prox model (combining all
meta-feature types) with a threshold of 0.4 which achieves a recall rate of 85%,
an efficiency gain of 73% and a lift score of 3.14. This means that the model
is able to effectively propose most of the pairs of datasets for schema matching
with the least effort possible (only proposing 27% of pairs for comparison), while
achieving this with a performance that is three times better than naive random
selection of dataset pairs for schema matching (as expressed by the lift score of
3.14 achieved by the All-Prox model).
Dataset Pairs Pre-filtering Precision Vs. RecallAs seen in Fig. 13, the
precision of the proximity models improved the performance of theschema
matching pre-filtering as seen by the higher precision rates compared to the
individual meta-features in Fig. 15. By combining the meta-features in a su-
pervised model we were able to achieve higher precision rates withthe same
recall rates, for example, a precision of 17% with a recall rate of 75% the All-Prox model. This is better than the best achievable precision with the
individual meta-features, which can achieve a precision of 4% with the same
recall rate for the level meta-feature type. However, we acknowledge
that the precision rates are low for all types of models and meta-features. We
can therefore conclude that our proposed proximity mining can only
be as an initial schema matching pre-filter which is able to prune unneces-
sary schema matching comparisons from further steps. Our can not be for the final schema matching task because it will produce falsepositives.
Therefore, dataset pairs should be further scrutinised with more comparisons to
assess their schema similarity (as seen in Fig. 1 bottom instance- matching Proximity Mining for Pre-filtering Schema Matching33
layer). Such comparisons instance- matching similar to ourprevious
work [4].
5.5 Experiment 3: Computational Performance Evaluation
In this experiment, we evaluate the computational performance in terms of time
and storage space consumption as follows:
– Dataset: OML02
– Evaluation metrics: Computational performance
– Relationship evaluated: all
– Input: All the pairs of datasets from OML02, a model (M
cls−ds
) and the
P ARISschema matching prototype.
– Output: -level and dataset-level metadata.
– Goal: test the comparable computational costs of running the different com-
ponents of our proximity mining vs. traditional instance-
schema matching techniques. We show the value of pre-filtering by means of
computational costs saving.
– Description: we take all the annotated pairs from OML02 and we do a
complete run which collects the required meta-features and metrics, and we
run the algorithms to computeSim(D
y
, D
z
). We measure the amount of
time and storage space it takes to process the pairs.
We ran the experiments for our a computer running on Linux
Debian, 8GB main memory, a dual-core Intel i7 processor running at 2.4GHz
and 4MB cache, Java v8 for the implementation of our algorithms, and Postgres
database v9.5.12 for the metadata storage and management. For the PARIS
baseline implementation, we a server with more resources as recommended
by the developers. The server runs on Linux Debian, Java v8, 24GB of memory
and a quad-core processor at 2.4 GhZ and 4MB cache. We present the results
below.
ResultsWe compare the computational performance by evaluating the amount
of time and storage space for running our and the PARIS- im-
plementation with the DL sample OML02. The results can be seen in Table13.
We list the tasks from our and compare to the baseline in the last row.
We compute the time for each task, the average time it takes, and the storage
space . For the matching, we keep the output in memory and do
not materialise it. We only materialise top-matching pairs. on the results in Table 13, our needs a total of 112 minutes
and 100MB storage space for the OML02 DL sample datasets of a total size of
2.1GB (i.e., 5% metadata space overhead). This is at least 2 orders of magnitude
less than the time and space consumption of the baseline PARIS implementation.
The most expensive steps in our were those for the numeric matching
tasks as they were much greater in amount than nominal attributes. Still, our ap-
proach is more efficient in terms of computational performance and pre-filtering
effectiveness as shown by our results. 34A. Alserafi .
Table 13: The computational performance of our vs. the PARIS im-
plementation in terms of time and storage space
TaskTimingAverage TimeStorage Space
Dataset Profiling263,019ms (4:23 minutes)1,295ms per dataset31.25MB
Numeric Matching1,184,000ms (19:44 min-
utes)
0.04ms per pairIn memory
Nominal Matching160,000ms (2:40 minutes)0.04ms per pairIn memory
Numeric Top Matching3,250,000ms (54:10 min-
utes)
0.1ms per pair
208ms per dataset pair
(15,576 dataset pairs)
7MB
Nominal Top Matching313,000ms (5:13 minutes)0.08ms per pair
19ms per dataset pair
(16,290 dataset pairs)
2.33MB
Dataset-level All Aggregations of At-
tribute Similarities
500,000ms (8:20 minutes)25ms per dataset pair
(19,931 dataset pairs)
35MB
Dataset-level Name Matching202ms (0 minutes)0.01ms per dataset pair
(20,503 pairs)
Part of Top Matching
metadata
Dataset-level Content Matching5,100ms (5.1 seconds)0.25ms per dataset pair
(20,503 pairs)
3.25MB
-level Name Matching, top
pairs computation, and aggregation
1,018,663ms (16:58 min-
utes)
0.03ms per pair
(35,283,824 pair)
51ms per dataset pair
(19,931 dataset pair)
12.5MB
Apply the proximity models on the
dataset pairs to score their similari-
ties
1,665ms (1.66 seconds)0.08ms per dataset pair
(20,503 dataset pair)
8.5MB
PARIS Alignment Implementation743,077,431ms (12,384:37
minutes)
36,241ms per dataset pair
(0:36 minutes per dataset
pair)
15,450MB (15.1GB)
5.6 Generalisability
In our experiments, we have the OpenML DL to create a 10-fold cross-
validation experimental setup. OpenML stores datasets representing heteroge-
neous subject-areas. Thus, we expect our proposed techniques to achieve similar
results with different heterogeneous DLs. We tested our withdiffer-
ent heterogeneous DL subsets covering randomly selected subject-areas in each
cross-validation fold. This further improves the generalisabilityof our results as
the results achieved proved to be stable between the different cross-validation
folds. Therefore, our is recommended in the early-pruning and schema
matching pre-filtering task in a DL environment with heterogeneoussubject-
areas. Under different settings, the data scientist should first test the perfor-
mance of our on a test sample and then select the best performing
cut-off thresholds accordingly. It is also crucial that the training samples se-
lected for creating the supervised models are representative of the specific DL
setting they are for. We also note, that although our experiments were done
over binary approximation for theRel(D
y
, D
z
) in the ground truth due
to the difficulty to find a ground truth with a similarity continuum,still our ap-
proach can be useful in dataset pairs ranking problems theSim(D
y
, D
z
)
continuous .
6 Conclusion
We have presented in this paper a novel for pre-filtering schema match-
ing metadata- proximity mining algorithms. The isable to Proximity Mining for Pre-filtering Schema Matching35
detect related dataset pairs containing similar data by analysing their meta-
data and a supervised learning model to compute their proximity score.
Those pairs exceeding a minimum threshold are proposed for more detailed, more
expensive schema matching at the value- granularity-level. Our was found to be highly effective in this early-pruning task, whereby dissimilar
datasets were effectively filtered out and datasets with similar data were effec-
tively detected in a -life DL setting. Our achieves high lift scores
and efficiency gain in the pre-filtering task, while maintaining a highrecall rate.
For future research, we will investigate the different techniques to improve the
scalability of our by improving level matching selectivity. We
also want to investigate the possibility of detailed semantic schemamatching at
the level. We will also investigate our proximity mining in
effectively clustering the datasets into meaningful groupings of similarity.
Acknowledgement.This research has been partially funded by the European
Commission through the Erasmus Mundus Joint Doctorate (IT4BI-DC).
References
1. Abedjan, Z., Golab, L., Naumann, F.: Profiling relational data: a survey. The
VLDB Journal24(4), 557–581 (2015). https://doi.org/10.1007/s00778-015-0389-
y,
2. Adomavicius, G., Sankaranarayanan, R., Sen, S., Tuzhilin,A.: Incorporating
contextual information in recommender systems a multidimensional ap-
proach. ACM Transactions on Information Systems (TOIS)23(1), 103–145 (2005).
https://doi.org/10.1145/1055709.1055714
3. Algergawy, A., Massmann, S., Rahm, E.: A Clustering- for Large-
Scale Ontology Matching. In: East European Conference on Advances in Databases
and Information Systems (ADBIS), . 415–428. Springer (2011).
4. Alserafi, A., Abell ́o, A., Romero, O., Calders, T.: Towards Information Profiling:
Data Lake Content Metadata Management. In: DINA Workshop, ICDM.. 178–
185. IEEE (2016). https://doi.org/10.1109/ICDMW.2016.0033
5. Alserafi, A., Calders, T., Abell ́o, A., Romero, O.: DS-prox: Dataset proxim-
ity mining for governing the data lake. In: International Conference on Simi-
larity Search and Applications. vol. 10609 LNCS, . 284–299.Springer (2017).
https://doi.org/10.1007/978-3-319-68474-1
20
6. Ben Ellefi, M., Bellahsene, Z., Dietze, S., Todorov, K.: Dataset Recommenda-
tion for Data Linking: An Intensional . In: Proceedingsof the Inter-
national Semantic Web Conference: The Semantic Web. Latest Advances and Domains. vol. 9678, . 36–51. Springer (2016).http://link.springer.com/10.
1007/978-3-319-34129-3
7. Bernstein, P.A., Madhavan, J., Rahm, E.: Generic Schema Matching , Ten Years
Later. Proceedings of the VLDB Endowment4(11), 695–701 (2011)
8. Bilke, A., Naumann, F.: Schema Matching Duplicates.In: Proceedings of the
21st International Conference on Data Engineering. . 69–80. IEEE (2005)
9. Breiman, L.: Random Forests. Machine Learning45(1), 5–32 (2001)
10. Chen, C., Halevy, A., Tan, W.c.: BigGorilla : An Open-Source Ecosystem for Data
Preparation and Integration. IEEE Data Engineering Bulletin41(2), 10–22 (2018) 36A. Alserafi .
11. Chen, Z., Jia, H., Heflin, J., Davison, B.D.: Generating Schema Labels
through Dataset Content Analysis. In: Companion of the The WebConfer-
ence 2018 on The Web Conference 2018 - WWW ’18. . 1515–1522 (2018).
https://doi.org/10.1145/3184558.3191601
12. Deng, D., Kim, A., Madden, S., Stonebraker, M.: SilkMoth: An Efficient Method
for Finding Related Sets with Maximum Matching Constraints. Proceedings of the
VLDB Endowment10(10), 1082–1093 (2017)
13. Furche, T., Gottlob, G., Libkin, L., Orsi, G., Paton, N.W.:Data wrangling for big
data: Challenges and opportunities. In: EDBT. vol. 16, . 473–478 (2016)
14. Gallinucci, E., Golfarelli, M., Rizzi, S.: Schema profilingof
document-oriented databases. Information Systems75, 13–25 (2018).
https://doi.org/10.1016/j.is.2018.02.007
15. Herlocker, J., Konstan, J.A., Terveen, L.G., Riedel, J.T.:Evaluating Collabora-
tive Filtering Recommender Systems. ACM Transactions on Information Systems
(TOIS)22(1), 5–53 (2004)
16. Kandel, S., Heer, J., Plaisant, C., Kennedy, J., Van Ham, F., Riche, N.H., Weaver,
C., Lee, B., Brodbeck, D., Buono, P.: Research directions in datawrangling: Vi-
sualizations and transformations for usable and credible data. Information Visual-
ization10(4), 271–288 (2011)
17. Kim, J., Peng, Y., Ivezic, N., Shin, J.: An Optimization for Semantic- XML Schema Matching. International Journal of Trade, Economics and
Finance2(1), 78 – 86 (2011)
18. Kruse, S., Papenbrock, T., Harmouch, H., Naumann, F.: Data Anamnesis : Ad-
mitting Raw Data into an Organization. Bulletin of the IEEE Computer Society
Technical Committee on Data Engineering . 8–20 (2016)
19. Lacoste-Julien, S., Palla, K., Davies, A., Kasneci, G., Graepel, T., Ghahramani,
Z.: SiGMa: Simple Greedy Matching for Aligning Large KnowledgeBases. In: Pro-
ceedings of the 19th ACM SIGKDD international conference. . 572–580 (2013).
https://doi.org/10.1145/2487575.2487592
20. Maccioni, A., Torlone, R.: KAYAK: A Framework for Just-in-Time Data Prepa-
ration in a Data Lake. In: International Conference on Advanced Information
Systems Engineering. . 474–489. Springer International Publishing (2018).
https://doi.org/10.1007/978-3-319-91563-0
21. Madhavan, J., Bernstein, P.a., Rahm, E.: Generic Schema Matching with Cupid.
VLDB1, 49–58 (2001)
22. Manning, C.D., Raghavan, P., Sch ̈utze, H.: An Introduction to Information Re-
trieval. No. c (2009)
23. Miller, R.: Open Data Integration. PVLDB11(12), 2130–2139 (2018)
24. Naumann, F.: Data profiling revisited. ACM SIGMOD Record42(4), 40–49 (2014)
25. Oliveira, A., Tessarolli, G., Ghiotto, G., Pinto, B., Campello, F., Marques, M.,
Oliveira, C., Rodrigues, I., Kalinowski, M., Souza, U., Murta,L., Braganholo, V.:
An efficient similarity- for comparing XML documents. Information
Systems78, 40–57 (2018). https://doi.org/10.1016/j.is.2018.07.001
26. de Oliveira, H.R., Tavares, A.T., L ́oscio, B.F.: Feedback- data set recommen-
dation for building linked data applications. In: Proceedings of the 8th Interna-
tional Conference on Semantic Systems - I-SEMANTICS ’12. p. 49. ACM (2012)
27. Pei, J., Hong, J., Bell, D.: A novel clustering- to schema match-
ing. In: Proceedings of the international conference on Advances in Information
Systems. . 60–69. Springer (2006)
28. Rahm, E.: Towards large-scale schema and ontology matching. In: Schema match-
ing and mapping, . 3–27. Springer Berlin Heidelberg (2011) Proximity Mining for Pre-filtering Schema Matching37
29. Rahm, E.: The Case for Holistic Data Integration. In: ADBIS. . 11–27 (2016).
https://doi.org/10.1007/978-3-319-44039-2
30. Rahm, E., Bernstein, P.A.: A survey of approaches to automatic schema matching.
VLDB Journal10(4), 334–350 (2001). https://doi.org/10.1007/s007780100057
31. Shvaiko, P.: A Survey of Schema- Matching Approaches. Journal on Data
Semantics3730, 146–171 (2005).
32. Steorts, R., Ventura, S., Sadinle, M., Fienberg, S.: A Comparison of Blocking
Methods for Record Linkage. In: International Conference on Privacy in Statis-
tical Databases. . 253–268 (2014)
33. Suchanek, F.M., Abiteboul, S., Senellart, P.: PARIS : Probabilistic Alignment of
Relations , Instances , and Schema. Proceedings of the VLDB Endowment5(3),
157–168 (2011). https://doi.org/10.14778/2078331.2078332
34. Tan, P.N., Steinbach, M., Kumar, V.: Introduction to data mining. Pearson Edu-
cation (2006)
35. Terrizzano, I., Schwarz, P., Roth, M., Colino, J.E.: Data Wrangling: The Challeng-
ing Journey from the Wild to the Lake. In: 7th Biennial Conference on Innovative
Data Systems Research CIDR’15 (2015)
36. Vanschoren, J., van Rijn, J.N., Bischl, B., Torgo, L.: OpenML: networked science
in machine learning. ACM SIGKDD Explorations Newsletter15(2), 49–60 (2014) Dataversifying Natural Sciences:
Pioneering a Data Lake Architecture for Curated
Data-Centric Experiments in Life & Earth Sciences
Genoveva Vargas-Solar
1
, Jérôme Darmont
2
, Alejandro Adorjan
4
, Javier A. Espinosa-Oviedo
1,3
,
Carmem Hara
5
, Sabine Loudcher
2
, Regina Motz
6
, Martin Musicante
7
and
José-Luis Zechinelli-Martini
8
1
CNRS, Univ. Lyon, INSA Lyon, UCBL, LIRIS, UMR5205, F-69221, France
2
Université de Lyon, Lyon 2, UR ERIC 5 avenue Mendès France, 69676 Bron Cedex, France
3
CPE Lyon, 43 Blvd. du 11 Novembre 1918, 69616 Villeurbanne Cedex, France
4
Unversidad ORT, Montevideo, Uruguay
5
Universidade Federal do Paranà, Dept. de Informatica, Curitiba - PR, 81531-980, Brazil
6
Instituto de Computación (INCO) Facultad de Ingeniería, Universidad de la Repúbica, Uruguay
7
Universidad Federal Rio Grande do Norte, DIMAP, Natal, Brazil
8
Fundación Universidad de las Américas, Puebla Exhacienda Sta. Catarina Mártir s/n 72820 San Andrés Cholula, Mexico
Abstract
This vision paper introduces a pioneering data lake architecture designed to meet Life & Earth sciences’ burgeoning data
management needs. As the data landscape evolves, the imperative to navigate and maximise scientific opportunities has never
been greater. Our vision paper outlines a strategic to unify and integrate diverse datasets, aiming to cultivate a
collaborative space conducive to scientific discovery. The core of the design and construction of a data lake is the development
of formal and semi-automatic tools, enabling the meticulous curation of quantitative and qualitative data from experiments.
Our unique "research-in-the-loop" methodology ensures that scientists across various disciplines are integrally involved in the
curation process, combining automated, mathematical, and manual tasks to address complex problems, from seismic detection
to biodiversity studies. By fostering reproducibility and applicability of research, our enhances the integrity and
impact of scientific experiments. This initiative is set to improve data management practices, strengthening the capacity of
Life & Earth sciences to solve some of our time’s most critical environmental and biological challenges.
Keywords
Life and Earth sciences, data-driven experiments, data lake, data curation
1. Introduction
These days, it is relatively easy and inexpensive to ac-
quire massive amount of data, even in continuous mode.
This has been no different for experimental and observa-
tional sciences Life & Earth sciences. Accessibility
to data about the Earth and its biodiversity, with varying
levels of provenance, quality and reliability, opens up the
possibility of constructing different perspectives on the
phenomena observed, leading to scientific conclusions
with different depths that target a wide range of knowl-
Published in the Workshop Proceedings of the EDBT/ICDT 2024 Joint
Conference (March 25-28, 2024, Paestum, Italy).
*
Genoveva Vargas-Solar.
†
The authors’ list is alphabetical except for the first two authors.
$genoveva.vargas-solar@cnrs.fr (G. Vargas-Solar);
jerome.darmont@univ-lyon2.fr (J. Darmont); aadorian@gmail.com
(A. Adorjan); javier.espinosa@liris.cnrs.fr (. J. A. Espinosa-Oviedo);
carmemhara@ufpr.br (C. Hara); sabine.loudcher@univ-lyon2.fr
(S. Loudcher); rmotz@fing.edu.uy (R. Motz); mam@dimap.ufrn.br
(M. Musicante); joseluis.zechinelli@udlap.mx (J. Zechinelli-Martini)
©2024 Copyright©2024 for this paper by its authors. permitted under Creative Commons
License Attribution 4.0 International (CC BY 4.0).
CEUR
Workshop
Proceedings
http://ceur-ws.org
ISSN 1613-0073
CEUR Workshop Proceedings (CEUR-WS.org)
edge consumers (civilians, decision-makers, scientists).
Traditionalschema-on-writeapproaches, such as the
Extraction, Transformation and Loading (ETL) process,
are ineffective for the data management requirements of
these experimental sciences. Data lakes are becoming
increasingly common for the management and analysis
of massive data. Data lakes are repositories that store raw
data in its original format. They can be well adapted for
storing data harvested from digital sources (observation
stations), social media, Web and in situ collectors.
The extraction of value through data-driven experi-
ments in the Life & Earth sciences is determined by two
main elements:
•
The maintenance of metadata gathering the con-
ditions under which experiments are performed
(quantitative perspective) to preserve the mem-
ory of the experimental process of knowledge
production process, and to enable understanding
and reproducibility.
•An open science perspective that can go beyond
data sharing and must consider the sharing of
arXiv:2403.20063v1 [cs.DB] 29 Mar 2024 know-how, decision-making, expertise, project
management, and people within the projects that
define the research must be considered.
This vision paper introduces our to designing
and building a data lake for collecting and integrating
data and meta data of Life & Earth sciences’ data-driven
experiments.
The remainder of the paper is organised as follows. 2 gives a general overview of approaches that ad-
dress curating and managing knowledge in Life & Earth
sciences. 3 describes the challenges associated
with curating data and data-driven experiments in Life &
Earth sciences often guided by researchers. In particular,
the gives the general challenges for building data
lakes containing curated data and producing knowledge
derived from data-driven experiments. 4 intro-
duces the general principle for building, maintaining and
exploiting a data lake. The data lake allows the creation
of "dataverses" that can export the history of the develop-
ment of experimental processes that lead to knowledge
in Life & Earth sciences. Finally, 5 concludes the
paper and discusses future work.
2. Related work
We introduce the main topics and approaches that un-
derline the vision of maintaining and sharing data to
perform data-driven experiments: data harvesting tools,
data curation techniques, data labs, data lakes, science
lakes and dataverses.
2.1. Data harvesting
Data available on the Web play a determining role in
decision-making in personal and corporate life. Collect-
ing and storing this data in a structured model helps inte-
grate them with other sources and the dataset in var-
ious applications, such as event detection and sentiment
monitoring. Online newspapers are essential sources of
information, accessed daily by thousands of people.
Various works in the literature report manual efforts to
extract data from pages on theWeb[1,2]. However, these
efforts have been eased by applying Web scraping tech-
niques. Some work complements automated extraction
processes to obtain clean and analysed data by imple-
menting curation procedures [3]. Among the various
existing tools available on theWebfor data extraction,
we can highlight ParseHub
1
is a web scraping tool that
facilitates data extraction from websites through an in-
teractive click- interface, saving the data directly to
the cloud in JSON and CSV formats. It navigates through
continuation pages and captures complete news articles,
1
https://www.parsehub.com/
with the ability to collect data on specific character
sequences. 80legs
2
offers sequential data extraction from
websites. Octoparse
3
simplifies the data extraction pro-
cess by enabling users to create a scraping workflow with
clicks. It includes features URL and string lists for
targeted scraping and ready-to- templates for popular
sites Amazon and Google. FactExtract [3] is tailored
for aggregating content from specific Senegalese news
sources, boasting automatic language detection for ten
languages, data cleaning, and analysis, all whilst avoid-
ing data duplication. This tool, which utilises Python’s
Newspaper library, also features automated daily updates
for the news content it monitors.ENoW- News Data Ex-
tractor from theWeb
4
is a news scrapping system that ex-
plores online newspapers.ENoWreceives search strings
as input and stores in a relational database data extracted
from the news and their full content.
2.2. Data curation
According to Garcov ., [4], research data curation
is “preparing research data and artefacts for sharing and
long-term preservation”. Research repositories are the
standard for publishing data collections to the research
communities. Datasets at an early collection stage are
generally not ready for analysis or preservation. Thus,
extensive preprocessing, cleaning, transformation, and
documentation actions are required to support usability,
sharing, and preservation over time [5]. Curated data
collections have the potential to drive scientific progress
[6], are relevant for reproducibility and improve the reli-
ability of sciences [7]. However, data curation introduces
challenges for supporting data-driven applications [8]
adopting quanti-qualitative methods. For example, re-
search challenges curating material across time, space
and collaborators [7]. Quantitative and qualitative re-
search methodologies apply ad-hoc data curation strate-
gies that keep track of the data that describe the tools,
techniques, hypothesis, and data harvesting criteria de-
fined a priori by a scientific team.
Several software tools that apply statistical techniques
and machine learning algorithms are available for quali-
tative researchers. Woods . [9] argue that Computer-
Assisted Qualitative Data Analysis Software (CAQDAS)
is a well-known tool for qualitative research. These tools
support qualitative techniques and methods for apply-
ing Qualitative Data Analysis (QDA). ATLAS.ti [10], De-
doose [11], MAXQDA [12], NVivo [13] implement the
REFI-QDA standard, an interoperability exchange for-
2
https://80legs.com/
3
https://www.octoparse.com/
4
L Reips, M Musicante, G Vargas-Solar, ATR Pozo, C.S Hara, ENoW-
Extrator de Dados de Notícias da Web, Demonstration Anais Esten-
didos do XXXVIII Simpósio Brasileiro de Bancos de Dados, 2023,
78-83 mat. CAQDAS [14] researchers and practitioners can
perform annotation, labelling, querying, audio and video
transcription, pattern discovery, and report generation.
Furthermore, CAQDAS tools allow the creation of field
notes, thematic coding, search for connections, memos
(thoughtful comments), contextual analysis, frequency
analysis, word location and data analysis presentation
in different reporting formats [15]. The REFI-QDA (Rot-
terdam Exchange Format Initiative)
5
the standard allows
the exchange of qualitative data to enable reuse in QDAS
[16]. QDA software such as ATLAS.ti [10], Dedoose [11],
MAXQDA [12], NVivo [13], QDAMiner [17], Quirkos
[18] and Transana [19] adopt REFI-QDA standard.
We assume that data curation consists of identifying,
systematizing, managing, and versioning research data,
considering versioning artefacts an essential component
of tracking changes along the research project.
2.3. Data labs
Data science environments provide data labs Kag-
gle
6
and Dryad
7
with stacks of services for (externalised)
data storage, tagging and exploring tools. These environ-
ments allow a collective sharing space of highly curated
data collection maintenance tools. There are specialised
repositories DataOne
8
and data repositories re3data
9
.
DataONE (Data Observation Network for Earth) is a
community-driven project that provides to various
environmental and ecological data across multiple mem-
ber repositories. It is designed as an innovative frame-
work aimed at facilitating research and enabling scien-
tists and researchers to preserve, , , and increase
the impact of their data. The platform provides robust
data management tools, ensuring datasets’ preservation
and integrity. DataONE underscores data stewardship
as a federated resource and supports scientific collabora-
tion and reproducibility. It is invaluable for researchers
seeking to address complex environmental challenges
through shared data and knowledge.
Re3data is a global registry of research data reposito-
ries that offers a comprehensive directory for researchers
seeking to , store, share, and manage their datasets.
It represents a variety of academic disciplines and pro-
vides detailed information about each repository, such
as policies, standards, and contact details. re3data
promotes data sharing, visibility, and reuse as a critical
reference point for finding suitable repositories for data
deposition. The platform enhances transparency in re-
search data management. It supports open science by
guiding users to trustworthy and reliable repositories,
5
https://www.qdasoftware.org
6
kaggle.com
7
https://datadryad.org/stash
8
https://www.dataone.org/about/
9
https://www.re3data.org
thereby facilitating the discovery of high-quality data
across different scientific fields.
2.4. Data lake, science lake and dataverse
Data lakesare expansive storage repositories that hold
vast raw data in their native format until needed. Stein
and Morrison [20] emphasised their potential for scala-
bility and flexibility in handling big data from various
sources. In recent studies, Dixon in 2010
10
defined the
term and its initial application in big data analytics. Quix . (2016) [21] delved into the architectural consid-
erations and challenges such as data governance and
metadata management.
Science lakes, an offshoot of data lakes, are tailored
specifically for the scientific community to address the
need for interdisciplinary research, data management
and complex analytics. Russom (2016) [22] suggested
that science lakes provide a more discipline-specific ap-
proach to data handling, enabling better metadata cura-
tion and domain-specific data models, which are crucial
for reproducibility in scientific research.
A data lake is a vast storage system that houses exten-
sive volumes of raw data in its original format. This ver-
satile system accommodates a range of data types, includ-
ing structured, semi-structured, and unstructured forms.
Data lakes are essential in environments focused on big
data analytics and are designed to manage data charac-
terised by large volume, high velocity, and diverse variety
from multiple sources. They are commonly utilised for ad-
vanced data processing activities such as machine learn-
ing and predictive analytics. Unlike traditional databases
following the schema-on-write , data lakes fol-
low the schema-on-read , providing flexibility
in how data is formatted and .
Dataverse.The concept of dataverse takes the no-
tion of data lakes further by creating a networked space
where data is stored, actively managed, and shared within
the scientific community. A dataverse is a data repos-
itory platform for publishing, citing, and discovering
datasets. It enables researchers to publish, cite, and dis-
cover datasets while providing metadata and tools to
ensure others can understand and data. Dataverses
are often domain-specific and support the principles of
open science, providing features such as data version
control, digital object identifiers (DOIs) for citation, and
tools for data analysis within the platform. They are
community-driven and emphasize the accessibility and
reusability of research data.
The most prominent example is the open-source Data-
verse project developed by the Institute for Quantita-
tive Social Science at Harvard University. The Dataverse
10
https://jamesdixon.wordpress.com/2014/09/25/
data-lakes-revisited/ Project, initiated by King [23], provides an open-source
platform for sharing, preserving, citing, exploring, and
analysing research data. It focuses on data citation and
reproducibility, as discussed by Crosas [24], who high-
lighted the platform’s role in fostering collaboration and
open science.
Different academic institutions have built their data-
verses for sharing and disseminating experimental sci-
entific results, including the data collections they curate:
University of Arizona
11
, the Different universities and
academic institutions have promoted their dataverses the University of Hamburg
12
, the University of Michi-
gan
13
and the Grenoble Dataverse
14
.
Summary.Together, these systems represent a shift
toward more open, integrated, and efficient ecosystems
for data management, offering novel solutions to the
challenges posed by the vast amounts of data generated
in modern research. They move away from traditional
databases and toward more fluid, dynamic systems that
can accommodate the ever-changing landscape of big
data and scientific research.
A dataverse and a data lake are concepts related to data
storage and management but serve different purposes and
are designed with varying cases of in mind. While
a dataverse is a scholarly platform aimed at curating,
sharing, and preserving research data with rich metadata
and community collaboration features, a data lake is a
more generalised and scalable storage solution for raw
data to support diverse data analytics and processing
workflows.
2.5. Data lakes and data verses in Life &
Earth sciences
Dataverses in Life & Earth sciences are specialised digital
infrastructures designed to address specific data manage-
ment needs for these scientific domains. They provide a
structured yet flexible environment where datasets can
be stored, accessed, shared, and analysed. These data-
verses typically offer robust metadata standards and tools
to ensure their data are well-described, making them dis-
coverable and usable for various research purposes.
In Life Sciences, dataverses often focus on genomics,
proteomics, clinical trials, and other biological data, in-
tegrating various sources of information to aid in com-
plex analyses phenotype-genotype correlations. For
Earth Sciences, dataverses might concentrate on geospa-
tial data, climate models, seismic activity records, and
ecological data, supporting efforts to understand and
model the Earth’s dynamic systems.
11
https://arizona.figshare.com
12
https://www.fdm.uni-hamburg.de/en/fdm.html
13
https://www.icpsr.umich.edu/web/about/cms/2365
14
https://scienceouverte.couperin.org/cellule-data-grenoble-alpes/
These repositories support open science by promoting
data sharing across disciplinary boundaries. This fea-
ture enables researchers to replicate studies and build
upon existing work, which is fundamental for advancing
knowledge. They also facilitate interdisciplinary collabo-
ration, allowing experts from different fields to contribute
to and draw from a collective data pool. For instance, a
dataverse in these fields might include a combination of
high-throughput experimental data, field observations,
and simulation outputs. The combination of openness
and rigorous data management positions dataverses as
critical resources in pursuing scientific discovery in Life
& Earth sciences.
In life and earth sciences, data lakes are pivotal for con-
solidating scientific data collected from various biodiver-
sity studies and geological events earthquakes. Once
curated, processed, and analysed, this data contributes
significantly to data-driven experiments underpinned by
well-established protocols. The harvested data enriches
the data lake and supports the creation of detailed, cu-
rated views for dissemination through dataverses.
Our vision emphasises the importance of developing
and maintaining data lakes with partially curated con-
tent in life and earth sciences, facilitating the continuous
cycle of experimental data feeding back into the lake and
subsequently sharing via dataverses.
3. Maintaining and sharing earth
and life sciences knowledge:
challenges
Various data on life and earth sciences have been ac-
quired from different sources [25]. Integrated to
data collections and their curated versions can facilitate
their maintenance, analysis and experimentation. It can
also demonstrate knowledge of the discipline with its vo-
cabulary, concepts and relationships in a synthetic way.
Curation, maintenance and exploration of data collec-
tions in the data lake calls for proposing techniques for
exploring data collections that can be explored and en-
riched while producing data and analytical results.
Data curation also means keeping track of the type of
experiments carried out on the data, their results and the
conditions under which they were carried out. Maintain-
ing a catalogue of data-related questions and experiments
can promote open science, share data and knowledge, and
share the data and knowledge the scientific community
has gained from it [26]. This information should also be
stored in the data lake.
Challenge 1: How to structure and organise life and
earth sciences metadata?Metadata modelling is a
way of structuring and organising earthquakes and biodi- versity. The metadata model must make the content of a
data lake findable, accessible, interoperable and reusable
(FAIR principles [27]). Metadata can represent the data’s
structural, semantic and contextual aspects (provenance,
conditions and assumptions under which the analytical
results are obtained, i.e., the metadata driving the analy-
sis). Most proposed models are on logic or struc-
tured by graphs [28,29] that can be specialised in seismic
geophysical data and biodiversity. Besides, associating
metadata can be achieved by considering quantitative
and qualitative perspectives through data curation. Com-
bining quantitative and qualitative approaches allows
for a meta-model of the content and produced in
experiments and the conditions in which the content is
produced, chosen, validated and considered representa-
tive knowledge for the domain of study.
Challenge 2: How to integrate data in the data lake?
Since the experiments require several data collections,
integrating the data into the data lake must be part of
a pipeline that includes data discovery, exploration, se-
lection and integration. This process should be designed on the requirements of life and earth science exper-
iments [25]. The heterogeneity of the data (, signals,
multimedia, proprietary formats from seismographs), the
speed of the data often produced in the form of streams in
the case of seismic sensors in addition to the volume are
aspects that require original contributions in the design,
maintenance and exploration of the data lake.
Challenge 3: How to integrate data in the data lake
considering scientists’ needs?The researcher’s in-
tervention, defined as a researcher-in-the-loop (RITL)
[30], is a crucial aspect of human intervention to assess
content concerning (i) the conditions in which it is pro-
duced and (ii) to make decisions about the tasks
to perform and the way a research project will move
forward. RITL is a case of Human-in-the-loop (HITL),
where the primary output of the process is a selection
of the data, not a trained machine-learning model. HITL
is crucial for handling supervision, exception control,
optimisation, and maintenance [31,32]. Under a RITL
, a human sees all data points in the relevant
selection at the end of the process. RITL requires
a systematic solid way of working
15
. This characteristic
is critical for designing content curation for quantitative
and qualitative research methods.
Scientific content should be extracted and computed,
including data, analytics tasks (manual and AI models),
and associated metadata. This curated content allows the
produced knowledge to be reusable and analytics results
to be reproducible [33], thereby adhering to the FAIR
principles [34].
15
https://hai.stanford.edu/news/humans-loop-design-interactive-ai-systems
4.Towards a curation for
building a Life & Earth sciences
data lake 1 illustrates the principle of our vision concerning
the way a life and earth sciences data lake can be built,
maintained and exploited. Our is on the
quantitative and qualitative curation of data harvested
digitally andin situ(left-hand side of the ). Hetero-
geneous raw data is gathered and stored in the data lake.
Then, algorithms (statistical and Artificial Intelligence)
and researchers can process, filter and classify data. This
filtering process produces and stores meta-data in the
data lake. Data exploration and integration (cleaning and
engineering) processes can be performed on data samples
from the data lake. They can be for experimental
purposes to produce content associated with the data
stored in the data lake. Clean and curated data associ-
ated with meta-data representing the quantitative and
qualitative perspective of the experiments can then be
shared in a data verse (right-hand side of the ).
Harvested data, models and knowledge integra-
tion.Various life and earth sciences data have been
harvested from different sources. Since they are hetero-
geneous and produced at different paces (continuous and
in batch), our proposes an integration on a pivot meta-representation. The principle is
to present a general meta-model of their content and
process them for extracting technical, structural and se-
mantic meta-data. This abstract representation provides
integrated to data collections and curated versions
under a global knowledge graph and can promote their
maintenance, analysis, and experimentation. It can also
show the knowledge of the discipline with its vocabu-
lary, concepts, and relations in a synthetic manner. The
data lake can be pivotal in collecting, processing, and
exporting raw data in a curated view.
Curation, maintenance, and exploration of data
collections for bringing data value from in situ ob-
servations and experiments.Since data acts as a
backbone in modelling phenomena for understanding
their behaviour, it is critical to developing good collec-
tion and maintenance: which are available data collec-
tions? Are they complete? Which is their provenance?
In which conditions were they collected? Have they been
processed? In which cases have they been , and what
are the associated results? We propose techniques to ex-
plore data collections graphs that can be explored
and enriched while data and analytics results are
produced. Data curation also means keeping track of the
type of experiments run on data, their results, and the
conditions in which they were performed. Maintaining a 1:General overview of the curation for building, maintaining and exploiting a data lake.
catalogue of data-related questions and experiments can
promote open science and share data and the knowledge
that the scientific community has derived from it.
Modelling and simulating experiments to answer
questions in life and earth sciences.Answering
research questions through data-driven experiments im-
plies:
•
Designing ad hoc experiment artefact models and
programming languages for enabling friendly,
context-aware, and declarative construction of
experiments in life and earth sciences.
•Collecting execution of experiments data (raw
input data, prepared datasets, experiments’ tasks
calibration and associated results).
Pilot experiments.The data lake will be tested in scenarios through collaboration with domain experts in
seismology and biodiversity studies in Brazil. The entry
point will be two pilot experiments, namely:
1.the classification process of seismic signals col-
lected by stations through different observations
to detect "natural" and human-made earthquakes
in the northern human-made earthquakes in the
northern region of Brazil;
2.the classification ofin situobservations of the
"carabela portuguesa"
16
and modelling its be-
haviour on the Brazilian coast.
16
The Portuguese caravel (Physalia physalis) is a monotypic colonial
species of siphonophore hydrozoan of the family Physaliidae. It
is commonly found in the open ocean in all warm waters of the
In both cases, it is necessary to (i) apply statistical
methods to investigate and unveil patterns in seisms
and biodiversity data, answering open problems or lead-
ing to research questions; (ii) build predictive models
to better describe or approximate phenomena, increas-
ing the knowledge about our planet. The conditions in
which statistics and prediction are performed, results,
observations, interpretation and validation of the results
are data to be integrated into the data lake.
Discussion.The originality of the work is to address
the construction of a data lake that includes:
1.
Raw collected data representing life and earth
sciences phenomena (streams, batch, multimedia,
proprietary).
2.
Data produced along data-driven experiments
adopting data science techniques including ar-
tificial intelligence algorithms (ML-driven data
lakes).
3.Contextual data describing the conditions in
which data are collected, and experiments are
designed and enacted. The data lake will provide
data curation modules for extracting metadata
according to a well-adapted model and modules
exploring data and them for designing experimentations, thereby adopting an open sci-
ence perspective.
world, especially in the tropical and subtropical regions of the
Pacific and Indian Oceans, as well as in the Atlantic Gulf Stream.
Its sting is dangerous and very painful https://es.wikipedia.org/
wiki/Physalia_physalis. 5. Conclusions and future work
Our vision is that it is necessary to address fundamental
research topics at the centre of Data Science, Big Data
management and analytics for solving data-driven prob-
lems in life and earth sciences.
The contribution is the design and exploration tech-
niques of a data lake with a well-adapted model for meta-
data about life and earth sciences experiments consuming
and producing quantitative and qualitative data. An im-
portant work will be to define exploration operators and
pipelines to exploit the content for further maintaining
and developing life and earth sciences experiments.
6. Acknowledgements
The work reported in this paper is done in the context of
the LETITIA
17
project, funded by theFédération Informa-
tique de Lyon
18
.
References
[1]
G. Vargas-Solar, J.-L. Zechinelli-Martini, J. A.
Espinosa-Oviedo, L. M. Vilches-Blázquez, Laclichev:
Exploring the history of climate change in latin
america within newspapers digital collections, in: Trends in Database and Information Systems:
ADBIS 2021 Short Papers, Doctoral Consortium and
Workshops: DOING, SIMPDA, MADEISD, Mega-
Data, CAoNS, Tartu, Estonia, August 24-26, 2021,
Proceedings, Springer, 2021, . 121–132.
[2]L. S. do Nascimento, C. S. Hara, M. N. Junior, M. No-
ernberg, Redes sociais como uma fonte de dados
alternativa para monitorar águas-vivas no brasil,
in: Livro de Memórias do IV SUSTENTARE e VII
WIPIS: Workshop internancional de Sustentabili-
dade, Indicadores e Gestão de Recursos Hídricos
(Online) – Even3, Piracicaba, 2022.
[3]E. N. Sarr, S. Ousmane, A. Diallo, Factextract:
automatic collection and aggregation of articles
and journalistic factual claims from online news-
paper, in: 2018 Fifth International Conference on
Social Networks Analysis, Management and Secu-
rity (SNAMS), IEEE, 2018, . 336–341.
[4]D. Garkov, C. Müller, M. Braun, D. Weiskopf,
F. Schreiber, " research data curation in visualiza-
tion: Position paper"(data) (2023).
[5]
S. Lafia, A. Thomer, D. Bleckley, D. Akmon,
L. Hemphill, Leveraging machine learning to detect
data curation activities, in: 2021 IEEE 17th Inter-
national Conference on eScience (eScience), IEEE,
2021, . 149–158.
17
http://vargas-solar.com/letitia/
18
https://fil.cnrs.fr
[6]A. Zuiderwijk, R. Shinde, W. Jeng, What drives and
inhibits researchers to share and open research
data? A systematic literature review to analyze
factors influencing open research data adoption,
PloS One 15 (2020).
[7]M. Vuorre, J. P. Curley, Curating research assets: A
tutorial on the git version control system, Advances
in Methods and Practices in Psychological Science
1 (2018) 219–236.
[8]M. Esteva, W. Xu, N. Simone, K. Nagpal, A. Gupta,
M. Jah, Synchronic curation for assessing reuse
and integration fitness of multiple data collections
(2022).
[9]M. Woods, R. Macklin, G. K. Lewis, Researcher
reflexivity: exploring the impacts of caqdas , In-
ternational Journal of Social Research Methodology
19 (2016) 385–403.
[10]ATLAS.ti, ATLAS.ti, https://atlasti.com, last ac-
cessed April 2023.
[11]Dedoose, Dedoose, https://www.dedoose.com/, last
accessed April 2023.
[12]V. Software, Maxqda, http://maxqda.com, last ac-
cessed April 2023.
[13]
NVivo, Nvivo, https://www.qsrinternational.com/,
last accessed April 2023.
[14]
N. Chen, M. Drouhard, R. Kocielnik, J. Suh,
C. Aragon, machine learning to support
qualitative coding in social science: Shifting the fo-
cus to ambiguity, ACM Transactions on Interactive
Intelligent Systems 8 (2018) 1–20.
[15]
J. C. Evers, Current issues in qualitative data analy-
sis software (qdas): A and developer perspec-
tive, The Qualitative Report 23 (2018) 61–73.
[16]
S. Karcher, D. D. Kirilova, C. Pagé, N. Weber, How
data curation enables epistemically responsible
reuse of qualitative data, The Qualitative Report 26
(2021) 1996–2010.
[17]QDAMiner, Qdaminer, https://provalisresearch.
com/products/qualitative-data-analysis-software/,
last accessed April 2023.
[18]
Quirkos, Quirkos, https://www.quirkos.com, last
accessed April 2023.
[19]
Transana, Transana, https://www.transana.com,
last accessed April 2023.
[20]
C. Giebler, C. Gröger, E. Hoos, H. Schwarz,
B. Mitschang, Leveraging the data lake: Current
state and challenges, in: Big Data Analytics and
Knowledge Discovery: 21st International Confer-
ence, DaWaK 2019, Linz, Austria, August 26–29,
2019, Proceedings 21, Springer, 2019, . 179–188.
[21]
R. Hai, C. Quix, M. Jarke, Data lake concept and
systems: a survey, arXiv preprint arXiv:2106.09592
(2021).
[22]P. Russom, Data warehouse modernization, TDWI
Best Pract Rep (2016). [23]G. King, An introduction to the dataverse network
as an infrastructure for data sharing, 2007.
[24]M. Crosas, G. King, J. Honaker, L. Sweeney, Au-
tomating open science for big data, The ANNALS
of the American Academy of Political and Social
Science 659 (2015) 260–273.
[25]U. S. da Costa, J. A. Espinosa-Oviedo, M. A. Mu-
sicante, G. Vargas-Solar, J.-L. Zechinelli-Martini, provenance in data analytics for seismology:
Challenges and directions, in: European Confer-
ence on Advances in Databases and Information
Systems, Springer, 2022, . 311–322.
[26]A. Adorjan, G. Vargas-Solar, R. Motz, Towards
a human-in-the-loop curation: A qualitative per-
spective, in: 2022 IEEE/ACS 19th International
Conference on Computer Systems and Applications
(AICCSA), IEEE, 2022, . 1–8.
[27]M. D. Wilkinson, M. Dumontier, I. J. Aalbersberg,
G. Appleton, M. Axton, A. Baak, N. Blomberg, J.-W.
Boiten, L. B. da Silva Santos, P. E. Bourne, .,
The fair guiding principles for scientific data man-
agement and stewardship, Scientific data 3 (2016)
1–9.
[28]
E. Scholly, P. N. Sawadogo, P. Liu, J. A. Espinosa-
Oviedo, C. Favre, S. Loudcher, J. Darmont, C. Noûs,
goldmedal: une nouvelle contribution à la mod-
élisation générique des métadonnées des lacs de
données, Revue des Nouvelles Technologies de
l’Information (2021).
[29]
A. Diouan, E. Ferey, S. Loudcher, J. Darmont,
C. Noûs, Métadonnées des lacs de données principes fair, in: 18e journées Business Intelli-
gence Big Data (EDA 2022), 2022.
[30]
R. Van de Schoot, J. de Bruin, Researcher-in-the-
loop for systematic reviewing of databases,
Zenodo: SciNLP: Natural Language Processing and
Data Mining for Scientific (2020).
[31]I. Rahwan, Society-in-the-loop: programming the
algorithmic social contract, Ethics and information
technology 20 (2018) 5–14.
[32]E. Mosqueira-Rey, E. Hernández-Pereira, D. Alonso-
Ríos, J. Bobes-Bascarán, Á. Fernández-Leal, Human-
in-the-loop machine learning: A state of the art,
Artificial Intelligence Review 56 (2023) 3005–3054.
[33]
J. Leipzig, D. Nüst, C. T. Hoyt, K. Ram, J. Greenberg,
The role of metadata in reproducible computational
research, Patterns 2 (2021) 100322.
[34]
P. P. F. Barcelos, T. P. Sales, M. Fumagalli, C. M. Fon-
seca, I. V. Sousa, E. Romanenko, J. Kritz, G. Guiz-
zardi, A fair model catalog for ontology-driven
conceptual modeling research, Conceptual Model-
ing. ER 73 (2022). Proceedings of the 7th Workshop on Challenges and Applications
of Automated Extraction of Socio-political Events from (CASE 2024), pages 1–5
March 22, 2024 ©2024 Association for Computational Linguistics
The Future of Web Data Mining: Insights from Multimodal and
Code- Extraction Methods
Evan Fellman
∗
Carnegie Mellon Univeristy
efellman@cs.cmu.edu
Jacob Tyo
∗
DEVCOM Army Research Laboratory
Carnegie Mellon Univeristy
jacob.p.tyo.civ@army.mil
Zachary C. Lipton
Carnegie Mellon Univeristy
Abstract
The extraction of structured data from web-
sites is critical for numerous Artificial Intel-
ligence applications, but modern web design
increasingly stores information visually in im-
ages rather than in . This shift calls into
question the optimal technique, as language-
only models fail without textual cues while multimodal models GPT-4 promise image
understanding abilities. We conduct the first
rigorous comparison between - and
vision- models for extracting event meta-
data harvested from comic convention websites.
Surprisingly, our results between GPT-4 Vi-
sion and GPT-4 uncover a significant ac-
curacy advantage for vision- methods in
an apples-to-apples setting, indicating that vi-
sion models may be outpacing language-alone
techniques in the task of information extrac-
tion from websites. We release our dataset and
provide a qualitative analysis to guide further
research in multimodal models for web infor-
mation extraction.
1 Introduction
The extraction of structured information from web-
sites represents a critical challenge in the field of
Artificial Intelligence (AI), especially in the con- of rapidly evolving web technologies. As the
virtual world becomes increasingly central to di-
verse aspects of society, the ability to efficiently
and accurately mine web data is of high importance.
This task, commonly known as web scraping, en-
tails navigating the complexities of varied website
architectures to extract useful information. The
ubiquity of dynamic, visually-rich, and interactive
content in modern web design further complicates
this landscape, presenting a formidable challenge
for automated data extraction technologies.
Historically, web scraping has been dominated
by rule- systems (Gulhane ., 2011)
(Lockard ., 2018), meticulously designed to
accommodate the specific structures of individ-
ual websites. The inherent diversity in web de-
sign necessitates a tailored for each site,
significantly limiting the scalability of these sys-
tems. Moreover, the dynamic nature of web con-
tent, where a single page may present different
types of data on interaction or other fac-
tors such as location or time, adds another layer of
complexity. Because of the bespoke nature, rule- systems often struggle to adapt to dynamic
elements, often requiring manual intervention for
maintenance and updates.
In the realm of machine learning (ML), the appli-
cation to web scraping presents unique challenges.
The vast differences between websites render the
tuning of existing ML systems a daunting task. In
most cases, ML- scraping methods must op-
erate in a zero-shot or few-shot setting, where the
model has little to no prior exposure to the specific
website from which data is to be extracted. This
scenario places a heavy reliance on the innate ca-
pabilities of the model to generalize across highly
varied environments, a task that has traditionally
proven to be challenging for ML systems. As a
result, these methods have often been less effective
than their rule- counterparts.
The advent of advanced multimodal AI mod-
els has signaled a potential paradigm shift in web
scraping methodologies. Pioneering models such
as GPT-4 (OpenAI, 2023) and LLaVA (Liu .,
2023) have demonstrated remarkable capabilities
in dealing with complex, multimodal data. These
models are equipped to understand and interpret in-
formation that spans across , images, and other
web elements, offering a more holistic to data extraction. Their prowess in zero-shot per-
formance, where the model can generate useful
responses without prior specific training on a task,
suggests a significant potential for application in
web scraping.
Despite these advancements, the field lacks a
1 comprehensive and rigorous analysis of such multi-
model AI models in the context of extracting prac-
tical web data. This gap in research motivates our
current study, where we aim to critically evaluate
and compare the effectiveness of these cutting-edge
techniques in web scraping. Our contributions are
as follows:
•A dataset, FanConInfo, of comic convention
websites complete with cleaned HTML, a ren-
dered screenshot, and human-annotated la-
bels.
•A rigorous analysis of GPT-4 Vision, GPT-4
, and GPT3.5 in extracting information
from FanConInfo. We find that leveraging
information from a screen capture of a website
boosts the accuracy of information extraction
by over 20%.
•
An error analysis of the methods guiding fu-
ture work. We find that the vision model pre-
dictions align most with human preferences.
2 Related Works
Information extraction from websites has tradition-
ally relied on processing raw HTML code and other
- structures. Hao . (2011) presents
a dataset of HTML code with well-defined tasks.
For example, on a webpage that describes a book,
the dataset asks a system to retrieve the title, au-
thor, ISBN-13, publisher, and publish-date the HTML. Both Hao . (2011) and DOM-
LM (Deng ., 2022; Zhou ., 2021) aim to
simplify the DOM tree and feed simplified em-
beddings to dense models, achieving state-of-the-
art results on benchmarks. More recently, Large
Language Models (LLMs) have been to either
directly extract information from website HTML,
or to generate a Python program to extract the in-
formation from the HTML (Arora ., 2023).
They found this method, (Arora ., 2023), out-
paces methods directly RoBERTa (Liu .,
2019) to answer questions, a zero-shot relation ex-
traction method (Lockard ., 2020) and DOM-
LM (Deng ., 2022). However, these language-
only approaches are intrinsically limited when data
is stored visually.
Research on pairing vision and language capa-
bilities together in a single model has made rapid
progress in interpreting images with , with mod-
els GPT-4 demonstrating excellent extrac-
tion capabilities from structured documents (Ope-
nAI, 2023), even establishing a state-of-the-art
on the Visual Question Answering (TextVQA)
dataset (Singh ., 2019), a dataset designed to
challenge model’s ability to reason with images.
Research is rapid and prolific in multimodal mod-
eling, including the recent work of the multilingual
PaLI (Chen ., 2023) and the modular system of
mPLUG-2 (Xu ., 2023) for multimodal Ques-
tion Answering (QA).
The dataset by Varlamov . (2022) features
hand-labeled news articles in raw HTML format,
focusing on identifying critical article components titles and publication dates. Similarly, the
Klarna Product Page Dataset (Hotti ., 2022)
contains 51,701 annotated product sale pages for
locating key web elements such as buy buttons and
prices. Additionally, the Boilerplate Detection us-
ing Shallow Features dataset (Kohlschütter ., 2010) includes HTML files labeled to dis-
tinguish main content from extraneous elements advertisements, thus aiding in refining web
scraping accuracy. None of the aforementioned
datasets provide the ability to compare purely and multimodal models on event information
extraction.
3 Methodology
3.1 FanConInfo
To enable a fair comparison between visual and
textual extraction techniques, we curate a novel
dataset, FanConInfo, of comic convention websites
which constitute a diverse corpus spanning a range
of designs, conventions, and web architectures.
We first extract an initial list of upcoming comic
conventions across North America from the aggre-
gator siteFanCons.com, encompassing fan gath-
erings to major comic expos. For each conven-
tion link, we collect a 3456 x 1878 screen capture
and the corresponding HTML content with Sele-
nium (SeleniumHQ, 2023). We remove all CSS
styling and<script>s from the HTML. Following
this, we manually annotate each event with the fol-
lowing attributes: name, start date, end date, and
location.
We manually confirmed that when GPT-4 Turbo the HTML of a webpage and GPT-4 Vision the screenshot of a webpage agree on the con-
vention name, the name is always correct for the
entirety of the dataset. Thus, when the two models
agree perfectly, we consider the response as the
gold answer. When the models disagree, which oc-
2 curs 41% of the time across all rows and columns, a
human determines the gold response. We only eval-
uate performance of methods on items that have
a label. It is conceivable that some webpages do
not list their date nor location, demonstrated in 1, in the above-the-fold portion. In total,
our curated dataset contains 86 comic convention
websites and is available here.
3.2 Models
For our vision- model, we leverage the re-
cently released GPT-4 Vision model from OpenAI,
gpt-4-vision-preview- referred to as GPT-4V.
We prompt the model as follows:
<screen capture placed here>
Get the following information from the given im-
age as a JSON object of strings. Only write the
JSON in your response. If any bit is unknown
then write N/A instead:
Conference Name: <Name of Conference>,
Start Date: <YYYY-MM-DD>,
End Date: <YYYY-MM-DD>,
Location: <Address or other location>
For our code- method, we employ the GPT-
4 (gpt-4-1106-preview- referred to as GPT-4T)
and GPT-3.5 (gpt-3.5-turbo-1106) models from
OpenAI. Rarely when GPT-3.5’s sequence length
is insufficient to accommodate the entire HTML
content, the HTML was truncated. These models
were prompted as follows:
<HTML placed here>
Get the following information from the above
HTML as a JSON object of strings. Only write
the JSON in your response. If any bit is unknown
then write N/A instead:
Conference Name: <Name of Conference>,
Start Date: <YYYY-MM-DD>,
End Date: <YYYY-MM-DD>,
Location: <Address or other location>
3.3 Evaluation
We assess extraction accuracy for 4 key metadata
fields: name, start date, end date, and location. We
combine the start date and end date into one cat-
egory. Since the models never deviated from the
requested format despite variations on the event
pages, a prediction for date is only considered ac-
curate if both are an exact match. To address minor
errors, we evaluate predictions for event names and
locations case-insensitive Exact Match (EM)
accuracy. Fuzzy matching employs the Fuzzy-
Wuzzy Python package (Inc, 2014), measuring:
•Event names: Partial ratio to capture semantic
changes with word order (e.g., "ComicCon"
vs. "Comic Convention").
•Locations: Partial token sort ratio to allow
coherent reordering (e.g., "X Hall, Y Ave.,
City" vs. "Y Ave., City, X Hall").
This balances exact and fuzzy matching
for a comprehensive assessment.
GPTNameDateLocation Avg
3.50.58(0.05) 0.73(0.06) 0.46(0.06) 0.59
4T0.62(0.05) 0.74(0.05) 0.56(0.06) 0.64
4V0.82 (0.04) 0.88 (0.04) 0.86 (0.04) 0.85
Table 1: Exact Match accuracy for on the FanConInfo
Dataset. The Avg column represents the average accu-
racy for each model.
Fuzzy NameFuzzy Location
GPT ScoreAccuracyScoreAccuracy
3.50.88 (0.03) 0.78 (0.05) 0.77 (0.04) 0.62 (0.06)
4T0.91 (0.02) 0.82 (0.04) 0.83 (0.04) 0.75 (0.06)
4V0.95 (0.02) 0.92 (0.03) 0.95 (0.02) 0.94 (0.03)
Table 2: Partial ratio (name) and partial token sort ratio
scores (location) on the FanConInfo Dataset. The score
is the average ratio and the accuracy is calculated on a score threshold of 0.85.
4 Results & Discussion
GPTNameDateLocation Avg
3.50.58(0.05) 0.91(0.05) 0.53(0.07) 0.67
4T0.63(0.05)1.00 (0.00)0.64(0.07) 0.76
4V0.83 (0.04) 1.00 (0.00) 0.87 (0.05) 0.90
Table 3: Exact Match accuracy for on the FanConInfo
Dataset, after removing instances where any of the mod-
els predicted that the information is not available. The
Avg column represents the average accuracy for each
model.
Table 1 shows the visual methodology achieves
an average exact match score of 85% while the top
- methodology achieves an average exact
match score of 64%. When relaxing exact match
criteria fuzzy matching, we see the visual
methodology achieves an average fuzzy score of
95% when retrieving the convention name while
the top code- method achieves an average
fuzzy score of 91% for the same task, as shown in
Table 2. When tasked to retrieve the convention
3 Fuzzy NameFuzzy Location
GPT ScoreAccuracyScoreAccuracy
3.50.89(0.02) 0.79(0.05) 0.86(0.03) 0.71(0.06)
4T0.92(0.02) 0.83(0.04) 0.94(0.02) 0.86(0.05)
4V0.96 (0.02) 0.92 (0.03) 0.98 (0.01) 0.96 (0.03)
Table 4: Partial ratio (name) and partial token sort ra-
tio scores (location) on the FanConInfo Dataset, after
removing instances where any of the models predicted
that the information is not available. The score is the
average ratio and the accuracy is calculated on a
score threshold of 0.85.
location, the visual methodology achieves an aver-
age fuzzy score of 95% while the top code-
method achieves an average fuzzy score of 83%.
Interestingly, GPT-4 Vision was the highest-
performing method across all categories and met-
rics. Because GPT-4 Vision and are the same
model, we conclude there exists an advantage when
rendering web information as a screen capture
in human-readable format versus the traditional
HTML machine code.
We also see that it may not always be necessary
to the biggest and most expensive model. GPT-
3.5 reaches nearly the same performance as GPT-4
, especially when the name is the of
interest. This reinforces the advantage of represent-
ing web information in human-readable format, as
increasing the model capability from GPT-3.5 to
GPT-4 had little effect when presenting the model
with the HTML representation.
We conducted a comparison between the results
of vision- and code- methods when both
indicate the presence of an answer within the pro-
vided mode. The findings are summarized in Tables
3 and 4. Remarkably, even when models express
the existence of an answer, the vision- method
consistently delivers more human responses.
4.1 Error Analysis
GPT-4 Vision’s errors predominantly come from
reading an alternate name prominently displayed,
demonstrated in 1. Occasionally interpret-
ing slogans or other emphasized information rather
than main headers with event details. However, we
do see that the model adapts well to unconventional
designs and heavy visual styling, demonstrated in 2. When given only the HTML, the errors
tend to primarily originate from missing content,
and in some cases, critical information may be ex-
clusively conveyed through images, resulting in
issues for models relying solely on the HTML. 1: Clandestine Comics. GPT-4 Vision read the
wrong part as the event name; it predicted "Maryland’s
Longest-Running Comic Show." 2: Epic Animation Comic Game Fest. Despite
the difficult to read font, GPT-4 Vision was capable of
capturing the name. Meanwhile, the date only appears
within an image.
Interestingly, we find that when both GPT-4 and GPT-4 Vision find a date for an event, Ta-
ble 3, both methods are correct 100% of the time.
The consistent format of dates enables models to
achieve high precision in EM.
5 Conclusion and Future Work
In this work, we carry out the first rigorous compar-
ison on practical website data showing strengths of
emerging visual approaches versus enduring pre-
cision of code for harvesting event details. Our
evaluations reveal superior performance in visual- methods with unparalleled adaptability on
designs with heavy imagery. As visual richness
accelerates across the web, combining modalities
will likely further outpace language-only methods
and overcome the shortcomings from unimodal
methodologies by blending state-of-the-art coding
reasoning with cross-format graphical resilience.
Furthermore, we release our dataset to facilitate
additional development in event information ex-
traction.
More broadly, our findings posit the understand-
ing of complex webpage images as an important
frontier with tangible value for structured data min-
4 ing from online resources. GPT-4 Vision proves
supreme through an average exact match of 85%
and fuzzy matching rates of 95% and 95% for name
and location data, respectively. We provide strong
evidence that rather than competing, effectively
integrating textual and visual cues can pave the
way for next-generation techniques to achieve levels of reliability in -world information ex-
traction across the full diversity of modern web
experiences - establishing multimodal web com-
prehension as a critical area for cross-disciplinary
AI development moving forward. Our future work
includes expanding this analysis to a wide range
of other datasets, including SWDE and the Klarna
Product Pages.
References
Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika
Narayan, Andrew Hojel, Immanuel Trummer, and
Christopher Ré. 2023. Language models enable sim-
ple systems for generating structured views of hetero-
geneous data lakes.
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-
giovanni, Piotr Padlewski, Daniel Salz, Sebas-
tian Goodman, Adam Grycner, Basil Mustafa, Lu-
cas Beyer, Alexander Kolesnikov, Joan Puigcerver,
Nan Ding, Keran Rong, Hassan Akbari, Gaurav
Mishra, Linting Xue, Ashish Thapliyal, James Brad-
bury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao
Jia, Burcu Karagol Ayan, Carlos Riquelme, An-
dreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil
Houlsby, and Radu Soricut. 2023. Pali: A jointly-
scaled multilingual language-image model.
Xiang Deng, Prashant Shiralkar, Colin Lockard, Binx-
uan Huang, and Huan Sun. 2022. Dom-lm: Learning
generalizable representations for html documents.
Pankaj Gulhane, Amit Madaan, Rupesh Mehta,
Jeyashankher Ramamirtham,Rajeev Rastogi,
Sandeep Satpal, Srinivasan H Sengamedu, Ashwin
Tengli, and Charu Tiwari. 2011. Web-scale infor-
mation extraction with vertex. In2011 IEEE 27th
International Conference on Data Engineering,
pages 1209–1220.
Qiang Hao, Rui Cai, Yanwei Pang, and Lei Zhang. 2011.
From one tree to a forest: A unified solution for struc-
tured web data extraction. InProceedings of the 34th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR
’11, page 775–784, York, NY, USA. Association
for Computing Machinery.
Alexandra Hotti, Riccardo Sven Risuleo, Stefan
Magureanu, Aref Moradi, and Jens Lagergren. 2022.
Graph neural networks for nomination and represen-
tation learning of web elements.
SeatGeek Inc. 2014.fuzzywuzzy: Fuzzy String Match-
ing in Python.
Christian Kohlschütter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection shallow features. InProceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining,
WSDM ’10, page 441–450, York, NY, USA.
Association for Computing Machinery.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning.arXiv preprint
arXiv:2304.08485.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach.arXiv preprint arXiv:1907.11692.
Colin Lockard, Xin Luna Dong, Arash Einolghozati,
and Prashant Shiralkar. 2018. Ceres: Distantly su-
pervised relation extraction from the semi-structured
web.
Colin Lockard, Prashant Shiralkar, Xin Luna Dong, and
Hannaneh Hajishirzi. 2020. Zeroshotceres: Zero-
shot relation extraction from semi-structured web-
pages.
OpenAI. 2023. Gpt-4 technical report.
SeleniumHQ. 2023. Selenium.https://selenium.
dev
. Python language bindings for Selenium Web-
Driver.
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards vqa models
that can read.
Maksim Varlamov, Denis Galanin, Pavel Bedrin, Sergey
Duda, Vladimir Lazarev, and Alexander Yatskov.
2022. A dataset for information extraction from
news web pages. In2022 Ivannikov Ispras Open
Conference (ISPRAS), pages 100–106.
Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,
Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei
Wang, . 2023. mplug-2: A modularized multi-
modal foundation model across , image and video.
arXiv preprint arXiv:2302.00402.
Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds,
and Sandeep Tata. 2021. Simplified dom trees for
transferable extraction from the web.
5 Citation:Zia, A.; Aziz, M.; Popa, I.;
Khan, S.A.; Hamedani, A.F.; Asif, A.R.
Artificial Intelligence- Medical
Data Mining.J. Pers. Med.2022,12,
1359. https://doi.org/10.3390/
jpm12091359
Academic Editor: Yu-Feng Hu
Received: 5 July 2022
Accepted: 17 August 2022
Published: 24 August 2022
Publisher’s Note:MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional affil-
iations.
Copyright:© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
Journal of
Personalized Medicine
Review
Artificial Intelligence- Medical Data Mining
Amjad Zia
1
, Muzzamil Aziz
2
, Ioana Popa
1
, Sabih Ahmed Khan
2
, Amirreza Fazely Hamedani
2
and Abdul R. Asif
1,
*
1
Department for Clinical Chemistry/Interdisciplinary UMG Laboratories, University Medical Center,
37075 Göttingen, Germany
2
Future Networks, eScience Group, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH
Göttingen (GWDG), 37077 Göttingen, Germany
*Correspondence: asif@med.uni-goettingen.de
Abstract:Understanding published unstructured textual data traditional mining ap-
proaches and tools is becoming a challenging issue due to the rapid increase in electronic open-source
publications. The application of data mining techniques in the medical sciences is an emerging trend;
however, traditional -mining approaches are insufficient to cope with the current upsurge in
the volume of published data. Therefore, artificial intelligence- mining tools are being
developed and to process large volumes of data and to explore the hidden features and corre-
lations in the data. This review provides a clear-cut and insightful understanding of how artificial
intelligence- data-mining technology is being to analyze medical data. We also describe
a standard process of data mining on CRISP-DM (Cross-Industry Standard Process for Data
Mining) and the most common tools/libraries available for each step of medical data mining.
Keywords: mining; artificial intelligence; machine learning; medical data; healthcare information
1. Introduction
With the rapid growth in online available medical literature, it is almost hard for read-
ers to obtain the desired information without an extensive time investment. For example,
in the ongoing COVID-19 pandemic, the of publications talking about COVID-19
increased very rapidly. In the first 2 years of the pandemic, there were228,640 articlesin
PubMed, 282,883 articles in PMC, and 7551 COVID-19 clinical trials listed in ClinicalTrials.
gov databases (Data accessed on 16 February 2022), and this is increasing at an amazing
speed. Because of the high degree of dimensional heterogeneity, irregularity, and timeliness,
these data are often underutilized. This exponential growth in the scientific literature has
made it difficult for the researchers to (i) obtain relevant information from the literature,
(ii) present information in a concise and structured manner from an unstructured literature
pile, and (iii) fully comprehend the current state and the direction of development in a
research field.
The rapidly increasing literature cannot be managed and/or processed tradi-
tional technologies and methods within an acceptable period. This massive volume of data
makes it rather difficult for researchers to explore, analyze, visualize, and obtain a concise
outcome. The process of extracting hidden, meaningful, and engrossing patterns from
unstructured literature is known as mining [1]. Traditional mining techniques
are not sufficient to cope with the current large volumes of published literature. Therefore,
a rapid increase in the development of data mining techniques on artificial
intelligence can be seen on the horizon for the benefit of patients and physicians. The
inclusion of artificial intelligence (also machine learning (ML), deep learning (DL), and
natural language processing (NLP) as the subsets) empowers the data mining process with
multifold benefits: Gaining insights into the decision-making process, processing
J. Pers. Med.2022,12, 1359. https://doi.org/10.3390/jpm12091359https://www.mdpi.com/journal/jpm J. Pers. Med.2022,12, 13592 of 23
large dataset with increased accuracy and efficiency, and the ability to learn and improve
continuously from the data.
The current review sheds light on the role of different AI- methods, i.e., NLP and
neural network (NN) in medical mining, the current data mining processes, different
database sources, and various AI- tools in the mining process along with
various algorithms. We reviewed the latest mining approaches, highlighted the key
differences between medical and non-medical data mining, and presented a set of tools and
techniques currently being for each step of medical literature mining. Additionally,
we described the role of artificial intelligence and machine learning in medical data mining
and pointed out challenges, difficulties, and opportunities along the road.
1.1. Medical vs. Non-Medical Literature Mining
Human medical data are unique and may be difficult when it comes to mining and
analysis. First, due to the fact that humans are the most advanced and the most observed
(in-depth) species on the globe, their observation is enriched because humans may provide
their sensory input easily compared to the other species on the earth [2]. However, medical
data mining faces numerous key challenges, mainly due to the heterogeneity and verbosity
of data coming from various non-standardized patient records. Similarly, the insufficient
quality of data is also a known issue in medical science that needs to be handled with
care for data mining. Such challenges can be met by standardization of the process of
selection of patients, collection, storage, annotation, and management of data [3]. However,
sometimes this means that existing data and data acquired at multiple centers without
good coordination and standard operating procedures (SOPs) could not be . The major
divergence between medical data and non-medical data mining is expected in ethical and
legal aspects. The of information that can be traced back to individuals involves privacy
risks, which could result in legal issues. More than fifteen Federal US departments with the
US Department of Health and Human Services have issued final revisions to the Federal
Policy for the Protection of Human Subjects “the Common Rule, 45 CFR 46, Subpart A”
(Protection of Human Subjects, 45 CFR 46 (2018). The federal framework for privacy and
security does not apply to the information, which is de-identified or anonymized [4].
The ownership of medical data is another critical issue, as the data are acquired
by different entities where the individuals may have been during their treatment or for
diagnostic purposes. These entities can gather and store the data as per the authorization
of the individual at the time of data acquisition. However, this permission on consent can
be withdrawn by the patient at any time, and/or the consent is only valid for a limited
period and data must be erased after this time [5]. Most of the clinical is produced in a
telegraphic way and the information is highly enriched. Additionally, it is written for the
clinical staff and colleagues, therefore is full of incomplete sentences and abbreviations.
Special tools are required to read, understand, and process this [6]. Electronic patient
records, also known as clinical , have a unique problem in that they are written in a
highly specialized language that can only be processed with a few available tools. Secondly,
patient records are sometimes written in a telegraphic and information-dense style for
clinician-to-clinician communication, and there exists no developed dictionary for such
communications to check grammar and spelling mistakes. In addition, doctors and medical
staff frequently rudimentary sentences and frequently fail to mention the object, such
as the patient, because the patient is implied in the . “Arrived with 38.3 fever and a
pulse of 132”, for example, could be written or simply mentioned.
1.2. of Artificial Intelligence and Machine Learning in Medical Literature Data Mining
The digital era has shown immense trust and growing confidence in machine learning
techniques to increase the quality of life in almost every field of life. This is the case in health
care and precision medicine, where a continuous feed of medical data from heterogeneous
sources becomes a key enabler for AI/ML-assisted treatments and diagnosis. For instance,
AI today can help doctors to bring better patient outcomes with early diagnosis and J. Pers. Med.2022,12, 13593 of 23
treatment plans as well as increased quality of life. Similarly, health organizations and
authorities also aim for the timely execution of AI routines for the prognosis of outbreaks
and pandemics at the national and international levels. Healthcare today is also witnessing
the of AI-aided procedures for operational management in the form of automated
documentation, appointment scheduling, and virtual assistance for patients. In this ,
we will see some -life references of AI\ML tools and technologies currently in
various areas of medical sciences (Table 1).
Table 1.AI\ML products and research prototypes from some leading organizations in healthcare.
Products/Research
Prototypes
Treatment/Field
of Study
Company/InstitutionReference
MergePACS™
Clinical Radiology
Imaging
IBM WatsonMerge PACS—Overview|IBM
BiometryAssist™Diagnostic UltrasoundSamsung Medison
https://www.intel.com/content/www/us/en/developer/
tools/oneapi/application-catalog/full-catalog/diagnostic-
ultrasound.html (accessed on 17 February 2022)
LaborAssist™Diagnostic UltrasoundSamsung Medison
Breast Cancer
Detection Solution
Ultrasound,
mammography, MRI
Huiying’s solution
https://builders.intel.com/ai/solutionscatalog/breast-
cancer-detection-solution-657 (accessed on 17 February 2022)
CT solution
Early detection
of COVID-19
Huiying’s solution
https://builders.intel.com/ai/solutionscatalog/ct-solution-
for-early-detection-of-covid-19-704 (accessed on
17 February 2022)
Dr. Pecker CT Pneumonia
CAD System
Classification and
quantification of
COVID-19
Jianpei Technology
https://www.intel.com/content/www/us/en/developer/
tools/oneapi/application-catalog/full-catalog/dr-pecker-
ct-pneumonia-cad-system.html (accessed on
17 February 2022)
Before going into further detail, it is worth mentioning that data mining and machine
learning concepts go hand in hand and overlap each other to an extent but with a clear
distinction of the overall outcome of both technologies. Data mining is the process of dis-
covering correlations, anomalies, and patterns in a large set of data from an experiment
or event to forecast results [7]. The basis of data mining is statistical modeling techniques
to represent data in some well-defined mathematical model and then this model to
create relationships and patterns among the data variables. Machine learning, on the other
hand, is a one-step-ahead to data mining, where machine learning algorithms
let the computer machine understand the data (with the help of statistical models) and
make predictions of its own. That said, data mining techniques always require human
interaction to find interesting patterns from a given dataset, whereas machine learning is a
relatively modernized technique that enables computer programs to learn from the data
automatically and provide predictions without any human interaction.
Natural Language Processing
Natural Language Processing (NLP) is an artificial intelligence (AI) discipline that
converts human language into machine language. With the increased usage of computer
technology over the last 20 years, this sector has grown significantly [8]. Clinical docu-
mentation, speech recognition, computer-assisted coding, data mining research, automated
registry reporting, clinical decision support, clinical trial matching, prior authorization, AI
chatbots and virtual scribes, risk adjustment models, computational phenotyping, review
management and sentiment analysis, dictation and EMR implementations, and root cause
analysis are some of the most popular applications of NLP in healthcare [9]. In the literature,
a wide range of applications of NLP have been illustrated.
Liu . [10] clinical for entity recognition word embedding (WE)-
skipgram and long short-term memory (LSTM) techniques and achieved an accuracy of
94.37 percent, 92.29 percent, and 85.81 percent for de-identification, event detection, and
concept extraction, respectively, on the micro-average F1-score. Deng . [11] concept embedding (CE)–continuous bag of words (CBOW), skip-gram, and random
projection to generate code and semantic representations from clinical . Afzal . [12] J. Pers. Med.2022,12, 13594 of 23
have developed a pipeline for question generation, evidence quality recognition, ranking,
and summarization of evidence from biomedical literature and presented an accuracy of
90.97 percent. Besides these examples, Pandey . [13] listed 57 papers published between
2017 and 2019 that NLP techniques and various sources, such as clinical ,
EHR inputs, Chinese medical , cancer pathology reports, biomedical , randomized
controlled trial (RCT) articles, clinical notes, and EMR -radiology reports, among others.
2. Standard Process for Data Mining
In response to the demand for a standard data mining method, industry leaders collab-
orated with a diverse group of practitioners (service providers, management consultants,
data mining users, data warehouse vendors) and data mining experts to develop a free,
well-documented, and non-proprietary data mining model [14]. Numerous methods are
available for data mining, such as ASUM (Analytics Solutions Unified Method), CRISP-
DM (Cross-Industry Standard Process for Data Mining), KDD (Knowledge discovery in
databases), SEMMA (Sampling, Exploring, Modifying, Modelling, and Assessing), and
POST-DS (Process Organization and Scheduling electing Tools for Data Science) [15]. In
this study, we employ the CRISP-DM model for data mining because it is a complete and
comprehensive data mining . In 1997, the CRISP-DM consortium developed a
generic process model for data mining to establish guidelines for data mining beginners, the
community, and experts, which can be modified for any particular need [14]. For example,
to deal with the problem of multidimensional time-series data in a neonatal intensive care
unit (NICU), the CRISP-DM model was modified to support and accommodate temporal
data mining (TDM), which is named CRISP-TDM [16]. In the lifecycle of a data mining
process, the CRISP-DM reference model has six phases ( 1): Business understanding,
data understanding, data preparation, modeling, evaluation, and deployment. The details
of the available tools and technologies for each phase are described in the rest of this article.
2.1. Business Understanding
The first and most critical part of data mining is business understanding, which
includes setting project objectives, and targets, assessing the situation, execution plans, and
risk assessments [14]. Setting project objectives requires a complete grasp of the project’s
genuine goal to define the associated variables. The steps in the data understanding phase
according to CRISP-DM are to (1) determine the business objectives (to fully comprehend
the project’s goal, identify the key players, and establish business success criteria), (2) assess
the situation (to identify resource availability (especially data), identify project risks and
potential solutions to those risks, and calculate the cost–benefit ratio), (3) clarify the data
mining goals (to establish project goals and success criteria), (4) produce a project plan (to
develop detailed plans for each project segment, including a timeline and technology and
tool selection).
Martins . [18] a data mining to predict cardiovascular diseases
(while RapidMiner and Weka software). The main question addressed by the project
is how to detect cardiovascular disease at an early stage in a person who is at a high risk
of developing the disease and thus avoid premature death. As a result, the primary set of
goals is to create a solution for predicting cardiovascular diseases in patients patient
data, to shorten the time required for disease diagnosis, and to provide the patients with
immediate and adequate treatment. J. Pers. Med.2022,12, 13595 of 23
J. Pers. Med. 2022, 12, x FOR PEER REVIEW 5 of 24 1. Cross-Industry Standard Process for Data Mining (CRISP-DM)—adapted from the webpage of the Data Science Process Alliance [17] (www.datascience-pm.com/crisp-dm-2/, accessed on 16 April 2022). The circular nature of the data mining process is symbolized by the outer circle, while the arrows that connect the phases show the most essential and common dependencies. 2.1. Business Understanding The first and most critical part of data mining is business understanding, which includes setting project objectives, and targets, assessing the situation, execution plans, and risk assessments [14]. Setting project objectives requires a complete grasp of the project’s genuine goal to define the associated variables. The steps in the data understanding phase according to CRISP-DM are to (1) determine the business objectives (to fully comprehend the project’s goal, identify the key players, and establish business success criteria), (2) assess the situation (to identify resource availability (especially data), identify project risks and potential solutions to those risks, and calculate the cost–benefit ratio), (3) clarify the data mining goals (to establish project goals and success criteria), (4) produce a project plan (to develop detailed plans for each project segment, including a timeline and technology and tool selection). 1.Cross-Industry Standard Process for Data Mining (CRISP-DM)—adapted from the webpage
of the Data Science Process Alliance [17] (www.datascience-pm.com/crisp-dm-2/, accessed on
16 April 2022). The circular nature of the data mining process is symbolized by the outer circle, while
the arrows that connect the phases show the most essential and common dependencies.
2.2. Data Understanding
The emphasis in this phase (second phase), according to CRISP-DM, is on data source
identification, data acquisition, initial data collection, familiarization with the data, and
identifying problems in the acquired data. The steps in the data understanding phase are
(1) acquire the initial data (to gather the data from various sources, insert it into the analysis
program, and integrate it), (2) explain the data (to study and report on the acquired data’s
surface properties such as field identities, data format, data quantity, and the of
records, etc.), (3) explore the data (to delve deeper into the data by querying, visualizing,
and identifying relationships between data points, as well as to generate an exploration
report), and (4) verify data quality (to inspect and document the data quality and any
quality-related issues) [14]. In this phase, one focuses on identifying data sources for
various types of data, the process of acquisition of the data, and handling restrictions
in data acquisition. A tremendous amount of data is generated by the health care industry J. Pers. Med.2022,12, 13596 of 23
and medical institutions every day from medical imaging, patient monitoring, and medical
records [7]. Some of the most common types of medical data are experimental data, medical
literature, clinical textual data, medical records, images/videos (e.g., MRI), and omics
data (e.g., genomics, proteomics). For example, Martins . [18] a data mining to predict cardiovascular diseases. For data understanding, the dataset for
cardiovascular disease prediction came from the Kaggle data repository and focused on
detecting cases of cardiovascular disease. The dataset included 70,000 registered patients
with 12 disease-related attributes collected during the patients’ medical examinations.
2.2.1. Literature Extraction/Data Gathering
The first task in the data understanding phase is to identify data sources, acquire data
from these sources, identify problems during data acquisition, such as data restrictions and
data privacy policies, and document the solutions [14]. /data mining frequently uses
public Internet- sources such as the World Wide Web. The retrieval of content from
public sources is referred to as “web scraping” or “web crawling”. Web scraping can be
performed manually, but it can also be performed automatically with the help of a web
crawler. Manual scraping a large database such as PubMed, which contains millions of peer-
reviewed publications, requires a lot of time and effort. Only automated processing can
provide the necessary quality, response time, and homogeneity for their analysis with such
a large database. As a result, there is always a high demand for web scraping techniques
and tools tailored to customer requirements. PubMed, for example, is a massive database
of biomedical literature that contains 34 million citations (as of11 May 2022) collected
from online books, life science journals, and MEDLINE, and a massive of publications are added every year [19]. Web crawlers are to search for and harvest the
necessary data from it. Guo . [20], for example, collected COVID-19 data published by
local health authorities a web crawler (developed the Python language and
connected with a MySQL database).
Although web scraping and web crawling may seem to be identical, they have several
distinctions ( 2). While the terms “web scraping” and “web crawling” are sometimes
interchanged, they refer to two distinct processes [21,22]. Web crawling is a broad term that
refers to the process of downloading information from a website, extracting the hyperlinks
included within, and following them ( 2). Typically, downloaded information is
saved in a database or indexed to enable searching. Essentially, search engines are crawlers.
All that is required is to see a page in its entirety and indexing it. When a bot crawls a
website, it scans each page and link, all the way to the website’s last line, looking for any
information. Web crawlers are primarily by major search engines such as Google,
Bing, and Yahoo, as well as statistics organizations and online aggregators. Typically, a
web crawler collects general information, while scrapers collect particular datasets [23,24].
On the other hand, web scraping is the process of obtaining data from a web page and
extracting specific information that can be saved almost anywhere (database, file, etc.) as
shown in 2. An online scraper, also known as a web data extractor, is similar to a
web crawler in that it detects and locates website content. In contrast to a web crawler,
which uses pseudo-random IDs, web scraping uses specific identifiers, such as the HTML
structure of the web pages from which data must be collected. Web scraping refers to
the of robots to extract specific datasets from the internet. The obtained data can be
compared, checked, and analyzed in accordance with the demands and objectives of a
specific organization [25].
Several mining tools are now available. Kaur and Chopra [26] compared
55 popular mining tools and their features and discovered three categories: (1) Pro-
prietary (company-owned—39 tools); (2) open source (free—13 tools); and (3) online mining tools (run directly from a website—3 tools). Four tools that were not examined
in the prior review but are now on the list of well-liked mining tools are contrasted
in Table 2. All of these Python- tools serve the same purpose, but with different
goals and objectives. ‘’Requests” has an advantage over other tools in that it is easy to , J. Pers. Med.2022,12, 13597 of 23
making it an excellent choice for any simple web scraping task. Scrapy is best suited for
large-scale web scraping projects, as opposed to the other three tools (requests, beautiful
soup, and selenium), which are best suited for small-scale scraping tasks. The “Beautiful
Soup” tool has advantages such as being simple to understand, learn, and , and it can
extract information from a disorganized website. Selenium has a significant advantage over
the other scraping tools described because it can scrape websites with heavy JavaScript.
Table 2 provides descriptions of more hierarchical comparisons.
J. Pers. Med. 2022, 12, x FOR PEER REVIEW 7 of 24 hyperlinks included within, and following them ( 2). Typically, downloaded infor-
mation is saved in a database or indexed to enable searching. Essentially, search engines are crawlers. All that is required is to see a page in its entirety and indexing it. When a bot crawls a website, it scans each page and link, all the way to the website’s last line, looking for any information. Web crawlers are primarily by major search engines such as Google, Bing, and Yahoo, as well as statistics organizations and online aggregators. Typ-
ically, a web crawler collects general information, while scrapers collect particular da-
tasets [23,24]. On the other hand, web scraping is the process of obtaining data from a web page and extracting specific information that can be saved almost anywhere (database, file, etc.) as shown in 2. An online scraper, also known as a web data extractor, is similar to a web crawler in that it detects and locates website content. In contrast to a web crawler, which uses pseudo-random IDs, web scraping uses specific identifiers, such as the HTML structure of the web pages from which data must be collected. Web scraping refers to the of robots to extract specific datasets from the internet. The obtained data can be compared, checked, and analyzed in accordance with the demands and objectives of a specific organization [25]. 2. Comparison between web crawling and web scraping. Several mining tools are now available. Kaur and Chopra [26] compared 55 pop-
ular mining tools and their features and discovered three categories: (1) Proprietary (company-owned—39 tools); (2) open source (free—13 tools); and (3) online mining tools (run directly from a website—3 tools). Four tools that were not examined in the prior review but are now on the list of well-liked mining tools are contrasted in Table 2. All of these Python- tools serve the same purpose, but with different goals and objec-
tives. ‘’Requests’’ has an advantage over other tools in that it is easy to , making it an excellent choice for any simple web scraping task. Scrapy is best suited for large-scale web scraping projects, as opposed to the other three tools (requests, beautiful soup, and sele-
nium), which are best suited for small-scale scraping tasks. The “Beautiful Soup” tool has advantages such as being simple to understand, learn, and , and it can extract infor-
mation from a disorganized website. Selenium has a significant advantage over the other scraping tools described because it can scrape websites with heavy JavaScript. Table 2 provides descriptions of more hierarchical comparisons. Table 2. Comparison between four mining tools. 2.Comparison between web crawling and web scraping.
Table 2.Comparison between four mining tools.
RequestsScrapyBeautiful SoupSelenium
What is it?HTTP library for Python
Open-source web
framework written
in Python
python library
Open-source application
framework tool and
python library
Goal
Sending HTTP/1.1
requests Python
•Can crawl or scrape
websites and extract
the structured data
and saves it
•
Can also be for
a wide range of
tasks, monitoring,
and
automated testing
•Can parse the data
and scrape the
web pages
•Extract information
from XML and
HTML documents
•Useful for web
scraping websites
that are JavaScript
heavy
Ideal usage for simple and
low-level complex web
scraping tasks
•
Framework for
complex web
scraping or web
crawling tasks.
• for large-scale
projects
• for smaller
web scraping tasks
•Toolkit for
searching through a
document (XML or
HTML) and
extracting
important
information
•Developed for web
testing
• for test
automation of web
applications
•Scraping
JavaScript-heavy
websites
• for small-scale
and low-level
complex projects J. Pers. Med.2022,12, 13598 of 23
Table 2.Cont.
RequestsScrapyBeautiful SoupSelenium
Advantage
•A simple way to
retrieve data
from URL
•Scraping data
from web
•Allows to read,
write, post, delete,
and update the data
for the given URL
•Extremely easy to
deal with cookies
and sessions
•Portable library
•Runs on Linux,
Windows, and Mac
•One of the faster
scraping libraries
•Can extract
websites much
faster than
other tools
•Consumes less
memory and
CPU usage
•Building a robust,
and flexible
application with
different functions
•Learning and
mastering it is easy
•Community
support is readily
available to
resolve issues.
•Deals with the Core
JavaScript-heavy
website
•Can handle AJAX
and PJAX requests
SelectorsNoneJCSS and XPathCSSCSS and Xpath
Documentation
Detailed and simple to
understand
Detailed and simple to
understand
Detailed and simple to
understand
Detailed and very
complex
GitHub stars46.8 k42.7 k-22.7 k
Reference
Chandra and
Varanasi [27]
Kouzis-Loukas [28]Richardson [29]Sharma [30] Restriction
When a web crawler visits a website, some pages or the entire website possess restrictions. These restrictions are implemented mainly by the site owners due to data
confidentiality, data integrity, and data quality, as well as legal concerns. A crawler usually
performs multiple requests per second and downloads large files to obtain the data in a
short time, which can cause a website server to crash. To tackle this problem, numerous
methods are available. Canonical tag, robots.txt, x-robots-tag, the metarobots tag, and
others are files provided by the website owners to follow the instructions for scraping the
website without creating any problem. For example, “robots.txt” files are frequently by websites to convey their scraping and crawling intents. Robots.txt files enable scraping
bots to crawl specific sites, while malevolent bots, on the other hand, are uninterested in
robots.txt files (which act as a “do not enter” sign) as explained below in 3.
Data Collection from Different Sources
The pace at which medical data are being generated is increasing day by day during the
massive information explosion year, and global information is being produced in massive
quantities in every field, including healthcare [31,32]. Administrative records, biometric
data, clinical registration, diagnostics, X-rays, electronic health records, patient report data,
treatments, results, and other types of medical data are all included in medical data. These
massive and complex characteristics make data difficult to deal with for a meaningful and
unknown outcome. Healthcare centers and medical institutions around the world have
proposed a variety of medical information systems to deal with rapidly growing data and
provide the best possible services and care to patients [32]. The most common way to
collect and store the data is by management software, which can store all electronic and
non-electronic records. Several software products are available, e.g., eHospital Systems
(adroitinfosystems.com/products/ehospital-systems, accessed on 11 April 2022) and the
DocPulse Clinic/Hospital Information Management System (docpulse.com, accessed on
11 April 2022). J. Pers. Med.2022,12, 13599 of 23
J. Pers. Med. 2022, 12, x FOR PEER REVIEW 9 of 24 bots to crawl specific sites, while malevolent bots, on the other hand, are uninterested in robots.txt files (which act as a “do not enter” sign) as explained below in 3. 3. Layout for restrictions. Data Collection from Different Sources The pace at which medical data are being generated is increasing day by day during the massive information explosion year, and global information is being produced in mas-
sive quantities in every field, including healthcare [31,32]. Administrative records, bio-
metric data, clinical registration, diagnostics, X-rays, electronic health records, patient re-
port data, treatments, results, and other types of medical data are all included in medical data. These massive and complex characteristics make data difficult to deal with for a meaningful and unknown outcome. Healthcare centers and medical institutions around the world have proposed a variety of medical information systems to deal with rapidly growing data and provide the best possible services and care to patients [32]. The most common way to collect and store the data is by management software, which can store all electronic and non-electronic records. Several software products are available, e.g., eHospital Systems (adroitinfosystems.com/products/ehospital-systems, accessed on 11 April 2022) and the DocPulse Clinic/Hospital Information Management System (docpulse.com, accessed on 11 April 2022). For mining, data collection from data sources is the key step. In medical science, various types of medical data, as well as trends, are generated at a rapid pace, which can be differentiated into five categories, as follows: 1. Hospital management software (Patient data/Clinical narratives). 2. Clinical trials. 3. Research data in Medicine. 4. Publication platforms for Medicine (PubMed, for instance). 5. Pharmaceuticals and regulatory data. Tables 3–5 provide further details about the different types of data sources. Patient data generated by clinical trials is available from various sources, as shown in Table 3. Medical researchers benefit from open- databases because they have enormous vol-
umes of data, rich data content, broad data coverage, and a cost-effective study strategy. There exist several datasets and databases publicly available related to various medical fields that contain many medical record variables (Table 4). Textual information is grow-
ing rapidly, and it is difficult to grab concise information fast and structured manner. The 3.Layout for restrictions.
For mining, data collection from data sources is the key step. In medical science,
various types of medical data, as well as trends, are generated at a rapid pace, which can
be differentiated into five categories, as follows:
1.Hospital management software (Patient data/Clinical narratives).
2.Clinical trials.
3.Research data in Medicine.
4.Publication platforms for Medicine (PubMed, for instance).
5.Pharmaceuticals and regulatory data.
Tables 3–5 provide further details about the different types of data sources. Patient data
generated by clinical trials is available from various sources, as shown in Table 3. Medical
researchers benefit from open- databases because they have enormous volumes of
data, rich data content, broad data coverage, and a cost-effective study strategy. There
exist several datasets and databases publicly available related to various medical fields that
contain many medical record variables (Table 4). Textual information is growing rapidly,
and it is difficult to grab concise information fast and structured manner. The published
literature is the most abundant and primary source of textual information in the health care
field (Table 5).
Table 3.Databases and registries for clinical trials.
Databases/RegistriesTrial NumbersProvided byLocationFounded YearURL
ClinicalTrials.gov405,612
U.S. National Library
of Medicine
Bethesda, MD, USA1997
https://clinicaltrials.gov/
(accessed on 11 April 2022)
Cochrane Central
Register of Controlled
Trials (CENTRAL)
1,854,672
a component of
Cochrane Library
London, UK1996
https:
//www.cochranelibrary.
com/central (accessed on
11 April 2022)
WHO International
Clinical Trials Registry
Platform (ICTRP)
353,502
World Health
Organization
Geneva, Switzerland-
https:
//trialsearch.who.int/
(accessed on 11 April 2022)
The European Union
Clinical Trials Database
60,321
European Medicines
Agency
Amsterdam, The
Netherlands
2004
https://www.
clinicaltrialsregister.eu/ctr-
search/search (accessed on
11 April 2022) J. Pers. Med.2022,12, 135910 of 23
Table 3.Cont.
Databases/RegistriesTrial NumbersProvided byLocationFounded YearURL
CenterWatch50,112-Boston, MA, USA1994
http:
//www.centerwatch.com/
clinical-trials/listings/
(accessed on 11 April 2022)
German Clinical Trials
Register (Deutsches
Register Klinischer
Studien—DRKS)
>13,000
Federal Institute for
Drugs and Medical
Devices
Cologne, Germany
https://www.bfarm.de/
EN/BfArM/Tasks/
German-Clinical-Trials-
Register/_node.html
(accessed on 11 April 2022)
Table 4.Research data in Medicine.
Databases
No. of
Datasets
Owned byDomainsAvailable ResourcesURLRef
Biologic Specimen
and Data
Repository
Information
Coordinating
Center (BioLINCC)
262
National Institute
of Health,
Calverton, MD,
USA
Cardiovascular,
pulmonary, and
hematological
Specimens and Study Datasets
https://biolincc.nhlbi.
nih.gov/studies/
(accessed on
4 April 2022)
[33]
Biomedical
Translational
Research
Information System
(BTRIS)
Five billion
rows of data
Bethesda, MD,
USA
Multiple subjectsStudy Datasets
https://btris.nih.gov/
(accessed on
4 April 2022)
[34]
Clinical Data Study
Request
3135
The consortium
of clinical study
Sponsors
Multiple subjectsStudy Datasets
https://www.
clinicalstudydatarequest.
com/ (accessed on
4 April 2022)
[35]
Surveillance,
Epidemiology, and
End Results (SEER)
-
National Cancer
Institute,
Bethesda, MD,
USA
Cancer (All
types)—Stage
and histological
details
Study Datasets
https://seer.cancer.gov/
(accessed on
4 April 2022)
[36]
Medical
Information Mart
for Intensive Care
(MIMIC)
MIMIC-III
53,423 patients
MIT Laboratory
for
Computational
Physiology,
Cambridge, MA,
USA
Intensive Care
Patient data (vital signs, medications,
laboratory measurements,
observations and notes charted by care
providers, survival data, hospital
length of stay, imaging reports,
diagnostic codes, procedure codes, and
fluid balance)
https://mimic.mit.edu/
(accessed on
4 April 2022)
[37,38]
MIMIC-CXR
65,379 patients
(377,110 images
of chest
radiographs)
[39]
National Health
and Nutrition
Examination Survey
(NHANES)
-
Centers for
disease control
and prevention,
Hyattsville, MD,
USA
Dietary
assessment and
other nutrition
surveillance
data nutritional status, dietary intake,
anthropometric measurements,
laboratory tests, biospecimens, and
clinical findings.
https://www.cdc.gov/
nchs/nhanes/index.htm
(accessed on
4 April 2022)
[40]
Global Burden of
Disease (GBDx)
-
Institute for
Health Metrics
and Evaluation,
Seattle, WA, USA
Epidemic
patterns and
disease burden
Surveys, censuses, vital statistics, and
other health-related data
https:
//ghdx.healthdata.org/
(accessed on
4 April 2022)
[41]
UK Biobank (UKB)0.5 millionStockport, UK
In-depth genetic
and health
information
Genetic, biospecimens, and health data
https://www.
ukbiobank.ac.uk/
(accessed on
4 April 2022)
[42]
The Cancer Genome
Atlas (TCGA)
molecularly
characterized
over
20,000 cancer
samples
spanning
33 cancer types
National Cancer
Institute, NIH,
Bethesda, MD,
USA
Cancer genomics
over 2.5 petabytes of epigenomic,
proteomic, transcriptomic, and
genomic data
https:
//www.cancer.gov/
about-nci/organization/
ccg/research/structural-
genomics/tcga (accessed
on 4 April 2022)
[43]
Gene Expression
Omnibus (GEO)
4,981,280
samples
National Center
for Bioinformatics
(NCBI), NIH,
Bethesda, MD,
USA
Sequencing and
gene expression
4348 datasets available
https://www.ncbi.nlm.
nih.gov/geo/ (accessed
on 4 April 2022)
[44] J. Pers. Med.2022,12, 135911 of 23
Table 5.Biomedical literature sources.
Source
Articles
(Million)
Launched byPublication TypeTopicOnlineLink
PubMed33
National Center
for Biotechnology
Information
(NCBI)
Abstracts
Biomedical and
life sciences
1996
https://www.ncbi.nlm.
nih.gov/pubmed/
(accessed on
4 April 2022)
PubMed Central
(PMC)
7.6
National Center
for Biotechnology
Information
(NCBI)
Full Biomedical and
life sciences
2000
https://www.ncbi.nlm.
nih.gov/pmc/
(accessed on
4 April 2022)
Cochrane Library-Cochrane
Abstracts and
full Healthcare-
https://www.
cochranelibrary.com/
search (accessed on
4 April 2022)
bioRxiv-
Cold Spring
Harbor
Laboratory
(CSHL)
Unpublished
preprints
Biological
sciences
2013
https:
//www.biorxiv.org/
(accessed on
4 April 2022)
medRxiv-
Cold Spring
Harbor
Laboratory
(CSHL)
Unpublished
manuscripts
Health sciences2019
https:
//www.medrxiv.org/
(accessed on
4 April 2022)
arXiv2.05Cornell Tech
Non-peer-
reviewed
Multidisciplinary1991
https://arxiv.org/
(accessed on
4 April 2022)
Google Scholar100 (in 2014)Google
full or
metadata
Multidisciplinary2004
https:
//scholar.google.com/
(accessed on
4 April 2022)
Semantic Scholar205.25
Allen Institute for
Artificial
Intelligence
Abstracts and
full Multidisciplinary2015
https://www.
semanticscholar.org/
(accessed on
4 April 2022)
Elsevier17 (as of 2018)Elsevier
Abstracts and
full Multidisciplinary1880
https:
//www.elsevier.com/
(accessed on
4 April 2022)
Springer Nature-
Springer Nature
Group
Abstracts and
full Multidisciplinary2015
https://www.
springernature.com/
(accessed on
4 April 2022)
Springer-Springer Nature
Abstracts and
full Multidisciplinary1842
https:
//link.springer.com/
(accessed on
4 April 2022)
2.3. Data Preparation
In the third phase (data preparation) of CRISP-DM, a final dataset is created from the
raw data, which will be in the modeling tool. This phase is the major part (ca. 80%) of
a /data mining project. The steps in the data preparation phase are (1) data selection (to
choose the dataset along with its attributes that will be for the analysis on the
project goals, quality, data type, and volume.), (2) data cleaning (to estimate missing data
and improve the dataset by correcting, imputing, or removing incorrect values), (3) data
construction (to create derived attributes or entirely records, as well as to transform
data as needed), (4) data integration (to create datasets and aggregate values
by combining data from multiple sources), (5) data formation (to remove inappropriate
characters from the data and change the data’s format or design so that it fits into the
model) [14].
2.3.1. Data Cleaning/Data Transformation
The primary goal of data cleaning is to detect and remove duplicate data and errors
from a dataset to create a reliable dataset. Cleaning data entails identifying and removing
entries from a dataset that are corrupt, incorrect, duplicated, incomplete, or improperly J. Pers. Med.2022,12, 135912 of 23
formatted (see 4). Data cleaning is required to analyze information from multiple
sources [45–47].
J. Pers. Med. 2022, 12, x FOR PEER REVIEW 13 of 24 4. Steps for data cleaning. Various related tools and python libraries are discussed in the following sections. Python Libraries for Data Cleaning include the following: 1. NumPy is a quick and easy-to- open-source Python library for data processing. Because many of the most well-known Python libraries, including Pandas and Mat-
plotlib, are on NumPy, it is a fundamentally crucial library for the data science environment. The primary purpose of the NumPy library is the straightforward ma-
nipulation of large multidimensional arrays, vectors, and matrices. For numerical cal-
culations, NumPy also offers effectively implemented functions [48]. 2. Data processing tasks such as data cleaning, data manipulation, and data analysis are performed the well-known Python library Pandas. The Python Data Analysis Library is referred to as “Pandas.” Multiple modules for reading, processing, and writing CSV, JSON, and Excel files are available in the library. Although there are many data cleaning tools available, managing and exploring data with the Pandas library is incredibly quick and effective [49]. 3. An open-source Python library for automating data cleaning procedures is called DataCleaner. Pandas Dataframe and scikit-learn data preprocessing features com-
prise its two separate modules [50]. The data are then transformed into the proper format after being cleaned (Excel, JSON, or XML). Data transformation makes it simpler to preprocess data and/or . De-
pending on the modifications that must be made, the data transformation may be straight-
forward or complicated. The data are easier to for both humans and computers after transformation because it is more structured and organized. Additionally, it becomes sim-
pler to integrate into various programs and systems [46]. Various related tools are discussed in the following sections. 1. Generation of Bibliographic Data is known as GROBID. It is a machine-learning li-
brary that has developed into a state-of-the-art open-source library for removing metadata from PDF-formatted technical and scientific documents. The library plans to reconstruct the logical structure of its original document in addition to simple bib-
liographic extraction in order to support large-scale advanced digital library pro-
cesses and analysis. GROBID develops fully automated solutions on machine learning models for that reason. ResearchGate, Mendeley, CERN Inspire, and HAL, France’s national publi-
cation repository, are just a few of the commercial and open- scientific services that the library is connected to. The result is to extract and transform PDF documents into XML TEI format, supple-
ment the extracted information with other online services, and illustrate the findings gath-
ered in PDF documents of scientific papers [51,52]. 4.Steps for data cleaning.
Various related tools and python libraries are discussed in the following sections.
Python Libraries for Data Cleaning include the following:
1.NumPy is a quick and easy-to- open-source Python library for data processing.
Because many of the most well-known Python libraries, including Pandas and Mat-
plotlib, are on NumPy, it is a fundamentally crucial library for the data science
environment. The primary purpose of the NumPy library is the straightforward
manipulation of large multidimensional arrays, vectors, and matrices. For numerical
calculations, NumPy also offers effectively implemented functions [48].
2.Data processing tasks such as data cleaning, data manipulation, and data analysis are
performed the well-known Python library Pandas. The Python Data Analysis
Library is referred to as “Pandas”. Multiple modules for reading, processing, and
writing CSV, JSON, and Excel files are available in the library. Although there are
many data cleaning tools available, managing and exploring data with the Pandas
library is incredibly quick and effective [49].
3.
An open-source Python library for automating data cleaning procedures is called
DataCleaner. Pandas Dataframe and scikit-learn data preprocessing features comprise
its two separate modules [50].
The data are then transformed into the proper format after being cleaned (Excel, JSON,
or XML). Data transformation makes it simpler to preprocess data and/or . Depending
on the modifications that must be made, the data transformation may be straightforward or
complicated. The data are easier to for both humans and computers after transformation
because it is more structured and organized. Additionally, it becomes simpler to integrate
into various programs and systems [46].
Various related tools are discussed in the following sections.
1.
Generation of Bibliographic Data is known as GROBID. It is a machine-learning
library that has developed into a state-of-the-art open-source library for removing
metadata from PDF-formatted technical and scientific documents. The library plans
to reconstruct the logical structure of its original document in addition to simple bibli-
ographic extraction in order to support large-scale advanced digital library processes
and analysis.
GROBID develops fully automated solutions on machine learning models
for that reason. ResearchGate, Mendeley, CERN Inspire, and HAL, France’s national
publication repository, are just a few of the commercial and open- scientific services
that the library is connected to. J. Pers. Med.2022,12, 135913 of 23
The result is to extract and transform PDF documents into XML TEI format, sup-
plement the extracted information with other online services, and illustrate the findings
gathered in PDF documents of scientific papers [51,52].
2.BioC is a straightforward and straightforward format for exchanging data and
annotations, as well as for simple processing. Its primary goal is to provide an
abundance of research data and articles for mining and information retrieval.
They are available in a variety of file formats, including BioC XML, BioC JSON,
Unicode, and ASCII. These formats are available through a Web API or FTP [53].
To summarize, data cleansing improves a dataset’s consistency, while transforma-
tion simplifies data processing. Both processes improve the training dataset’s quality for
model construction.
2.3.2. Feature Engineering
Choosing, modifying, and converting raw data into features that may be utilized
in supervised learning is a process of feature engineering, often referred to as feature
extraction. This machine learning technique, feature engineering, uses data to generate variables that are not present in the training set. To streamline and accelerate data
transformations while also improving model accuracy, it can generate features for both
supervised and unsupervised learning. With machine learning models, feature engineering
is necessary. Regardless of the architecture or the data, a bad feature will directly affect
your model. Numerous tools are available to automate the entire feature engineering
process and to generate a large pool of features in a short period for both classification and
regression tasks. Some feature engineering tools are FeatureTools, AutoFeat, TsFresh, Turi,
Azure Machine Learning Studio, ZOMBIE, FeatureFu, and OneBM [54,55].
Vijithananda . [56] extracted features from MRI ADC images of a brain tumor.
The following features were extracted from labeled MRI brain ADC image slices from
195 patients: Skewness, cluster shade, pixel values (he demographics), prominence, Grey
Level Co-occurrence Matrix (GLCM) features, energy, contrast, entropy, variance, mean,
correlation, homogeneity, and kurtosis. Both GLCM homogeneity and skewness were
excluded because they scored the lowest in the ANOVA f-test feature selection process. The
Random Forest classifier outperformed Decision Trees, Nave Bayes, Linear Discriminant
Analysis, K-Nearest Neighbors (KNN), and Logistic Regression and was chosen for further
model development. The final model had an accuracy of 90.41 percent in predicting
malignant and benign neoplasms.
2.3.3. Searching for Keywords
The extraction of keywords or key phrases from documents is known as keyword
extraction. They are chosen from among the phrases in the document and describe
the topic of the document. Several popular methods are available for automatically ex-
tracting keywords. Those are in processes that automatically extract keywords from
documents to select the most frequently and significant words or phrases from the document. This classifies keyword extraction methods as part of the natural language
processing field, which is important in machine learning and artificial intelligence. [57].
Keyword extractors are to extract words (keywords) or groups of two or more words
that form a phrase (key phrases).
FlashText, for example, is a free and open-source Python package that enables keyword
search and replacement and is one of the recently described keyword extraction tools [58].
It performs a full analysis an Aho-Corasick algorithm and a Trie Dictionary. As a
general rule, keyword matching entails scanning the corpus (human-created documents
comprise a large, structured set of texts) for each term. Consider the following scenario:
Someone has 100 keywords and needs to search through 2000 papers; a single term is
selected at a time and a search of the 2k corpus is performed; the search is continued for
100×2000 is 200,000 iterations. In addition to this keyword search tool, four Python- J. Pers. Med.2022,12, 135914 of 23
tools are selected from the various keyword and phrase extraction tools that are available,
and their features, benefits, and NLP tasks are contrasted in Table 6.
Table 6.Searching for relevant content.
Natural Language ToolkitSpaCyScikit-Learn NLP ToolkitGensim
What is it?
open-source python
platform for handling
human language data
open-source python library
for advanced natural
language processing
machine learning software
library for the Python
programming language
fastest python library for the
training of vector embedding
Features
• on NumPy,
SciPy, and Matplotlib
•An easy and efficient
way to analyze
predictive data
•Easily accessible and
reusable in different
contexts
Advantage
•Most well-known and
comprehensive NLP
libraries with many
extensions
•offers support in the
largest of
languages
•easy to •fully integrated with
Python
•compatible with other
deep learning
frameworks
•many already trained
statistical models
available
•applicable to many
different languages
•high speed and
performance
•freely available
•able to process long
texts
•platform-independent
usable
•simple and efficient
tools for machine
learning, data mining,
and data analysis
•freely available for
everyone
•applicable to different
application areas, natural language
processing
•Provides ready-to-
models and corpora
•Models pre-trained for
specific areas such as
health care
•Processes large amounts
of data streaming data
NLP Tasks
•Classification
•Tokenization
•Stemming
•Tagging
•Parsing
•Classification
•Tokenization
•Stemming
•Tagging
•Parsing
•Named Entity
recognition
•Sentiment Analysis
•Classification
•Topic Modeling
•Sentiment Analysis
• similarity
• summarization
•Topic Modeling
GitHub stars10.4 k22.4 k49 k12.9 k
Website
nltk.org (accessed on
16 March 2022)
spacy.io (accessed on
16 March 2022)
scikit-learn.org (accessed on
16 March 2022)
radimrehurek.com/gensim/
(accessed on 16 March 2022)
ReferenceBird . [59]Honnibal [60]
Pedregosa . [61],
Pinto . [62]
Rehurek and Sojka [63]
2.4. Modeling
In the fourth phase (known as modeling) of CRISP-DM, various modeling techniques
are tested and calibrated by adjusting the model parameters to achieve the best results [14].
The steps in the modeling process are (1) choosing a modeling technique to select one
or more task-specific models/algorithms/assumptions, (2) the creation of test designs to
determine the model’s strength by evaluating the model’s quality and validity, (3) the
building of models (to the modeling tool for building models from the prepared dataset,
adjust the model parameter, and describe the model), and (4) the evaluation of models
to explain the model outcome on subject knowledge, the predetermined success
norms, and the test design, rank the multiple generated models, and readjust the parameter
settings—if required.
From several available models for organizing and analyzing the data, the selection of
a model depends on the purpose (e.g., forecast) and the type of data (unstructured or
structured). A model is a set of data, patterns, and statistics. The available data-mining
models are divided into two categories: Predictive and descriptive. Descriptive models are
frequently to determine patterns in data that can be explained by humans. Predictive J. Pers. Med.2022,12, 135915 of 23
models known results from various datasets to forecast unidentified or future values
of other variables of interest. Predictive models are usually on the previously
provided data and their results. Classification, prediction, regression, and time series
analysis are tasks in the predictive models. Descriptive model data mining tasks comprise
clustering, associating rules, sequence discovery, and summarization ( 5). A of algorithms/methods are available for the prediction and analysis of patterns in the data.
However, the selection of the algorithm is mainly depending on the dependent variables
whether labeled or unlabeled. If the dependent variable/s in the dataset are labeled, a
supervised learning algorithm is . Decision trees, the random forest (RF), support
vector machines (SVMs), and competitive risk model are commonly algorithms. In
contrast, if the dependent variables in the data are not labeled, an unsupervised learning
method is . Clustering analysis, partition clustering, hierarchical clustering, principal
component analysis (PCA), and association analysis are some of the unsupervised learning
algorithms [64,65].
J. Pers. Med. 2022, 12, x FOR PEER REVIEW 16 of 24 building of models (to the modeling tool for building models from the prepared da-
taset, adjust the model parameter, and describe the model), and (4) the evaluation of mod-
els to explain the model outcome on subject knowledge, the predetermined success norms, and the test design, rank the multiple generated models, and readjust the param-
eter settings—if required. From several available models for organizing and analyzing the data, the selection of a model depends on the purpose (e.g., forecast) and the type of data (unstructured or structured). A model is a set of data, patterns, and statistics. The available data-mining models are divided into two categories: Predictive and descriptive. Descriptive models are frequently to determine patterns in data that can be explained by humans. Pre-
dictive models known results from various datasets to forecast unidentified or future values of other variables of interest. Predictive models are usually on the previously provided data and their results. Classification, prediction, regression, and time series anal-
ysis are tasks in the predictive models. Descriptive model data mining tasks comprise clustering, associating rules, sequence discovery, and summarization ( 5). A num-
ber of algorithms/methods are available for the prediction and analysis of patterns in the data. However, the selection of the algorithm is mainly depending on the dependent var-
iables whether labeled or unlabeled. If the dependent variable/s in the dataset are labeled, a supervised learning algorithm is . Decision trees, the random forest (RF), support vector machines (SVMs), and competitive risk model are commonly algorithms. In contrast, if the dependent variables in the data are not labeled, an unsupervised learning method is . Clustering analysis, partition clustering, hierarchical clustering, principal component analysis (PCA), and association analysis are some of the unsupervised learn-
ing algorithms [64,65]. 5. Predictive and descriptive data mining tasks. 5.Predictive and descriptive data mining tasks.
The dataset is the primary distinction between supervised and unsupervised machine
learning. It is referred to as supervised learning if the dataset employs a labeled dataset for
input and output, whereas unsupervised learning techniques unlabeled data. As the
name suggests, supervised learning entails the external supervision of a model’s training.
Unsupervised learning, on the other hand, does not involve any supervision. Additionally,
in the case of supervised learning, the goal is to predict the outcome of data. In the
case of unsupervised learning, the goal is to find hidden patterns and gain insight from
enormous amounts of data. In contrast to supervised learning models, which are
straightforward, unsupervised learning models require a large training set to produce
the desired results, making them computationally complex. Some of the applications of J. Pers. Med.2022,12, 135916 of 23
supervised learning models include diagnosis, identity fraud detection, image classification,
price predictions, sentiment analysis, spam detection, market forecasting, and weather fore-
casting. Unsupervised learning models are in the pipelines for anomaly detection, big
data visualization, customer personas, feature elicitation, recommended systems, structure
discovery, and targeted marketing [64,66].
As an instance of the modeling example, the suitability of a WebCrawler (Storm-
Crawler) for the acquisition of all health-related web content on the German Health Web
(Germany, Austria, and Switzerland) was investigated by Zowalla . [67]. For this
purpose, a support vector machine classifier model was trained to distinguish between
health-related and non-health-related web pages the dataset created from the German
health web. This model was tested for accuracy and precision on an 80/20 training/test
split and against a crowd-validated dataset. For predicting cardiovascular diseases, the
best-suited technique was the ‘Decision Tree’ compared with eight other techniques, i.e.,
Deep Learning, Nearest Neighbor (k-NN), Gradient Boosted Tree, Generalized Linear
Model, Logistic Regression, Naïve Bayes, Random Forest, and Rule Induction [18]. Fur-
thermore, some parameters were optimized the optimized parameters operator to
achieve better results when the ‘Decision Tree’.
2.5. Data Model Validation and Testing
This step’s primary goal is to validate and test the selected model for the data in
the model development process. The validation procedure is to ensure that the
developed model is accurate enough for the intended [68]. The first half of this
step, model validation, is important because the /newly developed model cannot
be relied on solely because it was designed to fit the training data and demonstrates that
the training data fits the model well. To validate a model, output predictions are made in
scenarios unrelated to the training set, and the same statistical measures of fit are computed.
The second half of this step involves testing the model with test data and comparing its
accuracy with the results of the validation step. Only when a model is compared to test
data and statistical calculations show a satisfactory match is it considered “ready”. For the
classification of tumor and non-tumor samples, Dong . [69] employed a training dataset
(which consists of mass spectrometry (MS) raw data obtained from 194 paired tumor and
non-tumor samples) to train different models and a similar type of dataset (which
consists of MS raw data obtained from 58 paired tumor and non-tumor samples) as a test
dataset. The convolutional neural network (CNN), gradient boosting decision tree (GBDT),
support-vector machine (SVM), principal component analysis (PCA) plus SVM, logistic
regression (LR), and random forest (RF) were compared, and the CNN model showed the
highest accuracy. Some of the ML model validation testing tools include Apache Spark,
Excel, Hadoop, KNIME, Python, R, RapidMiner, SAS, SQL, and Tableau.
2.6. Evaluation
In the fifth phase (known as evaluation) of CRISP-DM, a more thorough evaluation and
review of the model’s construction is conducted to ensure that the model properly achieves
the business objectives. The steps in the evaluation phase are (1) the assessment of outcomes
to assess how well the model achieves the project’s goals, discover additional constraints,
information, or clues about future paths, and present the project’s final statement, (2) the
review process to conduct a more in-depth review of the project and address quality
assurance concerns, and (3) the decision for further steps to determine whether or not to
proceed with the deployment or to make changes for the improvement [14].
After the analysis of data, the next step is to visualize the data meaningfully for
interpretation and communication purposes. visualization is primarily accomplished
through the of charts, graphs, maps, timelines, networks, word clouds, and so on. These
visualized results allow humans to read the most important aspects of a large amount of
information. There are several tools available to display the analyzed data. These tools
make it easy to identify and discover patterns, outliers, trends, and insights in data straight- J. Pers. Med.2022,12, 135917 of 23
forwardly and understandably. Effective data visualization has benefits and advantages
such as easy understanding of the outcome, effortless and prompt decision-making, and
a higher degree of engagement for a diverse audience over other communication meth-
ods (e.g., verbal communication). For successful data visualization, there are three main
principles: (1) Depending on the purpose, select the appropriate visualization style, (2) the
selected visualization style should be appropriate for the targeted audience, and (3) the
chosen visualization style should be accompanied by an effective graphic design [70]. The
most important aspects of selecting the appropriate visualization style are considering the
selected data and the aim of the visualization. For example, line and bar charts are suitable
for comparing data points across a dataset. Diverse visualization styles are available for
creating attractive and effective visual information, i.e., typographic visualization (e.g.,
word cloud), graph visualization (e.g., tree), chart visualization (e.g., bar/line chat), 3D
visualization, etc. Below, in Table 7, we provide a list of various visualization styles along
with a few of the available tools in each category.
Table 7.Data visualization style with exemplary tools.
Visualization StyleTool [Reference] marking/highlighting
cite2vec [71], TopicLens [72], SurVis [73], Poemage [74],
Overview [75]
Tags or word cloud
SentenTree [76], InfoVis [77], VisOHC [78], IncreSTS [79], Word
storms [80]
Bar charts
TextTile [81], SentiCompass [82], NewsViews [83], WeiboEvents
[84], CatStream [85]
Scatterplot
PhenoLines [86], SocialBrands [87], TopicPanorama [88],
#FluxFlow [89], PEARL [90]
Line chart
Vispubdata.org [91], GameFlow [92], MultiConVis [93],
Contextifier [94], Google+Ripples [95]
Node-link
NEREx [96], iForum [97], NameClarifier [98], DIA2 [99],
Information Cartography [100]
Tree
OpinionFlow [101], Rule- Visual Mappings [102],
HierarchicalTopics [103], Whisper [104], The World’s Languages
Explorer [105]
Matrix
Interactive Ambiguity Resolution [106], Fingerprint
Matrices [107], Conceptual recurrence plots [108], The
Deshredder [109], Termite [110]
Stream graph timeline
VAiRoma [111], CiteRivers [112], ThemeDelta [113], EvoRiver
[114], LeadLine [115]
Flow timelineTimeLineCurator [116], Interactive visual profiling [117]
Radial visualizationConToVi [118], ConVis [119]
3D visualizationTwo-stage Framework [120]
Maps/Geo chart
Can Twitter save lives? [121], Visualizing Dynamic Data with
Maps [122], Spatiotemporal Anomaly Detection [123]
Besides these tools, there are software available with gigantic capabilities to visualize
the data, such as, Microsoft Excel’s PivotTables, R, Tableau, Power-BI, datawrapper, and
Google Charts. These tools are easy to and very helpful in creating a clear and dynamic
display of data because of their interactive graphical interface. Furthermore, different li-
braries written in different programming languages are also available for data visualization,
which are easy to for programmers, such as JavaScript libraries (e.g., D3.js, Chart.js,
and Highcharts), python libraries (e.g., Matplotlib, Seaborn, and Plotly), and R libraries
(e.g., ggplot2, Leaflet, and Esquisse). The major challenges of data visualization are the
massive amount of data, the complexity of data, and missing/duplicate entries [124]. J. Pers. Med.2022,12, 135918 of 23
2.7. Deployment
In the deployment phase (sixth and final phase of CRISP-DM, Shearer [14]), the
knowledge gained from the project is organized and presented (e.g., live demonstrations)
in a way that is useful for the project, the company, and the customer. This phase’s
complexity varies greatly. The steps in the deployment phase are as follows: (1) Create
a deployment plan to formulate and note a deployment strategy for the model, (2) plan
the monitoring and maintenance to create well-thought-out planning of maintenance and
monitoring to shun problems during the operational phase of a model, (3) produce a final
report to prepare and present a final report of the project in the form of a written document
and verbal meeting, and (4) review the project to evaluate successes and failures, as well as
potential areas for improvement in future projects.
3. Conclusions and Future Outlook
The amount of medical data is rapidly increasing. From medical data, data
mining can be to extract and useful information or knowledge. The CRISP-DM
system presented in this study focuses on each step of data mining while medical
examples to explain each step. The authors plan to develop an artificial intelligence-
web crawling system with 4D visualization of the data in a summarized and easy-to-
understand manner and these data as a source of information for researchers, as well
as for the education of patients and medical staff in future work.
Author Contributions:Conceptualization, A.Z., M.A., I.P., S.A.K., A.F.H. and A.R.A.;writing—original
draft preparation, A.Z., M.A., I.P. and A.R.A.; writing—review and editing, A.Z., M.A. and A.R.A.;
Artificial Intelligence and Machine Learning in Medical Literature Data Mining, M.A., S.A.K. and
A.F.H.; Natural Language Processing, A.Z., S.A.K. and A.F.H.; funding acquisition, M.A. and A.R.A.
All authors have read and agreed to the published version of the manuscript.
Funding:
This work was Funded by the German Bundesministerium für Bildung und Forschung
(BMBF) under the CELTIC-NEXT AI-NET-PROTECT project.
Institutional Review Board Statement:Not applicable.
Informed Consent Statement:Not applicable.
Data Availability Statement:Not applicable.
Acknowledgments:The authors acknowledge the support by the German Research Foundation
(DFG) and the Open Publication Fund of the University of Göttingen.
Conflicts of Interest:The authors declare no conflict of interest.
References
1.
Sumathy, K.L.; Chidambaram, M. Mining: Concepts, Applications, Tools and Issues—An Overview.Int. J. Comput. Appl.
2013,80, 29–32. [CrossRef]
2.Cios, K.J.; Moore, G.W. Uniqueness of medical data mining.Artif. Intell. Med.2002,26, 1–24. [CrossRef]
3.Yang, Y.; Li, R.; Xiang, Y.; Lin, D.; Yan, A.; Chen, W.; Li, Z.; Lai, W.; Wu, X.; Wan, C.; . Standardization of Collection, Storage,
Annotation, and Management of Data Related to Medical Artificial Intelligence.Intell. Med.2021. [CrossRef]
4.
Thorpe, J.H.; Gray, E.A. Big data and public health: Navigating privacy laws to maximize potential.Public Health Rep.2015,130,
171–175. [CrossRef]
5.McGuire, A.L.; Beskow, L.M. Informed consent in genomics and genetic research.Annu. Rev. Genom. Hum. Genet.2010,11,
361–381. [CrossRef]
6.Tayefi, M.; Ngo, P.; Chomutare, T.; Dalianis, H.; Salvi, E.; Budrionis, A.; Godtliebsen, F. Challenges and opportunities beyond
structured data in analysis of electronic health records.WIREs Comp. Stat.2021,13, 1–19. [CrossRef]
7.
Han, J.; Kamber, M.; Pei, J.Data Mining: Concepts and Techniques, 3rd ed.; Morgan Kaufmann Publisher: Waltham, MA, USA, 2011;
ISBN 978-0-12-381479-1.
8.
Locke, S.; Bashall, A.; -Adely, S.; Moore, J.; Wilson, A.; Kitchen, G.B. Natural language processing in medicine: A review.Trends
Anaesth. Crit. Care2021,38, 4–9. [CrossRef]
9.Vyas, A. Top 14 Cases of Natural Language Processing in Healthcare. 6 July 2019. Available online: https://marutitech.com/
-cases-of-natural-language-processing-in-healthcare/ (accessed on 29 July 2022). J. Pers. Med.2022,12, 135919 of 23
10.Liu, Z.; Yang, M.; Wang, X.; Chen, Q.; Tang, B.; Wang, Z.; Xu, H. Entity recognition from clinical texts via recurrent neural network.
BMC Med. Inform. Decis. Mak.2017,17, 67. [CrossRef]
11.
Deng, Y.; Faulstich, L.; Denecke, K. Concept Embedding for Relevance Detection of Search Queries Regarding CHOP.Stud. Health
Technol. Inform.2017,245, 1260.
12.
Afzal, M.; Hussain, M.; Malik, K.M.; Lee, S. Impact of Automatic Query Generation and Quality Recognition Deep Learning
to Curate Evidence from Biomedical Literature: Empirical Study.JMIR Med. Inform.2019,7, e13430. [CrossRef]
13.
Pandey, B.; Kumar Pandey, D.; Pratap Mishra, B.; Rhmann, W. A comprehensive survey of deep learning in the field of medical
imaging and medical natural language processing: Challenges and research directions.J. King Saud Univ. Comput. Inf. Sci.2021,
34, 5083–5099. [CrossRef]
14.Shearer, C. The CRISP-DM Model: The Blueprint for Data Mining.Int. J. Data Warehous.2000,5, 13–22.
15.
Costa, C.J.; Aparicio, J.T. POST-DS: A Methodology to Boost Data Science. In Proceedings of the 15th Iberian Conference on
Information Systems and Technologies (CISTI), Sevilla, Spain, 24–27 June 2020; . 1–6, ISBN 978-989-54659-0-3.
16.Catley, C.; Smith, K.; McGregor, C.; Tracy, M. Extending CRISP-DM to incorporate temporal data mining of multidimensional
medical data streams: A neonatal intensive care unit case study. In Proceedings of the 22nd IEEE International Symposium on
Computer- Medical Systems, Albuquerque, NM, USA, 2–5 August 2009; . 1–5, ISBN 978-1-4244-4878-4.
17.Data Science Process Alliance. What Is CRISP DM? Available online: https://www.datascience-pm.com/crisp-dm-2/ (accessed
on 16 April 2022).
18.Martins, B.; Ferreira, D.; Neto, C.; Abelha, A.; Machado, J. Data Mining for Cardiovascular Disease Prediction.J. Med. Syst.2021,
45, 6. [CrossRef]
19.NCBI Resource Coordinators. Database resources of the National Center for Biotechnology Information.Nucleic Acids Res.2013,
41, D8–D20. [CrossRef]
20.Guo, C.X.; He, L.; Yin, J.Y.; Meng, X.G.; Tan, W.; Yang, G.P.; Bo, T.; Liu, J.P.; Lin, X.J.; Chen, X. Epidemiological and clinical features
of pediatric COVID-19.BMC Med.2020,18, 250. [CrossRef] [PubMed]
21.Miut
,
escu, A. Web Scraping vs. Web Crawling: Understand the Difference. WebScrapingAPI [Online]. 7 January 2021. Available
online: https://www.webscrapingapi.com/web-scraping-vs-web-crawling/ (accessed on 19 April 2022).
22.Octoparse. What Is Web Scraping—Basics & Practical Uses—DataDrivenInvestor. DataDrivenInvestor [Online]. 25 January 2022.
Available online: https://medium.datadriveninvestor.com/what-is-web-scraping-basics-practical-uses-66e1063cfa74 (accessed
on 19 April 2022).
23.Batsakis, S.; Petrakis, E.G.; Milios, E. Improving the performance of focused web crawlers.Data Knowl. Eng.2009,68, 1001–1013.
[CrossRef]
24.Yuan, X.; MacGregor, M.H.; Harms, J. An efficient scheme to remove crawler traffic from the Internet. In Proceedings of
the Eleventh International Conference on Computer Communications and Networks. Eleventh International Conference on
Computer Communications and Networks, Miami, FL, USA, 14–16 October 2002; . 90–95, ISBN 0-7803-7553-X.
25.DeVito, N.J.; Richards, G.C.; Inglesby, P. How we learnt to stop worrying and love web scraping.Nature2020,585, 621–622.
[CrossRef]
26.Kaur, A.; Chopra, D. Comparison of mining tools. In Proceedings of the 5th International Conference on Reliability, Infocom
Technologies and Optimization (ICRITO) (Trends and Future Directions), Noida, India, 9 July–9 September 2016; . 186–192,
ISBN 978-1-5090-1489-7.
27.Chandra, R.V.; Varanasi, B.S.Python Requests Essentials: Learn How to Integrate Your Applications Seamlessly with Web Services Python Requests; Packt: Birmingham, UK; Mumbai, India, 2015; ISBN 9781784395414.
28.Kouzis-Loukas, D.Learning Scrapy: Learn the Art of Efficient Web Scraping and Crawling with Python; Packt: Birmingham, UK, 2016;
ISBN 9781784390914.
29.Richardson, L. Beautiful Soup Documentation. Available online: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
(accessed on 16 April 2022).
30.Sharma, P.R.Selenium with Python: A Beginner’s Guide; BPB: Delhi, India, 2019; ISBN 9789389328820.
31.
Gu, D.; Li, J.; Li, X.; Liang, C. Visualizing the knowledge structure and evolution of big data research in healthcare informatics.
Int. J. Med. Inform.2017,98, 22–32. [CrossRef]
32.Ristevski, B.; Chen, M. Big Data Analytics in Medicine and Healthcare.J. Integr. Bioinform.2018,15, 1–5. [CrossRef]
33.Giffen, C.A.; Carroll, L.E.; Adams, J.T.; Brennan, S.P.; Coady, S.A.; Wagner, E.L. Providing Contemporary to Historical
Biospecimen Collections: Development of the NHLBI Biologic Specimen and Data Repository Information Coordinating Center
(BioLINCC).Biopreserv. Biobank.2015,13, 271–279. [CrossRef]
34.
Cimino, J.J.; Ayres, E.J.; Remennik, L.; Rath, S.; Freedman, R.; Beri, A.; Chen, Y.; Huser, V. The National Institutes of Health’s
Biomedical Translational Research Information System (BTRIS): Design, contents, functionality and experience to date.J. Biomed.
Inform.2014,52, 11–27. [CrossRef] [PubMed]
35.Mayo-Wilson, E.; Doshi, P.; Dickersin, K. Are manufacturers sharing data as promised?BMJ2015,351, h4169. [CrossRef]
[PubMed]
36.
Doll, K.M.; Rademaker, A.; Sosa, J.A. Practical Guide to Surgical Data Sets: Surveillance, Epidemiology, and End Results (SEER)
Database.JAMA Surg.2018,153, 588–589. [CrossRef] [PubMed] J. Pers. Med.2022,12, 135920 of 23
37.Johnson, A.E.W.; Pollard, T.J.; Shen, L.; Lehman, L.W.H.; Feng, M.; Ghassemi, M.; Moody, B.; Szolovits, P.; Celi, L.A.; Mark, R.G.
MIMIC-III, a freely accessible critical care database.Sci. Data2016,3, 160035. [CrossRef]
38.
Johnson, A.; Bulgarelli, L.; Pollard, T.; Horng, S.; Celi, L.A.; Mark, R.MIMIC-IV; Version 1.0; PhysioNet: Cambridge, MA,
USA, 2021. [CrossRef]
39.
Johnson, A.E.W.; Pollard, T.J.; Berkowitz, S.J.; Greenbaum, N.R.; Lungren, M.P.; Deng, C.Y.; Mark, R.G.; Horng, S. MIMIC-CXR, a
de-identified publicly available database of chest radiographs with free- reports.Sci. Data2019,6, 317. [CrossRef]
40.
Ahluwalia, N.; Dwyer, J.; Terry, A.; Moshfegh, A.; Johnson, C. Update on NHANES Dietary Data: Focus on Collection, Release,
Analytical Considerations, and Uses to Inform Public Policy.Adv. Nutr.2016,7, 121–134. [CrossRef]
41.Vos, T.; Lim, S.S.; Abbafati, C.; Abbas, K.M.; Abbasi, M.; Abbasifard, M.; Abbasi-Kangevari, M.; Abbastabar, H.; Abd-Allah, F.;
Abdelalim, A.; . Global burden of 369 diseases and injuries in 204 countries and territories, 1990–2019: A systematic analysis
for the Global Burden of Disease Study 2019.Lancet2020,396, 1204–1222. [CrossRef]
42.Palmer, L.J. UK Biobank: Bank on it.Lancet2007,369, 1980–1982. [CrossRef]
43.
Weinstein, J.N.; Collisson, E.A.; Mills, G.B.; Shaw, K.R.M.; Ozenberger, B.A.; Ellrott, K.; Shmulevich, I.; Sander, C.; Stuart, J.M. The
Cancer Genome Atlas Pan-Cancer analysis project.Nat. Genet.2013,45, 1113–1120. [CrossRef]
44.Davis, S.; Meltzer, P.S. GEOquery: A bridge between the Gene Expression Omnibus (GEO) and BioConductor.Bioinformatics2007,
23, 1846–1847. [CrossRef]
45.
Woolley, C.S.C.; Handel, I.G.; Bronsvoort, B.M.; Schoenebeck, J.J.; Clements, D.N. Is it time to stop sweeping data cleaning under
the carpet? A novel algorithm for outlier management in growth data.PLoS ONE2020,15, e0228154. [CrossRef] [PubMed]
46.
Coupler.io Blog. Data Cleansing vs. Data Transformation|Coupler.io Blog. Available online: https://blog.coupler.io/data-
cleansing-vs-data-transformation/#What_is_data_transformation (accessed on 24 June 2022).
47.Elgabry, O. The Ultimate Guide to Data Cleaning—Towards Data Science. 28 February 2019. Available online: https://
towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4 (accessed on 24 June 2022).
48.Harris, C.R.; Millman, K.J.; van der Walt, S.J.; Gommers, R.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor, J.; Berg, S.;Smith, N.J.; .
Array programming with NumPy.Nature2020,585, 357–362. [CrossRef] [PubMed]
49.McKinney, W. Data structures for statistical computing in python. In Proceedings of the Python in Science Conference, Austin,
TX, USA, 28 June–3 July 2010; . 51–56.
50.Gordon, B.; Fennessy, C.; Varma, S.; Barrett, J.; McCondochie, E.; Heritage, T.; Duroe, O.; Jeffery, R.; Rajamani, V.; Earlam, K.; .
Evaluation of freely available data profiling tools for health data research application: A functional evaluation review.BMJ Open
2022,12, e054186. [CrossRef] [PubMed]
51.Lopez, P. GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications. In
Research and Advanced Technology for Digital Libraries; Agosti, M., Borbinha, J., Kapidakis, S., Papatheodorou, C., Tsakonas, G., Eds.;
Springer: Berlin/Heidelberg, Germany, 2009; . 473–474, ISBN 978-3-642-04345-1.
52.
Lo, K.; Wang, L.L.; Neumann, M.; Kinney, R.; Weld, D. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, Online, 5–10 July 2020; Jurafsky, D., Chai, J.,
Schluter, N., Tetreault, J., Eds.; Association for Computational Linguistics: Stroudsburg, PA, USA, 2020; . 4969–4983.
53.Comeau, D.C.; Wei, C.H.; Do ̆gan, R.I.; Lu, Z. PMC mining subset in BioC: 2.3 million full articles and growing.arXiv
2018, arXiv:1804.05957. [CrossRef]
54.Rawat, T.; Khemchandani, V. Feature engineering (FE) tools and techniques for better classification performance.Int. J. Innov.
Eng. Technol.2017,8, 169–179. [CrossRef]
55.Heaton, J. An empirical analysis of feature engineering for predictive modeling. In Proceedings of the SoutheastCon 2016,
Norfolk, VA, USA, 30 March–3 April 2016; IEEE: Manhattan, NY, USA, 2016; . 1–6, ISBN 978-1-5090-2246-5.
56.
Vijithananda, S.M.; Jayatilake, M.L.; Hewavithana, B.; Gonçalves, T.; Rato, L.M.; Weerakoon, B.S.; Kalupahana, T.D.; Silva, A.D.;
Dissanayake, K.D. Feature extraction from MRI ADC images for brain tumor classification machine learning techniques.
BioMed Eng. OnLine2022,21, 52. [CrossRef]
57.Rus, A. Keyword-Recherche: Die richtigen Keywords Finden Leicht Gemacht. Evergreen Media AR GmbH. 7 September 2021.
Available online: https://www.evergreenmedia.at/ratgeber/keyword-recherche/ (accessed on 20 April 2022).
58.Singh, V. Replace or Retrieve Keywords in Documents at Scale. 2017. Available online: https://arxiv.org/pdf/1711.00046
(accessed on 19 April 2022).
59.Bird, S.; Klein, E.; Loper, E.Natural Language Processing with Python; O’Reilly: Beijing, China; Farnham, UK, 2009;
ISBN 9780596516499.
60.Honnibal, M. spaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and
Incremental Parsing. Sentometrics Research. 1 January 2017. Available online: https://sentometrics-research.com/publication/
72/ (accessed on 19 April 2022).
61.Pedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Müller, A.; Nothman, J.; Louppe, G.; .
Scikit-learn: Machine Learning in Python.JMLR2011,12, 2825–2830.
62.Pinto, A.; Oliveira, H.G.; Alves, A.O. Comparing the Performance of Different NLP Toolkits in Formal and Social Media . In
Proceedings of the 5th Symposium on Languages, Applications and Technologies (SLATE’16), Maribor, Slovenia, 20–21 June 2016;
Mernik, M., Leal, J.P., Oliveira, H.G., Eds.; Schloss Dagstuhl—Leibniz-Zentrum fuer Informatik GmbH: Wadern/Saarbruecken,
Germany, 2016; ISBN 978-3-95977-006-4. J. Pers. Med.2022,12, 135921 of 23
63.Rehurek, R.; Sojka, P. Gensim-python framework for vector space modelling.NLP Cent. Fac. Inform. Masaryk. Univ. Brno Czech
Repub.2011,3, 2.
64.
Nadif, M.; Role, F. Unsupervised and self-supervised deep learning approaches for biomedical mining.Brief. Bioinform.2021,
22, 1592–1603. [CrossRef]
65.
Wu, W.T.; Li, Y.J.; Feng, A.Z.; Li, L.; Huang, T.; Xu, A.D.; Lyu, J. Data mining in clinical big data: The frequently databases,
steps, and methodological models.Mil. Med. Res.2021,8, 44. [CrossRef]
66.Berry, M.W.Supervised and Unsupervised Learning for Data Science; Springer: Berlin/Heidelberg, Germany, 2020; ISBN 978-3-030-22474-5.
67.
Zowalla, R.; Wetter, T.; Pfeifer, D. Crawling the German Health Web: Exploratory Study and Graph Analysis.J. Med. Internet Res.
2020,22, e17853. [CrossRef] [PubMed]
68.
Tsioptsias, N.; Tako, A.; Robinson, S. (Eds.)Model Validation and Testing in Simulation: A Literature Review; Schloss
Dagstuhl—Leibniz-Zentrum fuer Informatik GmbH: Wadern/Saarbruecken, Germany, 2016; p. 11.
69.
Dong, H.; Liu, Y.; Zeng, W.-F.; Shu, K.; Zhu, Y.; Chang, C. A Deep Learning- Tumor Classifier Directly MS Raw Data.
Proteomics2020,20, e1900344. [CrossRef] [PubMed]
70.
OWOX. What Is Data Visualization: Definition, Examples, Principles, Tools. Available online: https://www.owox.com/blog/
articles/data-visualization/ (accessed on 12 April 2022).
71.Berger, M.; McDonough, K.; Seversky, L.M. cite2vec: Citation-Driven Document Exploration via Word Embeddings.IEEE Trans.
Vis. Comput. Graph.2017,23, 691–700. [CrossRef] [PubMed]
72.Kim, M.; Kang, K.; Park, D.; Choo, J.; Elmqvist, N. TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale
Document Collections.IEEE Trans. Vis. Comput. Graph.2017,23, 151–160. [CrossRef]
73.Beck, F.; Koch, S.; Weiskopf, D. Visual Analysis and Dissemination of Scientific Literature Collections with SurVis.IEEE Trans. Vis.
Comput. Graph.2016,22, 180–189. [CrossRef]
74.McCurdy, N.; Lein, J.; Coles, K.; Meyer, M. Poemage: Visualizing the Sonic Topology of a Poem.IEEE Trans. Vis. Comput. Graph.
2016,22, 439–448. [CrossRef]
75.Brehmer, M.; Ingram, S.; Stray, J.; Munzner, T. Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool
for Investigative Journalists.IEEE Trans. Vis. Comput. Graph.2014,20, 2271–2280. [CrossRef]
76.Hu, M.; Wongsuphasawat, K.; Stasko, J. Visualizing Social Media Content with SentenTree.IEEE Trans. Vis. Comput. Graph.2017,
23, 621–630. [CrossRef]
77.Hinrichs, U.; Forlini, S.; Moynihan, B. Speculative Practices: Utilizing InfoVis to Explore Untapped Literary Collections.IEEE
Trans. Vis. Comput. Graph.2016,22, 429–438. [CrossRef]
78.Kwon, B.C.; Kim, S.-H.; Lee, S.; Choo, J.; Huh, J.; Yi, J.S. VisOHC: Designing Visual Analytics for Online Health Communities.
IEEE Trans. Vis. Comput. Graph.2016,22, 71–80. [CrossRef]
79.Liu, C.-Y.; Chen, M.-S.; Tseng, C.-Y. IncreSTS: Towards -Time Incremental Short Summarization on Comment Streams
from Social Network Services.IEEE Trans. Knowl. Data Eng.2015,27, 2986–3000. [CrossRef]
80.Castellà, Q.; Sutton, C. Word storms: Multiples of word clouds for visual comparison of documents. In Proceedings of the 23rd
International Conference on World Wide Web—WWW ‘14, Seoul, Korea, 7–11 April 2014; Chung, C.-W., Broder, A., Shim, K.,
Suel, T., Eds.; ACM Press: York, NY, USA, 2014; . 665–676, ISBN 9781450327442.
81.Felix, C.; Pandey, A.V.; Bertini, E. TextTile: An Interactive Visualization Tool for Seamless Exploratory Analysis of Structured Data
and Unstructured .IEEE Trans. Vis. Comput. Graph.2017,23, 161–170. [CrossRef] [PubMed]
82.Wang, F.Y.; Sallaberry, A.; Klein, K.; Takatsuka, M.; Roche, M. SentiCompass: Interactive visualization for exploring and
comparing the sentiments of time-varying twitter data. In Proceedings of the 2015 IEEE Pacific Visualization Symposium
(PacificVis), Hangzhou, China, 14–17 April 2015; . 129–133, ISBN 978-1-4673-6879-7.
83.
Gao, T.; Hullman, J.R.; Adar, E.; Hecht, B.; Diakopoulos, N. NewsViews: An automated pipeline for creating custom geovisual-
izations for news. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Toronto, ON, Canada,
26 April–1 May 2014; Jones, M., Palanque, P., Schmidt, A., Grossman, T., Eds.; ACM: York, NY, USA, 2014; . 3005–3014,
ISBN 9781450324731.
84.
Ren, D.; Zhang, X.; Wang, Z.; Li, J.; Yuan, X. WeiboEvents: A Crowd Sourcing Weibo Visual Analytic System. In Proceedings of the
2014 IEEE Pacific Visualization Symposium (PacificVis), Yokohama, Japan, 4–7 March 2014; . 330–334, ISBN 978-1-4799-2874-3.
85.Garcia Esparza, S.; O’Mahony, M.P.; Smyth, B. CatStream: Categorising tweets for profiling and stream filtering. In
Proceedings of the 2013 International Conference on Intelligent Interfaces—IUI ’13, Santa Monica, CL, USA, 19–22 March
2013; Kim, J., Nichols, J., Szekely, P., Eds.; ACM Press: York, NY, USA, 2013; p. 25, ISBN 9781450319652.
86.Glueck, M.; Naeini, M.P.; Doshi-Velez, F.; Chevalier, F.; Khan, A.; Wigdor, D.; Brudno, M. PhenoLines: Phenotype Comparison
Visualizations for Disease Subtyping via Topic Models.IEEE Trans. Vis. Comput. Graph.2018,24, 371–381. [CrossRef] [PubMed]
87.Liu, X.; Xu, A.; Gou, L.; Liu, H.; Akkiraju, R.; Shen, H.-W. SocialBrands: Visual analysis of public perceptions of brands on social
media. In Proceedings of the 2016 IEEE Conference on Visual Analytics Science and Technology (VAST), Baltimore, MD, USA,
23–28 October 2016; . 71–80, ISBN 978-1-5090-5661-3.
88.Wang, X.; Liu, S.; Liu, J.; Chen, J.; Zhu, J.; Guo, B. TopicPanorama: A Full Picture of Relevant Topics.IEEE Trans. Vis. Comput.
Graph.2016,22, 2508–2521. [CrossRef]
89.Zhao, J.; Cao, N.; Wen, Z.; Song, Y.; Lin, Y.-R.; Collins, C. #FluxFlow: Visual Analysis of Anomalous Information Spreading on
Social Media.IEEE Trans. Vis. Comput. Graph.2014,20, 1773–1782. [CrossRef] J. Pers. Med.2022,12, 135922 of 23
90.Zhao, J.; Gou, L.; Wang, F.; Zhou, M. PEARL: An interactive visual analytic tool for understanding personal emotion style derived
from social media. In Proceedings of the 2014 IEEE Conference on Visual Analytics Science and Technology (VAST), Paris, France,
25–31 October 2014; . 203–212, ISBN 978-1-4799-6227-3.
91.Isenberg, P.; Heimerl, F.; Koch, S.; Isenberg, T.; Xu, P.; Stolper, C.D.; Sedlmair, M.; Chen, J.; Moller, T.; Stasko, J. Vispubdata.org:
A Metadata Collection About IEEE Visualization (VIS) Publications.IEEE Trans. Vis. Comput. Graph.2017,23, 2199–2206.
[CrossRef]
92.
Chen, W.; Lao, T.; Xia, J.; Huang, X.; Zhu, B.; Hu, W.; Guan, H. GameFlow: Narrative Visualization of NBA Basketball Games.
IEEE Trans. Multimed.2016,18, 2247–2256. [CrossRef]
93.Hoque, E.; Carenini, G. MultiConVis: A Visual Analytics System for Exploring a Collection of Online Conversations. In
Proceedings of the 21st International Conference on Intelligent Interfaces, Sonoma, CL, USA, 7–10 March 2016; Nichols, J.,
Mahmud, J., O’Donovan, J., Conati, C., Zancanaro, M., Eds.; ACM: York, NY, USA, 2016; . 96–107, ISBN 9781450341370.
94.
Hullman, J.; Diakopoulos, N.; Adar, E. Contextifier: Automatic Generation of Annotated Stock Visualizations. In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, Paris, France, 27 April–2 May 2013; Mackay, W.E., Brewster, S.,
Bødker, S., Eds.; ACM: York, NY, USA, 2013; . 2707–2716, ISBN 9781450318990.
95.Viégas, F.; Wattenberg, M.; Hebert, J.; Borggaard, G.; Cichowlas, A.; Feinberg, J.; Orwant, J.; Wren, C. Google + Ripples: A Native
Visualization of Information Flow. In Proceedings of the 22nd International Conference on World Wide Web—WWW ’13, Rio de
Janeiro, Brazil, 13–17 May 2013; Schwabe, D., Almeida, V., Glaser, H., Baeza-Yates, R., Moon, S., Eds.; ACM: York, NY, USA,
2013; . 1389–1398, ISBN 9781450320351.
96.
El-Assady, M.; Sevastjanova, R.; Gipp, B.; Keim, D.; Collins, C. NEREx: Named-Entity Relationship Exploration in Multi-Party
Conversations.Comput. Graph. Forum2017,36, 213–225. [CrossRef]
97.Fu, S.; Zhao, J.; Cui, W.; Qu, H. Visual Analysis of MOOC Forums with iForum.IEEE Trans. Vis. Comput. Graph.2017,23, 201–210.
[CrossRef]
98.Shen, Q.; Wu, T.; Yang, H.; Wu, Y.; Qu, H.; Cui, W. NameClarifier: A Visual Analytics System for Author Name Disambiguation.
IEEE Trans. Vis. Comput. Graph.2017,23, 141–150. [CrossRef]
99.Madhavan, K.; Elmqvist, N.; Vorvoreanu, M.; Chen, X.; Wong, Y.; Xian, H.; Dong, Z.; Johri, A. DIA2: Web- Cyberin-
frastructure for Visual Analysis of Funding Portfolios.IEEE Trans. Vis. Comput. Graph.2014,20, 1823–1832. [CrossRef]
[PubMed]
100.Shahaf, D.; Yang, J.; Suen, C.; Jacobs, J.; Wang, H.; Leskovec, J. Information cartography: Creating Zoomable, Large-Scale Maps of
Information. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
Chicago, IL, USA, 11–14 August 2013; Ghani, R., Senator, T.E., Bradley, P., Parekh, R., He, J., Grossman, R.L., Uthurusamy, R.,
Dhillon, I.S., Koren, Y., Eds.; ACM: York, NY, USA, 2013; . 1097–1105, ISBN 9781450321747.
101.Wu, Y.; Liu, S.; Yan, K.; Liu, M.; Wu, F. OpinionFlow: Visual Analysis of Opinion Diffusion on Social Media.IEEE Trans. Vis.
Comput. Graph.2014,20, 1763–1772. [CrossRef] [PubMed]
102.Abdul-Rahman, A.; Lein, J.; Coles, K.; Maguire, E.; Meyer, M.; Wynne, M.; Johnson, C.R.; Trefethen, A.; Chen, M. Rule-
Visual Mappings—with a Case Study on Poetry Visualization.Comput. Graph. Forum2013,32, 381–390. [CrossRef]
103.Dou, W.; Yu, L.; Wang, X.; Ma, Z.; Ribarsky, W. HierarchicalTopics: Visually exploring large collections topic hierarchies.
IEEE Trans. Vis. Comput. Graph.2013,19, 2002–2011. [CrossRef]
104.Cao, N.; Lin, Y.R.; Sun, X.; Lazer, D.; Liu, S.; Qu, H. Whisper: Tracing the Spatiotemporal Process of Information Diffusion in Time.IEEE Trans. Vis. Comput. Graph.2012,18, 2649–2658. [CrossRef]
105.Rohrdantz, C.; Hund, M.; Mayer, T.; Wälchli, B.; Keim, D.A. The World’s Languages Explorer: Visual Analysis of Language
Features in Genealogical and Areal Contexts.Comput. Graph. Forum2012,31, 935–944. [CrossRef]
106.
Stoffel, F.; Jentner, W.; Behrisch, M.; Fuchs, J.; Keim, D. Interactive Ambiguity Resolution of Named Entities in Fictional Literature.
Comput. Graph. Forum2017,36, 189–200. [CrossRef]
107.Oelke, D.; Kokkinakis, D.; Keim, D.A. Fingerprint Matrices: Uncovering the dynamics of social networks in prose literature.
Comput. Graph. Forum2013,32, 371–380. [CrossRef]
108.
Angus, D.; Smith, A.; Wiles, J. Conceptual recurrence plots: Revealing patterns in human discourse.IEEE Trans. Vis. Comput.
Graph.2012,18, 988–997. [CrossRef]
109.Butler, P.; Chakraborty, P.; Ramakrishan, N. The Deshredder: A visual analytic to reconstructing shredded docu-
ments. In Proceedings of the 2012 IEEE Conference on Visual Analytics Science and Technology (VAST), Seattle, WA, USA,
14–19 October 2012; . 113–122, ISBN 978-1-4673-4753-2.
110.Chuang, J.; Manning, C.D.; Heer, J. Termite: Visualization techniques for assessing textual topic models. In Proceedings of
the International Working Conference on Advanced Visual Interfaces—AVI ‘12, Capri Island, Naples, Italy, 21–25 May 2012;
Tortora, G., Levialdi, S., Tucci, M., Eds.; ACM: York, NY, USA, 2012; p. 74, ISBN 9781450312875.
111.Cho, I.; Dou, W.; Wang, D.X.; Sauda, E.; Ribarsky, W. VAiRoma: A Visual Analytics System for Making Sense of Places, Times,
and Events in Roman History.IEEE Trans. Vis. Comput. Graph.2016,22, 210–219. [CrossRef]
112.Heimerl, F.; Han, Q.; Koch, S.; Ertl, T. CiteRivers: Visual Analytics of Citation Patterns.IEEE Trans. Vis. Comput. Graph.2016,22,
190–199. [CrossRef] [PubMed]
113.Gad, S.; Javed, W.; Ghani, S.; Elmqvist, N.; Ewing, T.; Hampton, K.N.; Ramakrishnan, N. ThemeDelta: Dynamic Segmentations
over Temporal Topic Models.IEEE Trans. Vis. Comput. Graph.2015,21, 672–685. [CrossRef] [PubMed] J. Pers. Med.2022,12, 135923 of 23
114.Sun, G.; Wu, Y.; Liu, S.; Peng, T.-Q.; Zhu, J.J.H.; Liang, R. EvoRiver: Visual Analysis of Topic Coopetition on Social Media.IEEE
Trans. Vis. Comput. Graph.2014,20, 1753–1762. [CrossRef] [PubMed]
115.
Dou, W.; Wang, X.; Skau, D.; Ribarsky, W.; Zhou, M.X. LeadLine: Interactive visual analysis of data through event
identification and exploration. In Proceedings of the 2012 IEEE Conference on Visual Analytics Science and Technology (VAST),
Seattle, WA, USA, 14–19 October 2012; . 93–102, ISBN 978-1-4673-4753-2.
116.Fulda, J.; Brehmel, M.; Munzner, T. TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured .IEEE
Trans. Vis. Comput. Graph.2016,22, 300–309. [CrossRef]
117.
Janicke, S.; Focht, J.; Scheuermann, G. Interactive Visual Profiling of Musicians.IEEE Trans. Vis. Comput. Graph.2016,22, 200–209.
[CrossRef]
118.
El-Assady, M.; Gold, V.; Acevedo, C.; Collins, C.; Keim, D. ConToVi: Multi-Party Conversation Exploration Topic-Space
Views.Comput. Graph. Forum2016,35, 431–440. [CrossRef]
119.
Hoque, E.; Carenini, G. ConVis: A Visual Analytic System for Exploring Blog Conversations.Comput. Graph. Forum2014,33,
221–230. [CrossRef]
120.
Oesterling, P.; Scheuermann, G.; Teresniak, S.; Heyer, G.; Koch, S.; Ertl, T.; Weber, G.H. Two-stage framework for a topology-
projection and visualization of classified document collections. In Proceedings of the 2010 IEEE Symposium on Visual Analytics
Science and Technology (VAST), Salt Lake City, UT, USA, 25–26 October 2010; . 91–98, ISBN 978-1-4244-9488-0.
121.
Thom, D.; Kruger, R.; Ertl, T. Can Twitter Save Lives? A Broad-Scale Study on Visual Social Media Analytics for Public Safety.
IEEE Trans. Vis. Comput. Graph.2016,22, 1816–1829. [CrossRef]
122.
Mashima, D.; Kobourov, S.G.; Hu, Y. Visualizing Dynamic Data with Maps.IEEE Trans. Vis. Comput. Graph.2012,18, 1424–1437.
[CrossRef] [PubMed]
123.Thom, D.; Bosch, H.; Koch, S.; Worner, M.; Ertl, T. Spatiotemporal anomaly detection through visual analysis of geolo-
cated Twitter messages. In Proceedings of the 2012 IEEE Pacific Visualization Symposium (PacificVis), Songdo, Korea,
28 February–2 March 2012; . 41–48, ISBN 978-1-4673-0866-3.
124. Siddiqui, A.T. Data Visualization: A Study of Tools and Challenges.Asian J. Technol. Manag. Res.2021,11, 18–23. Language Models Enable Simple Systems for Generating
Structured Views of Heterogeneous Data Lakes
Simran Arora
Stanford University
simarora@stanford.edu
Brandon Yang
Stanford University
bcyang@stanford.edu*
Sabri Eyuboglu
Stanford University
eyuboglu@stanford.edu*
Avanika Narayan
Stanford University
avanikan@stanford.edu
Andrew Hojel
Stanford University
ahojel@stanford.edu
Immanuel Trummer
Cornell University
itrummer@cornell.edu
Christopher Ré
Stanford University
chrismre@stanford.edu
ABSTRACT
A long standing goal in the data management community is devel-
oping systems that input documents and output queryable tables
without effort. Given the sheer variety of potential documents,
state-of-the art systems make simplifying assumptions and do-
main specific training. In this work, we ask whether we can main-
tain generality by the in-context learning abilities of large
language models (LLMs). We propose and evaluateEvaporate, a
prototype system powered by LLMs. We identify two strategies
for implementing this system: prompt the LLM to directly extract
values from documents or prompt the LLM to synthesize code that
performs the extraction. Our evaluations show a cost-quality trade-
off between these two approaches. Code synthesis is cheap, but
far less accurate than directly processing each document with the
LLM. To improve quality while maintaining low cost, we propose
an extended implementation,Evaporate-Code+, which achieves
better quality than direct extraction. Our insight is to generate
many candidate functions and ensemble their extractions weak supervision.Evaporate-Code+outperforms the state-of-the
art systems asublinearpass over the documents with the LLM.
This equates to a 110×reduction in the of documents the
LLM needs to process across our 16 -world evaluation settings.
PVLDB Reference Format:
Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew
Hojel, Immanuel Trummer, and Christopher Ré. Language Models Enable
Simple Systems for Generating Structured Views of Heterogeneous Data
Lakes. PVLDB, 17(2): 92 - 105, 2023.
doi:10.14778/3626292.3626294
PVLDB Artifact Availability:
The source code, data, and/or other artifacts have been made available at
https://github.com/HazyResearch/evaporate.
1 INTRODUCTION
Organizations often seek insights trapped in heterogeneous data
lakes (e.g.the web, corporate data lakes, and electronic health
records) [10,26,58]. In their raw form, these data sources cannot
* Equal contribution.
This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 17, No. 2 % ISSN 2150-8097.
doi:10.14778/3626292.3626294
easily support analytical queries. A long standing goal of the data
management community is to develop systems that automatically
convert heterogeneous data lakes into queryable, structured ta-
bles [12,15,50,70, inter alia.]. In this work, we investigate whether
recent large language models can help address this problem.
We study systems that take asinputheterogeneous documents
(e.g.HTML webpages, PDFs, ) andoutputa tabular, structured
view of the documents. These systems must identify the schema
and perform extraction to populate the table.
Example 1.Medical researchers frequently data span-
ning electronic health records (EHR), clinical trials, knowl-
edge sources (e.g. PubMed), and FDA reports to understand
and monitor patients and treatments [8]. Consider the large
collection ofFDA 510(k)reviews for premarket medical
devices, which have been the subject of multiple studies
[68,72]. Our objective is to output a table that automati-
cally structures the attributes that are distributed in the
∼20-pagePDFs, for instance thedevice classification,
predicate device code, andindications for .
Systems designed to tackle this problem must balance a three-
way tradeoff betweencost(data lakes may hold millions of docu-
ments),quality(output tables should be able to accurately support
an analyst’s queries), andgenerality(different data lakes have
different document types and structure). See 2 for a formal
task definition and further discussion of this tradeoff.
Given the range of formats, attributes, and domains across docu-
ments, prior systems rely on simplifying assumptions (e.g. handling
one document format). The majority of works focus on structuring
HTML[12,15,25], assuming the attributes and values are at specific
positions in theHTML-DOM [22,45,46,71]. For unstructured ,
current approaches linguistic tools (e.g., dependency parsers)
to introduce structure [15,25,31,51] and then apply heuristic rules
over the resulting structure to extract information. The documents
in Example 1 highlight the limitations of the prior approaches: they
lack (e.g.HTML) structure and, consistent with recent evaluation
efforts [71], we find the SoTA approaches for unstructured per-
form poorly on long semi-structured PDFs (See [5]). Some systems
assume there is a human-in-the-loop, labeling data and writing
heuristic rules for extraction [57,61], while others assume to annotated training documents from the domain [22,45,46]. Re-
searchers manually annotated the reports in Example 1 [68].
arXiv:2304.09433v3 [cs.CL] 7 Mar 2025 Data lake: A collection of semi-structured documents (e.g. HTML, TXT, XML)
Tables: A structured view of the data in the input documents. 2222
31
def
def
def
Filter: Compare and LLM outputs, filtering out functions that disagree.
Prompt LLM: List all attributes about the player mentioned in this document. Aggregation Synthesis
Schema Identification
Output
InputEVAPORATE-�CODE+
Prompt LLM: Write a to extract draft year from the document. def draft_year(doc): from bs4 import BeautifulSoup soup = BeautifulSoup(doc) ...
draft year
Point Guard
Center
Center
Small Forward
Point Guard
Center
Power Forward
Small Forward
2018
2014
2014
2007
2009
2012
2017
2003
Luka Doncic
Nikola Jokic
Joel Embiid
Kevin Durant
Steph Curry
Anthony Davis
Jayson Tatum
LeBron James
2017
2017
1999
2017
2017
Estimate Quality
name
position
2017
Prompt LLM: Extract draft year directly from the document. position
draft year
name
headline
Filtered Attributes
✔
✔
✔
✘ Candidates
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def draft_year(doc)
Filtered Functions
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def extract_draft_year(doc)
def draft_year(doc)
2017 1: The provides a collection of documents (e.g. NBA player bios) andEvaporateoutputs a table by identifying
attributes and populating columns.Evaporateavoids running expensive LLM inference on all documents by (1) synthesizing
the key attributes from a small sample of documents and (2) synthesizing (e.g. Pythonic) functions that then are reused at scale
to process documents. Because quality is variable,Evaporate(3) applies an algorithm that generates many candidate
functions and ensembles their extractions weak supervision.
In this work, we explore whether we can improve generality
by leveraginglarge language models(LLMs). An LLM is a deep
learning model that is pretrained on broad data and can be adapted
to diverse tasks, from machine translation to data wrangling [14,49].
At inference time, the models take as input a natural language
task description termed aprompt[11,14] and generate a natural
language response. See 2.3 for more background on LLMs.
Evaporate.( 3) We presentEvaporate, a system that
uses LLMs to produce structured views of semi-structured data
lakes. Our evaluation spans 16 -world settings from movie and
university websites toFDA 510(k)reviews [22,32,34,37,45,68,72].
The inputs a collection of documents andEvaporateauto-
matically identifies the schema and performs extraction to populate
the table. Our implementation requiresno customization, training,
or human effortto support the diverse evaluation settings. We pro-
pose two fundamental strategies for implementing this interface,
identifying a tradeoff between their cost and quality:
(1)Evaporate-Direct( 2) The LLM directly extracts
values from documents.
(2)Evaporate-Code( 4) The LLMsynthesizes codethat
is then applied to process documents at scale.
Evaporate-Codeis cheap, but underperformsEvaporate-Direct
by 24.9% (13.8 F1 points) averaged across our evaluation settings. We
thus seek a code synthesis . We presentEvaporate-
Code+, which achieves better quality than direct extraction. Our
insight is to synthesize many code snippets for extraction and
ensemble their outputs weak supervision.
Direct Extraction ( 3.1).Our first implementation,
Evaporate-Direct, applies asingle prompt(included in [5]) to
each document in the input. The prompt instructs the LLM to both
identify the schema and extract values. Remarkably, we find that
in some settings, with a single prompt and no task specific modifi-
cations, performance is already competitive with state-of-the-art
systems that rely on domain specific assumptions and training.
However, this implementation is very expensive. LLMs are op-
timized for interactive, human-in-the-loop applications (e.g.Chat-
GPT) [69], not high-throughput data processing tasks [60]. The of tokens processed by an LLM inEvaporate-Direct
growslinearlywith the size of the data lake. As of March 2023,
applying OpenAI’s models to the 55 million Wikipedia articles
would cost over $110k (gpt-3.5, $0.002/1k tokens) and $1.1M
(-davinci-003, $0.02/1k tokens) dollars [1,52]. There arebil-
lionsof webpages on the broader Internet [35] and the facts change
over time. For instance, NBA players are added to Wikipedia, a
player’steamchanges after trades, and thepoints per gamemet-
ric changes after every game. Data processing is aroutine expense
(repeated by multiple data analysts), not a one-time cost [59].
Code Synthesis ( 3.2).Can we produce the structured
table a sublinear pass of the LLM over the documents?We
proposeEvaporate-Code, which splits the task into two sub-tasks:
(1) identify the table schema and (2) extract values. This view allows
us to exploit the distinctredundanciesof each sub-task that occur
when running LLM inference on every document:
(1)Schema Generation.In order to identify a schema, we only
process a small sample of documents with the LLM. This
succeeds because there is redundancy in the attributes men-
tioned across documents. For e.g., in Example 1, most re-
ports mention apredicate device name.
(2) Synthesis.We prompt the LLM to synthesize (e.g.
Pythonic)functions, that can be applied at scale across the
documents. This works because of redundancy in the for-
matting of -value pairs. For e.g., the FDA 510(k)s the consistent format “Predicate device name: k".
The of tokens processed by the LLM inEvaporate-
Codeisfixedand does not grow with the size of the data lake (as
illustrated in 3), addressing the cost issues ofEvaporate-
Direct. However, the LLM synthesizes variable quality information
extraction functions. The extractions are up to 14 points worse in
Pair F1 score than those produced usingEvaporate-Direct.
2 Code Synthesis + Aggregation.( 3.3) To improve qual-
ity while keeping costs low, we proposeEvaporate-Code+. Study-
ing the synthesized functions, we observe some only work for
a narrow slice of documents, while others exhibit syntactic and
logical errors. To reduce variance, we synthesize many candidate
functions, then estimate their quality and aggregate their extrac-
tions usingweak supervision. This builds on our work [4], which
broadly applies weak supervision to prompting for the first time.
Weak supervision (WS) is a statistical framework for modeling
and combining noisy sources with varied coverages without any
labeled data [57,66]. However, WS is typically applied overhuman-
generatedfunctions while our setting consists ofmachine-generated
functions. This presents issues when attempting to apply existing
WS tools. (1) WS theoretically assumes all noisy sources are better
than random performance (50% accuracy), yet 40% of our generated
functions arebelow 25%( 3.2). (2) WS attempts to deploy
functions that achieve high quality on narrow slices of data (high
precision), and allow the toabstainon data external to the
slice (low recall). While humans can express when functions should
abstain, the machine-generated functions do not contain this logic.
To handle theopenWS setting, we introduce a algorithm for
ensembling the functions (Algorithm 1).
We summarize our overall contributions as follows.
(1)Our system offers capabilities for the long-studied
structured view generation problem.Existing systems
require in-domain training and handle limited document
formats (e.g. HTML [16,22,24,45]).Evaporaterequires
no training and succeeds on different document formats
(HTML, PDF, TXT) off-the-shelf. ( 4).
(2)We study a tradeoff space between direct extrac-
tion and code synthesis for data tasks.Evaporate
asymptoticallyreduces the of tokens that the LLM
needs to process to generate the outputs. At 10k documents
per evaluation setting, this amounts to a 110x cost reduc-
tion. Further, prior works LLMs for data tasks require
users to manually write prompts [49].Evaporateis built
with task-agnostic prompts that generalize across settings.
(3)We present an algorithm and theoretical analysis for
applying weak supervision to open-ended functions
and extraction tasks.AlthoughEvaporate-Codeis more
efficient,Evaporate-Directachieves significantly higher
quality. our algorithm,Evaporate-Code+outper-
formsEvaporate-Direct, which directly processes every
document, by 10.1 F1 points (18%) (Table 3).
(4)We extensively validate the system on 16 data set-
tings from 5 domains and 3 data formats, and across
4 LLMs.(1)Evaporateoutperforms the SoTAlearned
baseline systems by 3.2 F1 points (6%) when generating
tables (both schema generation and extraction) end-to-
end, and 6.7 F1 points (10%) on the extraction step. (2)
Evaporate-Code+achieves a 10.1 F1 point increase over
Evaporate-Direct, -davinci-003. (3) Across four
unique LLMs we show the relative quality ofEvaporate-
Directvs.Evaporate-Code+remains consistent.
We define the problem in 2. We presentEvaporatein 3, evaluations in 4, and related works in 5.
2 PRELIMINARIES
We first define the problem setting and system desiderata.
2.1 Problem Setting
We study the problem of constructing a structured view (i.e.data-
base table) of a set of semi-structured documents (e.g.HTML, PDF,
TXT). Formally, we define the problem as follows:
•
Input: provides a set of푛semi-structured documents
퐷={푑
1
,푑
2
, ...푑
푛
}(e.g.A collection of FDA 510(k) reviews
for premarket notification submission for medical devices).
•Output:System outputs a table defined by a set of names퐴={푎
1
,푎
2
, ...푎
푚
}(e.g.푎
1
=indications for ,
푎
2
=classification) and a set of푛extracted records for
푅={푟
1
,푟
2
, ...푟
푛
}, one per document, where푟
푖
is an푚-tuple
(e.g.푟
1
=(“fracture", “x-ray")).
Unlike prior work which proposes systems that rely on manual
labeling [61] or manual prompt tuning [49,63], we aim to develop
automatedsolutions, which require no effort.
Measuring System QualityWe compare the generated table(퐴,푅)
to a manually curated “ground-truth" table(
ˆ
퐴,
ˆ
푅). The coverage
of an refers to the fraction of documents that include the and its value. Following prior work, we prioritize attributes
with highcoverage, which tend to be useful for analysis [16,18]. We
measure agreement between the tables Pair F1. For additional
details on our evaluation setup, see 4.3.
2.2 System Desiderata
Current systems for producing structured views are limited in their
generality, cost/flexibility, and quality/usability [16,18,51,71]. Here
we review the existing systems.
Generality.The ideal system will generalize across document
formats and domains, without manually engineered rules or task-
specific training.This is important because the input documents
퐷could focus on any imaginable topic or any file format [71].
Existing systems featurize documents by tagging the named entities
(NER), dependency parse tree, and part-of-speech (POS), and train
a model to predict whether a span of is a useful fact [39].
Unfortunately, the performance of the parse, NER, and POS tags
drastically degrade on semi-structured data (e.g.HTMLelements)
and longer sequences of (i.e. full documents) [71]. We provide
detailed error analysis [5]. A specialized class of systems focuses on
processing semi-structured webHTMLdocuments by leveraging
the HTML DOM tree as features [13,16,22,25,46, inter alia.].
However, the systems thus do not support other document formats.
Cost.The ideal system will enable users to manage a cost-coverage
tradeoff, rather than requiring them to extract “all-or-nothing”.The
existing systems are built to extractallpossible facts in the doc-
uments, without prioritizing important attributes or allowing the to influence what is extracted [21,71]. Processing every line
of every document can be expensive. To mitigate this, the can
define the attributes of interest then apply a closed IE system for
extraction, however this requires upfront human effort.Desider-
ata: The ideal system will enable users to manage a cost-coverage
tradeoff, rather than requiring them extract “all-or-nothing”.
3 Quality.The ideal system will output a table(퐴,푅)with full
columns (i.e. high-coverage attributes) and accurate, consistently for-
matted extractions.Existing OpenIE systems commonly extract
tuples in unnormalized forms directly from documents [21]. This
can make the resulting extractions difficult to for analysis, re-
quiring advanced systems or -defined post-processing code for
resolving subject, objects, and predicates to a canonical form [15].
2.3 Background on Large Language Models
In this , we provide background onlarge language models
(LLMs), which are central to our work.
Definition 1 (Large Language Model).A machine learning
model,F, trained on a self-supervised task (e.g. next word prediction)
over a massive corpus of [29]. Language models can be to
generate on provided context. For example:
F(All that glitters) →is not gold.
Numerous studies have demonstrated LLMs capable of solving tasks without updating any model parameters, a phenomenon
termedin-context learning[2,14,49]. Specifically, these studies
show that when passed an appropriate description of the task, the
model often generates completing the task.
Definition 2 (Prompt).A natural language task-specification to elicit a particular generation from an LLM. Prompts often
include demonstrations of the task. For example, the prompt below
elicits the translation of the word cheese into French:
F(Translate. Eng: hello, Fr: bonjour; Eng: cheese, Fr:
| {z }
Prompt
) →fromage
| {z }
Generation
Examples of prompts in this work are provided in Figures 2
and 4. All prompts in the system are provided in [5].
3EVAPORATE: A PROTOTYPE SYSTEM
POWERED BY LANGUAGE MODELS
We introduceEvaporate, a prototype system that uses LLMs to
materialize a structured view of a heterogeneous, semi-structured
data lake. Compared to prior systems, which rely on manual labeling
[61] or tuning prompts to a domain [49],Evaporateexposes a
remarkablygeneralinterface: the inputs documents and the
system automatically outputs a structured view of those documents,
without any domain specific training or prompt customization.
Overview.We instantiate theEvaporateinterface with three
different implementations. We can feed every document to the LLM
and prompt it to extract values directly (direct extraction, 2),
or feed a small sample of documents to the LLM and prompt it
to writecodeto do the extraction (code extraction, 4). In 3.1 and 3.2, we describe baseline implementations
of these two strategies,Evaporate-DirectandEvaporate-Code.
We find that these two implementations tradeoff cost and quality.
Then, in 3.3, we propose a code extraction implementation
that uses weak supervision to improve quality and retain low cost.
Prompt ManagementEvaporateapplies a set of task-agnostic
prompts, which are all provided verbatim in the technical report
[5]. These tasks are not modified for different tasks. Within the
system, the prompts are Python f-strings, with placeholders for
Sample : Patient birth date: 1990-01-01 Prescribed medication: aspirin, acetaminophen Prescribed dosage: 1 tablet, 2 tablets, 3 tablets Doctor's name: Dr. Burns Date of discharge: 2020-01-01 Hospital address: 123 Main Street, York, NY Question: List all relevant attributes about that are mentioned in this sample if any. Answer: - Prescribed medication: aspirin, acetaminophen - Prescribed dosage: 1 tablet, 2 tablets, 3 tablets ---- Sample : <document> Question: List all relevant attributes that are mentioned in this sample if any. Answer: Integrated Prompt
task demonstrations
placeholder 2: Prompt forEvaporate-Directstructured. The
prompt template, which includes placeholders for in-context
examples and and the inference example (i.e., data lake doc-
uments), is applied to each document in the data lake.
inputs chunks of from the particular dataset being processed.
The LLM is prompted with the formatted strings. We a caching
tool that we helped develop calledManifest[53] to store input
and completion pairs from the LLM prompting in a local SQLite
database, where keys are the prompt-inputs and values are the
completions . Therefore, if users repeatedly run the system on the
same dataset, they do not incur the LLM inference costs again.
3.1Evaporate-Direct
In this , we describe a simpledirect extractionimplemen-
tation,Evaporate-Directthat applies a single prompt template
to every document. This prompt template, which is included in 2, instructs the LLM to both identify the schema and extract
values (see [5] for the full prompt). It consists of a few in-context
examples that are general,i.e.are not customized to a particular
format, domain, or document.
Below we discuss how we (1) manage long documents that can-
not fit in the LLM’s context window, (2) process the LLM’s textual
outputs, (3) prioritize the most useful attributes according to prin-
ciples described in prior work [16].
Managing long documents.The input toEvaporateis a file path
to raw documents, which can be several pages long. For instance
the Medical FDA reports in Example 1 are∼20 pages long. How-
ever, the underlying Transformer architecture of modern LLMs is
limited to processing a fixed of tokens (e.g.a few thousand
tokens), referred to as thecontext window, during each inference
call.Evaporatetherefore splits the raw documents such that each
piece is within the context window. Each chunk is inserted into the
prompt in turn as shown in 2.
4 Processing outputs.Language models output open ended so the last step is to convert this to a usable table. To facilitate
this data transformation, we can specify formats in our prompt
demonstrations to encourage the LLM to organize the output in
a similar structure. For instance, the demonstration in 2
specifies a list format with<>: <value(s)>per entry.
Evaporateoutputs in this format can be de-serialize into a table.
Prioritizing common attributes.The list of extracted attributes
and values can contain the niche attributes for specific documents,
whereas a common database design principle is to capture the high
frequency attributes [15]. ThereforeEvaporatetakes the union of
attributes outputted across documents and ranks by frequency to
enable prioritizing head attributes.
Analysis.We analyze thisdirect extractionimplementation,
Evaporate-Direct, along the axes of our three desiderata. Results
processing the documents withEvaporate-Directare reported in
Table 3 and are discussed in detail in 4.
Overall, the quality matches or exceeds the baseline systems
(described in 4), on 8 of the 16 settings. This is surprising
given the simplicity — i.e.Evaporate-Directusesonefixed prompt
to processall 16 settings. However, fundamental cost limitations
impede the -world deployment of this .
However, the high cost of this implementation limits its applica-
bility to large, recurring workloads. The of tokens processed
by the LLM scales linearly with the size of the data lake,O(푛). Data
lakes can containbillionsof documents [35,50]. Further, in most
organizations, data processing is not a one time cost. Data lakes
are dynamically changing, soEvaporate-Directwould need to
berepeatedlyapplied.
3.2Evaporate-Code
In this , we presentEvaporate-Code, which significantly
reduces cost compared toEvaporate-Direct. Here, we perform
schema identification separately from value extraction, which -
lows us to exploit fundamental differences between the sub-tasks
to reduce cost. In schema identification, we find that we only need
to process a small sample of documents because attributes are con-
sistent across documents. On the other hand, in order to extract
values, we must process every document. However, the ways in
which values appear across documents (i.e.their relative positions
in the document) tend to be consistent, meaning the extraction
logic is consistent across documents.
The two steps of the decomposed implementation are:
(1)Schema synthesis.( 3.2.1) We observe that the at-
tribute outputs contain relatively consistent<attributes>,
even though thevaluesdiffer from document to document.
To exploit this redundancy,Evaporate-Directprompts
an LLM to analyze a small sample of documents to identify
attributes for the output schema For example, given a sam-
ple of the Medical Device FDA Reports, the LLM outputs a
devices table with attributes "510(k) ".
(2) synthesis( 3.2.2). We observe consis-
tencies in how attributes are embedded across documents.
E.g., the510(k) codein the FDA documents always starts
with the letter “k” and theplayer positionattribute is
always in theHTML“infobox” element in NBA player Wiki
0246
Log Documents
4
6
8
10
Log Toks. Processed
Direct
Code
1234
Log Attributes
6
7
8
Log Toks. Processed
Direct
Code 3: Tradeoffs between processing the documents via
direct prompting (Direct) versus code synthesis (Code). For
small data lakes and large numbers of attributes, Direct is
sufficient. As the of documents grows, Code is orders-
of-magnitude more efficient. Left is evaluated at 10 attributes,
Right at 10K documents, assuming 10K tokens per document.
pages. A researcher would likely exploit such redundancies
when manually scraping the documents for analysis. In
Evaporate-Code, we propose to the LLM to automati-
cally synthesize a data-lake-specific suite offunctions, that
can then be applied at scale to process many documents.
Next, we provide details for each sub-task.
3.2.1Schema Synthesis .Evaporatefirst uses an LLM to identify
attributes퐴={푎
1
,푎
2
, ...푎
푚
}for the output schema.
Generating candidate attributesConcretely, we sample a set ̃
퐷
of
푘<<푛documents from퐷. For each, we prompt the LLM to extract
the most useful attributes from the document as inEvaporate-
Direct. Recall this yields a set of attributes ranked by how fre-
quently they were extracted across documents. We retain attributes
that are explicitly mentioned in the document to ensure provenance
in schema identification.
Re-ranking candidate attributesBecauseEvaporatenow identi-
fies the attributes from a small set of documents, we observe that
Evaporate’s ranking is noisier than when every document was
processed inEvaporate-Direct, i.e. an important may be
selected by the LLM a few times amongst the푘documents. Thus, we
show the LLM a union of extracted attributes and prompt it to iden-
tify the most useful attributes (prompt in [5]). The frequency-
rank is upweighted if the is in the LLM output.
3.2.2 Synthesis.Given the attributes퐴={푎
1
,푎
2
...푎
푚
},
the objective ofEvaporate-Code’s second phase is to extract the
values of the attributes for each document푑
푖
∈퐷. Our key insight,
as discussed, is that -values are expressed in similar ways
from document to document. To exploit this, instead of processing
every document with the LLM to extract values for 푎
푖
, we
propose to the LLM togenerate codethat can then be reused to
process many documents. 4 shows anEvaporatefunction synthesis prompt. The
in-context examples show pairs of snippets and functions to
extract an of interest.Evaporatesearches the data lake
via a simple keyword search for document portions that mention푎
푖
,
and includes this in the prompt.Evaporatesynthesizes functions
for attributes following the rank-order of attributes derived during
schema synthesis. This means that values of the most relevant (and
5 Here is a file sample: DESCRIPTION: This post answers the question, "How do I sort a dictionary?" DATES MODIFIED: The file was modified on the following dates: 2009-03-05T00:49:05 2019-04-07T00:22:14 2011-11-20T04:21:49 USERS: The users who modified the file are: Jeff Jacobs ... Question: Write a python called "get_dates_modi-
fied_field" to extract the "DATES MODIFIED" field from the . Include any imports. import re def get_dates_modified_field(: str): parts= .split("USERS")[0].split("DATES MODIFIED")[-1] pattern = r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}’ return re.findall(pattern, ) ---- Here is a file sample: <title>U.S. GDP Rose 2.9% in the Fourth Quarter </title> <meta itemProp="datePublished" content="2023-01-26T10:30:00Z"/> ... Question: Write a python called "get_date_pub-
lished_field" to extract the "datePublished" field from the . Include any imports. from bs4 import BeautifulSoup def get_date_published_field(: str): soup = BeautifulSoup(, parser="html.parser") date_published_field = soup.find('meta', itemprop="-
datePublished") date_published_field = date_published_field['content’] return date_published_field ---- Here is a file sample: <document> Question: Write a python called "get_<>_-
field" to extract the <> from the . Include any imports. Generation Prompt
task demonstrations
placeholder
import re def get_dates_modified_field(: str): parts= .split("USERS")[0].split( "DATES MODIFIED" )[-1] pat = r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}’ return re.findall(pat, ) from bs4 import BeautifulSoup def get_date_published_field(: str): soup = BeautifulSoup( , parser="html.parser" ) date_published_field = soup.find( 'meta', itemprop="datePublished" ) return date_published_field['content’] draft year 4: A representative prompt for synthesis,
containing two data lake agnostic in-context examples.
frequent) attributes as determined byEvaporateare extracted first.
The can stop the synthesis when desired.
Analysis.We briefly analyze theEvaporate-Codeimplementa-
tion along the axes of our three desiderata. Results processing the
documents withEvaporate-Directare reported in Table 3 and
are discussed in detail in 4.
Cost. 3 demonstrates the asymptotic differences in cost
betweenEvaporate-DirectandEvaporate-Code.Evaporate-
Codeis asymptotically more efficient as a of the of documents: the of LLM calls required with gen-
eration is proportional to the of attributes, not the of documents. The crossover point is at∼40 documents. Meanwhile,
Evaporate-Directhas the potential to extract multiple attributes
from the in-context document per inference call, whileEvaporate-
Coderequires generating functions for each . Thus,
the cost ofEvaporate-Codegrows with the of attributes,
while the cost ofEvaporate-Directapproach is constant. The
crossover point is at∼2,500 attributes ( 3).
Quality.The tables generated byEvaporate-Codeare on aver-
age 21.9 pair F1 points worse than those produced usingEvaporate-
Directon the SWDE datasets (Table 2). This suggests that there
is a cost-quality tradeoff between the two implementations, since
Evaporate-Codeis much cheaper.
3.3Evaporate-Code+
In this we discuss an extension ofEvaporate-Code, which
enables significant quality improvements while keeping costs low.
This implementation, which we callEvaporate-Code+, synthesizes
manycandidate functions and ensembles their extractions weak supervision. We decompose the task into three parts:
(1)Schema identification.( 3.2.1) Same as inEvaporate-
Code.
(2) synthesis.( 3.3.1) Same as inEvaporate-
Code, except instead of generating a single per
, we generate manycandidate functions. Below we
describe techniques to encourage diversity among candi-
dates.
(3) Aggregation.( 3.3.2) The synthesized
candidate functions have varying qualities and coverages,
making them unreliable. We then introduce a weak supervi-
sion (WS) algorithm to aggregate over their different
predictions for the values across documents.
3.3.1 Synthesizing Diverse Candidate Functions.We find that the
quality of LLM-generated functions varies significantly depend-
ing on the document chunk and in-context examples in the
prompts. To address the variability in quality, we adopt the
strategy we previously proposed in Arora . [4]. This strategy
curates multiple diverse prompt templates for the same task (i.e.
multiple generation prompts in the style of 4) and
prompts the LLM with each in turn to produce a diverse set of candidates퐹={푓
1
, 푓
2
, ...푓
푘
}.
Evaporatepermits the of multiple generation
prompts. We 푃
퐴
and푃
퐵
(included in [5]) in this work.푃
퐴
has
zero in-context examples and a task description that encourages
the LLM to regex.푃
퐵
has two in-context examples and a task
description that encourages the LLM to import and any Python
library. We find that neither consistently outperforms the other.푃
퐴
produces higher quality functions on69%,45%,60%,91%, and31%of
attributes on the 8 SWDE Movie, 5 SWDE University, FDA reports,
Enron, and Wikipedia player pages settings respectively. Design-
ing a single “perfect” prompt can be challenging soEvaporate
aggregates results from multiple prompts.
3.3.2 Aggregating Candidate Functions.Next, we discuss how to
combine the aggregations of the candidate functions.
6 Background: Methods for Unsupervised AggregationBecause we
lack ground truth labels in our setting, it is not possible to directly
evaluate the quality of the candidate functions. A popular unsu-
pervised aggregation strategy is to take the Majority Vote (MV)
across outputs [67]. Formally, MV treats the functions as
independent of one another and assigns equal weight to all func-
tion outputs. However, the functions are not of equal quality —
over 40% of synthesized functions result in less than 25 F1 in
extraction quality. Therefore,Evaporateuses weak supervision
(WS), a popular standard statistical framework for modeling the
accuracies and correlations between noisy sources of information
without any labeled data [27,57]. In WS, we learn alabel modelthat
is parametrized by the accuracies and correlations of the candidate
functions. WS is widely in industry [57].
Unfortunately, existing WS setups make the following assump-
tions that do not apply in our setting. The standard setup assumes
human-designedfunctions while our setting usesmachine-generated
functions that output non-standardized extracted .
(1)
Assumption 1: Functions will abstain on examples
where they do not apply [27,57].The value re-
turned by a for a document could benullfor two
reasons: (1) the does not exist in the document (e.g.
a Wikipedia page may be missing a college since
the players did not attended college) or (2) the exists but the was not sophisticated enough to
extract it (e.g. theproduct codeattribute could start with
a lowercase “k” of uppercase “K” in FDA reports, but the
particular is only designed to extract for lower-
case, resulting in empty strings for uppercase documents).
Note that in (1), if the outputs a value, it has low
precision and we would want to ignore the . Note
that in (2), the has high precision and ideally our
system learns to utilize such functionsselectively. Unfortu-
nately, it is challenging to determine whether the outputsnullfor reason (1) or (2) in our setting, whereas in
the traditional WS setup with human-provided functions,
humans specify this logic directly (e.g. “If the email has a
URL, “vote” that it contains spam, otherwise abstain” [62]).
(2)Assumption 2: Functions are correlated with the gold
label푦at better than random performance [27,57].
While this is reasonable when functions are human-designed,
Evaporateuses machine-generated functions. We find 51%
of generated functions are below 50 F1.
(3)Assumption 3: Weak supervision is typically applied
to tasks with well defined classes in a classification set-
ting [27,57].In our case, the output of the functions are ex-
tracted , and thus there is a virtually unconstrained out-
put space of possible extractions that vary from document
to document (e.g. NBA players have varieddate of birth
values). Thenumberof unique extractions collected by the
functions can also differ across documents.
We propose the following to be able to leverage WS.
Let퐷
푒푣푎푙
be a small sample of documents from the data lakeD.
We have the set of generated functions퐹and LLMF.
Handling abstentions.To estimate the probability
that an empty output from a is an abstension, we propose
Algorithm 1: Aggregation (fromEvaporate-
Code+)
1:Input:DocumentsD, candidate functions퐹, LLMF.
Output:Predicted extractions
ˆ
푦
푖
, ...,
ˆ
푦
푛
for documents.
2:Collect sample predictionsSampleD
푒푣푎푙
⊂ Dand apply
the functions푓
푗
∈퐹and LLMFto obtain
ˆ
푦
푖푗
and
ˆ
푦
푖F
for
document푑
푖
.
3:
Handle abstensions: For empty
ˆ
푦
푖푗
, we need to determine if
they representfunction abstensionsorpredictionsthat푑
푖
has
no-value for the . UseFto decide between cases:
compute푒as the fraction of푑
푖
∈ D
푒푣푎푙
with non-empty
ˆ
푦
푖F
4:Score functions: Compute a score
ˆ
푎
푗
for푓
푗 metric
푚(·) on푒.
if푒>휏then
ˆ
푎
푗
=
Í
푖=푛
푖=1
푚(
ˆ
푦
푖F
,
ˆ
푦
푖푗
)|
ˆ
푦
푖F
≠∅
else
ˆ
푎
푗
=
Í
푖=푛
푖=1
푚(
ˆ
푦
푖F
,
ˆ
푦
푖푗
)
5:
Filter low quality functionsRemove푓
푗
∈퐹with
ˆ
푎
푗
≤0.5to
create퐹
′
.
6:Collect votesApply푓∈퐹
′
to all푑
푖
∈ Dto collect “votes” for
the -value in푑
푖
. Post process empty votes as
abstensionsor no attributepredictionsdepending on푒.
7:AggregationUse weak supervision to obtain the final
prediction
ˆ
푦
푖
given the votes{
ˆ
푦
푖푗
|푓
푗
∈퐹
′
}.
to measure the fraction푒of the퐷
푒푣푎푙
documents for whichF
extracts a value. Intuitively, when푒is high, our prior should be
that the appears in a large fraction of documents, so we
should assume functions areabstainingwhen they output empty
values. When푒is low, the appears in few documents, so
we should assume the functions arepredictingempty values. We
can 푒to guide both our evaluation and downstream
aggregation. Note that it is possible forFto abstain or hallucinate
values, affecting the estimate of푒.
Handling functions with worse than random quality.We
propose to utilize the extractions froFon a small set of documents
퐷
푒푣푎푙
(e.g. we |퐷
푒푣푎푙
| ≤10) as an estimate of the ground truth
extractions for those documents. We can then estimate the quality,
ˆ
푎
푗
, of 푓
푗
by comparing its outputs against the outputs ofF
on document푑
푖
∈ D
푒푣푎푙
. If we are in the low푒regime, we should
evaluate the outputs on all푑∈ D
푒푣푎푙
. In the high푒regime, we
should evaluate the outputs on only the푑∈ D
푒푣푎푙
for which푓
푗
extracted a value. We finally filter푓
푗
if
ˆ
푎
푗
≤0.5, where0.5derives
from the typical WS assumptions [27, 57, 65].
Note thatFis an LLM with its own error rate푒, affecting the
estimate
ˆ
푎
푗
. We theoretically study the impact of푒on the label
model learned via WS. We provide the proof in [5].
Proposition 1: We have푚functions with empirical accuracies
ˆ
푎,
evaluated against noisy labels with error rate푒, the accu-
racies estimated by the weak supervision label model are ̃
푎
, and the
measured error is below some threshold휖. Then, if each labels
a minimum of
푛≥
1
2(훾−휖−푒)
log(
2푚
훿
)
7 datapoints, the weak supervision label model will succeed in learning
accuracies such that||푎
∗
− ̃
푎||
∞
<훾with a probability1−훿.
Handling unconstrained output spaces.The푘generated
functions can produce[0..푘]unique prediction votes for a single
unlabeled document푑
푖
, and the of unique votes can differ
from document푑
푖
to푑
푗
. Therefore, for each푑
푖
∈ D, we bucket
the unique votes and take the푏buckets representing the most
frequently occurring votes. The votes for functions that outputted
values outside the top-푏are marked as abstensions. If the of
unique votes is<푏, placeholder values are inserted into the top-푏.
Finally, as the “classes” differ across documents, we introduce a con-
straint to the objective encouraging the class-conditional
accuracies to be equal.
After addressing these assumptions, we can leverage prior ap-
proaches to aggregate the noisy extractions from the can-
didates into higher-quality extractions as in [4,57]. Under WS, the
output of each is viewed as a “vote” for the true label and
the objective is to construct a latent graphical model to account
for the varied accuracies and correlations amongst the functions,
without to any labeled data. Our aggregation method is
summarized in Algorithm 1.
Analysis.We briefly analyze theEvaporate-Code+implemen-
tation along the axes of our three desiderata. Results processing
the documents withEvaporate-Code+are reported in Table 3 and
are discussed in detail in 4.
Cost.As withEvaporate-Code, the of tokens processed
by the LLM inEvaporate-Code+is fixed with respect to the num-
ber of documents. 3 demonstrates the asymptotic differences
in cost betweenEvaporate-DirectandEvaporate-Code. The of tokens that must be processed by the LLM grows only
by a constant factor: the of candidates generated.
The can set this to balance cost and quality.
Quality.Of the three implementations,Evaporate-Code+pro-
duces the highest quality tables.Evaporate-Code+outperforms
Evaporate-Directby 12.1 F1 points (22%) on average, while us-
ing far fewer computational resources. aggregation
leads to an improvement of 25.1 F1 points overEvaporate-Code.
4 EVALUATIONS
We now evaluateEvaporate, validating the following claims:
• synthesis enables asymptotic cost reduc-
tions for processing data with LLMs.There has been
significant recent interest in developing various data man-
agement applications with LLMs [17,36,40,49]. Prior work
directly processes data with the LLM.Evaporate-Code+
reduces the of tokens the LLM needs to process by
110x relative toEvaporate-Direct.
• synthesis + aggregation results in higher
quality than direct extraction.Despite the fact that
Evaporate-Directprocesses each document with the LLM
directly,Evaporate-Code+performs 10.1 F1 points (18%)
better on average. on comparisons withEvaporate-
Code, which only synthesizes one , we show that aggregation is key in enabling the improvements.
•Evaporateachieves higher quality than state-of-the-
art baselines, while exposing a more general interface.
Evaporate-Code+expresses tasks via merelysixnatural
language prompts (all provided in [5]) and uses no training.
Yet, it exceeds SoTA systems by 3.2 F1 (6%) points when
generating tables from scratch and 6.7 points (10%) when ex-
tracting pre-defined gold attributes. Meanwhile, it supports
a broader range of settings than any of these baselines.
•
The identified tradeoffs hold across language models.
We evaluate on four models from three unique providers
[6,43,52]. We findEvaporate-DirectandEvaporate-
Code+remain competitive in quality across LLMs.
4.1 Experimental Setup
We primarily evaluateEvaporateon the end-to-end task ofstruc-
tured view generation. For the purpose of comparison to prior work,
we also evaluate on the sub-task ofclosed information extraction.
We first define these tasks, their metrics, and the baselines. We then
provide implementation details forEvaporate.
Structured view generation task.This captures the end-to-
end task of identifying the schema and populating the output table.
This task is often discussed as a vision system [16], and given
the difficulty of this task, there are limited comparable works. We
therefore compare to the closest line of work, OpenIE systems,
where the task is to extract all facts from documents [7,51]. We
compare to two sets of baselines: (1) Deng . [22], Lockard . [45,46]forHTML-specific OpenIE, and (2) Kolluru .
[39]for generic unstructured . The former models explicitly theHTML-DOM tree structure to process the page, assuming values are leaf nodes, and explicitly train on documents
from the domain of interest. The latter class of systems first label
sentences linguistic tools (i.e.dependency parsers, part of
speech taggers, and named entity taggers), and fine tune LLMs over
these features to perform the task [71].
Metrics.The standard metric is Pair F1 [22,45], an F1 score
applied to the predicted vs. gold sets of tuples of the form (document
ID푑
푖
, 푎
푗
, value푟
푖
, 푗). The tuple must exactly match a tuple
in the ground truth to be marked correct. SinceEvaporateranks the
attributes and generates functions in this order, for fair comparison,
we report OpenIE scores for all tuples up to푘attributes, where푘
is the of gold attributes for the setting. We note that the
prior systems extract all-or-no tuples, in contrast.
Closed information extraction task.This captures the set-
ting where the provides a pre-defined schema andEvaporate
is to populate the table. We compare to state-of-the-art ap-
proaches for closed IE including: (1) Deng . [22], Lockard .
[45,46]forHTML-specific ClosedIE and (2) Clark . [19], He .
[33]for generic unstructured . The former models explicitly theHTML-DOM tree structure to process the page, assuming values are leaf nodes, and explicitly train on documents
from the test domain. The latter are pretrained LLMs that have been
fine tuned on massive amounts of labeled(, value)
pairs [56]. We report ClosedIE results the F1 metric on a
value-by-value basis across each document.
EvaporateImplementation Details.In the following experi-
ments, we instantiateEvaporatewith currently popular, LLM APIs.
Experiments in Sections 4.3 and 4.4.1 usetext-davinci-003from
OpenAI. In 4.4.2, we evaluate additional LLMs from three
8 model providers. For experiments, we 10 sample documents
per data lake for the schema synthesis, synthesis, and verification. We apply Algorithm 1 over the top-10 scoring
functions that are synthesized for each and data lake. The
prompts remain constant across data lakes and models. In [5], we
provide ablations that show how the system’s quality changes as
we vary the of sample documents and top-푘functions.
When the measuring cost for alternate implementations ofEvap-
orate, we compute total of tokens processed by the LLM
to perform the end-to-end task (i.e.the sum of the of tokens
in the prompt and model generation). We this metric because
the wall-clock time and dollar cost of a model fluctuate, but both
should be proportional to the of tokens processed.
4.2 Evaluation Settings
We evaluateEvaporateon 16 settings representing a range of
-world data lakes. First, we a benchmark suite of 13 Movie
and University websites to compareEvaporateto state-of-the-
art information extraction systems [22,32,45]. Next, to evaluate
on more unstructured data (i.e.non-HTML) we turn to:Enron
a corporate email corpus that has been analyzed in over three
thousand academic papers [3,34,37],FDA 510(k)reviews for
premarket notification submissions for medical devices, which have
been the subject of multiple important research studies [68,72],
andNBAWikipedia pages for NBA players, which include more
complexHTMLthan the existing benchmarks [22]. We release the
benchmarks and provide additional details in [5]. Here we briefly
describe the properties we aim to study with each setting:
(1)Benchmark Suite: SWDE Movies & UniversitiesSWDE
is the standard benchmark for document-level IE in prior
work [22,32,45,46, inter alia.]. There are 8 sets of web-
pages for Movies (e.g. IMDB) and 5 sets of webpages for
Universities (e.g. US News). For each website, the bench-
mark contains 1063-2000 pages and annotations for 8-274
attributes. We SWDE to compare to the state-of-the-art
and test on a range of types, e.g. simpler Movie
“runtime” through complex Movie “cast” and popular Movie
“director” through infrequent “second assistant director”.
(2)ComplexHTML: NBAAs SWDE attributes always oc-
cur in separate leaf nodes of theHTML-DOM tree, we NBA Player Wikipedia pages to evaluate on more complex
HTML. E.g., theNBAdraft contains the draft round,
year, pick , and team by which the player was se-
lected. We evaluate on 100 randomly selected player pages
(spanning the 1940s-present) and 19 annotations.
(3)Unstructured : Enron and FDAWe observe a lack
of existing benchmarks for document-level IE over unstruc-
tured — intuitively, this setting has been challenging
with prior generations of models due to the lack ofany
grounding structure whatsoever (i.e.recall current systems
rely onHTML-DOM elements or sentence-level NER, de-
pendency, and POS tags). We turn to the Enron and FDA
settings described above. The Enron setting contains 15
gold attributes and 500k documents. The FDA setting con-
tains 16 gold attributes and 100 PDF documents, which are
up to 20 pages long, randomly sampled from FDA 510(k). 5: T-SNE Visualization of documents in the SWDE
dataset. T-SNE is performed on the first 16 principal compo-
nents of TF-IDF vectors. Colors indicate the source website.
Dataset ProtocolsBecause the baselines we compare against
on SWDE require training data, they perform website-wise cross
validation (i.e.train on some websites and evaluate on others). They
do this for several combinations such that every website appears
in the evaluation set.Evaporate, in contrast, does not require any
training data. We simply evaluateEvaporateon all websites so
that our method is evaluated on the same examples as the baselines.
We process each dataset withEvaporateseparately, to match
the protocol by the baselines [22]. However, we may not have to the sources of the documents in a -world data lake –
they may be mixed together. a standard TF-IDF vectorizer
and K-means clustering to the mixture of documents, we verify we
can perfectly recover the document sources without any labeled
data or supervision ( 5, details in [5]). Intuitively, clustering
semi-structured data may be simple due to the rich formatting.
4.3 ComparingEvaporateto Baselines
First we validate thatEvaporateoutperforms baselines, defined
in 4.1, both in generality (i.e. the flexibility to support data
from different domains and formats) and quality. We then compare
the efficiency ofEvaporate-Code+vs. the baselines.
4.3.1 Quality and generality comparisons.
Systems for semi-structured .Shown in Table 2,Evaporate
outperforms the state-of-the-art on SWDE. We compare to the met-
rics reported the baseline works. RecallEvaporateuses no training
whatsoever and can be applied across document formats (HTML,
PDF,TXT). In contrast, the baselines are limited toHTMLand ex-
plicitly perform supervised learning labels from webpages
within the Movie and University domains respectively [22,46]. E.g.,
Deng . [22]assumes values are the leaf-nodes of the
HTML-DOM tree and thus does not work on non-HTML.
The baseline systems restrict scope to attributes that are specifi-
cally mentioned in theHTML<body>, even though attributes
are frequently mentioned in theHTMLheader (e.g. within<title>
elements) and tags (e.g.<a href='year/2012'>). We validateEvap-
oratecan identify and extract attributes mentioned anywhere in
the document. We extend the SWDE benchmark to include the
attributes scattered throughout the fullHTMLand findEvaporate
achieves 52.2 and 49.0 on Movies and University respectively on
the more challenging setting. We release the annotations.
9 Table 1: Quality ofEvaporate-Code+evaluated on ClosedIE
in F1 and OpenIE in Pair F1 -davinci-003.
Source (Format)
ClosedIEOpenIE
F1RPF1
FDA (TXT)80.162.068.164.9
Enron Emails (TXT)93.380.394.686.9
Wiki NBA (HTML)84.7
55.788.268.2
SWDE Movie (HTML)79.548.571.056.8
SWDE University (HTML)73.750.971.459.0
Average82.358.9 78.5 66.7
Table 2: Comparisons to state-of-the-art on ClosedIE in -
F1 and OpenIE in Pair F1. The baselines train on in-domain
documents, whileEvaporateuses no training [22].
System
SWDE MovieSWDE University
Closed Open
ClosedOpen
ZeroShot Ceres [46]-50.0-50.0
RoBERTa-Base49.335.636.638.0
RoBERTa-Structural47.739.946.542.3
DOM-LM [22]71.954.168.055.2
Evaporate-Direct84.445.272.653.8
Evaporate-Code55.033.040.522.2
Evaporate-Code+79.556.873.759.0
Systems for unstructured .We are not aware of strong base-
lines that apply beyondHTMLdocument formats. The most rele-
vant baseline is the OpenIE6 system for performing OpenIE over
any unstructured from Kolluru . [39]. We find the system
only handles well formatted sentences and struggles to extend to
heterogeneous data types. We find that even when documents con-
tain full sentences, the system extracts an extremely large set of
relations and does not enforce consistent extractions across docu-
ments. For instance, on a sample FDA 510(k) document, OpenIE6
extracts 427 relations with 184 relations having a confidence level
at 0.99. We include a detailed error analysis in [5].
4.3.2Efficiency comparisons.We compare the efficiency ofEvap-
oratewith the (estimated [28]) 175B parameter OpenAI model vs.
the baseline, which uses a pretrained 125M parameter RoBERTa
model [22], decomposed in terms of pretraining, fine-tuning, infer-
ence, and parameter (memory) cost. FLOPS as reported in
the OpenAI reference work Brown . [14], for data settings with
푛documents and푚attributes, the costs are:
•RoBERTaThe model is 125M parameters, total pretraining
FLOPS is 1.50E+21, and inference FLOPS per token is2× of Parameters, which is0.250GFLOPS. The total
inference cost is computed by
푛×
tokens
푑표푐푢푚푒푛푡
×0.250GFLOP
•GPT3-175BThe model is 175B parameters, total pretrain-
ing FLOPS is 3.14E+23, and inference FLOPS per token is
2× of Parameters, which is350GFLOPS. The total
inference cost is computed by
푚×푃×
tokens
푐ℎ푢푛푘
×350GFLOP
where푃is the of prompts per . Note that
for our evaluated implementation푃≈10푐since generation is performed on10documents.
Evaporateuses a model with 1,400x more parameters and 300x
higher pretraining cost. However, users of the baselines likely need
to locally fine-tune and host the models. The inference costs of the
baseline andEvaporateon our datasets are in the same order of
magnitude (summarized in [5]). The extended cost comparison and
analysis are provided in [5]. Users should select a method depending
on the data setting, i.e. the of documents and attributes. We
note thatEvaporateallows users to tradeoff quality and efficiency
by changing the underlying language model to smaller variants.
4.4Comparing Implementations ofEvaporate
This work proposes a fundamental tradeoff space between directly
processing data workloads with LLMs vs synthesizing code that
does the processing. We first discuss the tradeoffs for a fixed LLM
(-davinci-003), which is the current best-in-class LLM [42]
( 4.4.1), and next across a range of LLMs trained by three
distinct model providers ( 4.4.2).
4.4.1Tradeoffs betweenEvaporateImplementations.As detailed in 3.2, the base routine (“Evaporate-Direct”) inEvaporate
entails directly processing documents with the LLM, while the
optimized routine (“Evaporate-Code”) synthesizes functions for
processing. Next we evaluate these along our desiderata.
Generality is maintained.LLMs take as input and pro-
vide as output — this unified natural language interface means
Evaporate-DirectandEvaporate-Codecan ingest any docu-
ment format without additional engineering.Critically, our results
withEvaporaterequire no effort, no training whatsoever, and
no customization when applied to the 16 different settings.
Asymptotic cost reduction. 3 demonstrates the asymp-
totic differences in cost between directly processing the data lake
withEvaporate-Directvs. withEvaporate-Code+. ( 3 Left)
Evaporate-Directis asymptotically more efficient as a of
the of documents in the data lake. The of LLM calls
required with generation is proportional to the of attributes to be extracted, not the of documents. The
crossover point is at∼40 documents.
( 3 Right)Evaporate-Directcan extract multiple (i.e.
every) in the in-context documents in a single inference
call, whileEvaporate-Code+synthesizes functions for each
. Thus, the cost of synthesis grows with the num-
ber of attributes, while the cost ofEvaporate-Directis constant.
The crossover point is at∼2,500 attributes.
Empirically across our settings,Evaporate-Code+realizes a
110x average reduction in the of tokens the LLM needs
to process (assuming 10k documents per setting and 378×given
the true benchmark sizes) in the of tokens the LLM must
process compared toEvaporate-Direct(Table 3). Further, data
lakes are constantly changing and functions can be reused while
Evaporate-Directwould need to be re-run, multiplying the cost.
In runtime, we observe that the generated functions are efficient
in processing the documents. For example, over the 9,500 10 Table 3: Quality (OpenIE Pair F1) and cost ( of tokens processed by the LLM) for producing the structured views. We
compare the direct prompting and code synthesis implementations -davinci-003.Evaporate-Code+is evaluated on
the full datasets, whileEvaporate-Directis evaluated on a randomly sampled proportion of documents due to the cost (20%
of FDA and Wiki, 2% of SWDE, and 0.0002% of Enron, given the respective sizes).
Source (Format)
Evaporate-DirectEvaporate-Code+Relative Performance
QualityCost / 10K DocumentsQualityCost / 10K Documents
QualityCost Reduction
F1Tokens (M)Cost ($)F1Tokens (M)Cost ($)
FDA (TXT)45.5145.62,90062.81.938+17.377x
Enron Emails (TXT)93.821.2425
86.90.612-6.935x
Wiki NBA (HTML)44.8650.113,00068.23.060+23.4217x
SWDE Movie (HTML)45.2282.95,660
56.82.346+11.6123x
SWDE University (HTML)53.8190.13,80059.01.938+5.2100x
Average56.62585,15766.71.939+10.1110x
runs (from 95 functions evaluated on 100 documents each) in the
FDA 510(k) setting, we find that the average time to run one func-
tion over one document is 0.00025s on a 2 CPU machine.
Improved quality and reliability.Even thoughEvaporate-
Directdirectly processes each document with the LLM,Evaporate-
Code+surprisingly performs 10.1 F1 (18%) better (Table 3).
What are the failure modes ofEvaporate-Direct?The method
yields inconsistent generations. On the Medical FDA report set-
ting:(1) The LLM misses an average of 4.4 attributes that are present
in the gold schema (27.5% of gold attributes) per document. Among
the gold attributes that are missed, allare extractedin at least one
document. (2) Further, the LLM outputs an average of 9.7 attributes
or values that are not explicitly mentioned in the documents. (3)
Finally, attributes are reworded in diverse ways across documents
— the attributeclassificationis extracted in 4 different ways
across the sample of 10 documents (i.e. “classification”, “device
classification”, “regulatory information”, missing). Since the error
modes are quite varied, it is unclear how to improve quality.
Why doesEvaporate-Code+improve quality?We validate that
our Algorithm 1 for selecting and aggregating functions leads to
the quality improvements overEvaporate-Direct.
Synthesizing diverse functionsWe find that diverse
prompts helps address the lack of reliability in synthesis.
To synthesize functions,Evaporate-Code+uses a prompt template
that includes one-to-two in-context examples and a placeholder for
the inference example, i.e. document ( 4). We can produce
multiple prompts in the template by swapping the in-context exam-
ples or sampling more documents (change the inference example).
We find both means of increasing diversity benefit quality:
•In-context demonstrationsOur implementation (Table 1)
instantiates two prompts by swapping in-context demon-
strations ,푃
퐴
and푃
퐵
. Quality 푃
퐴
or푃
퐵
alone is 8.5 and
8.0 F1 points worse than both to synthesize functions
on SWDE Movie and SWDE University respectively.
•Inference documentsUsing three versus five sample doc-
uments in the prompts forEvaporate-Code+, the ClosedIE
and OpenIE quality improve by 6.8 F1 points (9%) and 6.5 F1
points (14%) respectively, averaged across the 16 settings.
Estimating quality the LLM.In Table 4, we
first evaluate the two unsupervised aggregation baselines in prior
work off-the-shelf: Majority Vote (MV) and Weak Supervision (WS)
Table 4: Quality under alternate approaches of aggregating
the synthesized functions. Baselines are in the left columns:
Majority Vote (MV) and Weak Supervision (WS). Components
of Algorithm 1 are in the right columns: “Abstain” accounts
for abstensions and “Filter” filters low quality functions.
SourceMVWS
WSWS
FilterAbstain+Filter
FDA (TXT)52.951.155.062.8
Enron Emails (TXT)81.482.786.986.9
Wiki NBA (HTML)59.564.9
68.468.2
SWDE Movie (HTML)44.346.3
56.656.8
SWDE University (HTML)42.743.557.359.0
Average56.2 57.764.866.7
[4,57,67]. Next we measure the effect of filtering functions and
handling abstentions as proposed in Algorithm 1.
In Table 4, we observe WS with filtering provides a consistent
boost across settings compared to WS — 7.1 F1 point higher average
quality and up to 13.8 F1 points on the SWDE University setting.
Additionally handling abstensions leads to a 1.9 F1 point increase in
average quality over WS with filtering, with up to 7.8 F1 points on
the FDA setting. Qualitatively, accounting for abstensions is helpful
when attributes are expressed in diverse ways across documents,
which is not applicable to all settings such as Enron. These results
highlight the importance ofEvaporate-Code+’s aggregation ap-
proach for the system’s overall reliability. Without Algorithm 1,
quality does not improve overEvaporate-Direct.
4.4.2 Understanding the Tradeoff Space across Varied Language
Models.The are an increasing of LLMs being made avail-
able. These models are trained by various providers each distinct protocols [42]. To understand whether the tradeoffs we
identified hold for different LLMs, we evaluateEvaporateusing
three additional LLMs from three different providers: (1)GPT-4
[52], (2)Anthropic Claude-V1[6], and (3)Jurassic Jumbo-2-
Instruct[43]. Results are summarized in Table 5.
Overall results.The quality withgpt-4is comparable to that
obtained usingtext-davinci-003. Both theEvaporate-Direct
andEvaporate-Code+quality decrease withclaudeandjumbo,
consistent with the results of large-scale benchmarking efforts [42],
however therelativequality of the two implementations are similar
to Table 3. Both appear to remain competitive in quality and the
quality of the approaches appear to increase together.
11 Table 5: OpenIE (Pair F1) results evaluatingEvaporateusing alternate LMs from three model providers. For cost reasons, we
applyEvaporate-Directto samples of 10 documents each. For fair comparison, we report the score ofEvaporate-Code+on
the same sample instead of the full set of documents.푘is the of gold attributes for the setting.
Source (Format)
Evaporate-DirectEvaporate-Code+Schema ID
FDA Wiki Movie University Enron
FDA Wiki Movie University EnronF1@푘
OpenAI GPT-4 [52]59.240.535.156.192.757.561.454.957.285.567.3
Anthropic Claude-V1 [6]45.120.627.544.388.144.433.538.730.484.769.0
Jurassic Jumbo-2-Instruct [43]25.90.013.329.290.3
1.20.020.618.685.762.3
We find the precision ofEvaporate-Code+remains high across
models. Algorithm 1 helpsEvaporatefilter the low quality func-
tions and if this eliminatesallthe candidate functions, the is excluded from the output table. We find that when an attributeis
included in the output, it has high precision, consistent with Table 1
where the precision withtext-davinci-003is almost 20 points
higher than the recall. The average precision scores corresponding
toEvaporate-Code+in Table 5 are 70.9 (gpt-4), 67.6 (claude),
and 50.9 (jumbo) usingEvaporate-Code+and in contrast are 61.9
(gpt-4), 55.1 (claude), and 49.9 (jumbo) usingEvaporate-Direct,
emphasizing a precision-recall tradeoff between approaches.
Understanding the errors.Overall,Evaporaterelies on versatile
reasoning capabilities (i.e.to identify the schema and extract at-
tribute valuesdirectlyfromnoisyprovided context, and the ability
to synthesize code) and excitingly, the results validate that these
capabilities co-exist within multiple model families. We investi-
gate which of the required reasoning capabilities contributes to
lower quality in comparison totext-davinci-003. We find that
the schema synthesis step plays a small role. Considering the top
ranked schema attributes according toEvaporate, we measure the
average F1@푘between the predicted and gold sets of attributes,
where푘is the of gold attributes per setting. The average
F1@푘fortext-davinci-003is 71.9, and the right-hand column
of Table 5 shows the alternate models perform comparably.
We find the two main sources of errors are (1) the inability to
generate a for particular attributes, and (2) occasionally,
low qualitydirectextractions in particular cases (e.g.,claudemay
respond “I’m not sure, please give me more information.” in a Chat-
Bot style, when prompted to extract an value). The models
we evaluate are optimized for ChatBot applications [38].
5 RELATED WORK
Structured Querying of Heterogeneous Data.Converting hetero-
geneous data to structured databases is a long standing data man-
agement problem [12,16,31, inter alia.]. In contrast to systems
for knowledge base construction (KBC) or closed information ex-
traction (IE) [61,70], which assume there is a predefined schema
and focus on populating the database according to the schema, the
setup we focus on relies on OpenIE. OpenIE is the task of extracting
useful facts without to a predefined ontology (i.e. the types
or categories of facts to be extracted) [7, 20]. Given the breadth of
input documents, the ability to construct a schema and populate
the corresponding database on-the-fly is useful.
Existing systems for this problem introduce assumptions about
the data-domain [22,55,64], file-format (e.g., XML files) [30], or
the syntactic patterns of useful facts [12,15,25,31,39,48,51,71].
For instance, in early systems, Cafarella . [15]focuses on facts
expressed as triples (two entities with a descriptive string of their re-
lationship in between) with hypernym “is-a” relationships between
the entities. The recent deep learning systems (1) require
domain- and document-format specific training, (2) focus on rea-
soning over sentences, in contrast to long documents, and (3) rely
on high quality linguistic tools (e.g. dependency parse, POS, NER)
to help introduce structure over unstructured [39, 71].
For the narrower problem of generating structured views from
web data[15,23,25], the current state-of-the-art approaches (1) distant supervision to train site-specific extraction models [45]
(domain specific), and (2) rely on assumptions about where in the
HTML-DOM attributes and values are located (format specific)
Deng . [22], Lockard . [46]. We investigate the feasibility
of a domain and document-format agnostic .
Language Models for Data Management.Given the recency of in-
context learning, there are few works exploring the benefits for data
processing. Most closely related, Chen . [17]presents a system
for querying heterogeneous data lakes with in-context learning.
The proposed involves processingevery documentwith
the LLM to extract values of interest. We propose an alternate and tradeoff space for processing data with LLMs.
Other recent work applies language models for tasks such as
data wrangling [49] or code generation for SQL queries [63]. Un-
likeEvaporate, supporting various data formats, these prior ap-
proaches focus on relational data only and design manual prompts
to demonstrate high quality.
Data Programming.We build on work in data programming
and weak supervision [57].Evaporateautomatically generates
functions rather than human-designed functions. We WS
on open ended tasks in contrast to the classification tasks considered
in the prior work on automated WS [9,65]. We show how to the LLM to handle abstensions and filter low quality functions.
6 CONCLUSION
We proposeEvaporate, a system that uses LLM in-context learning
to generate structured views of semi-structured data lakes. We
identify and explore a cost-quality tradeoff between processing data
directly with an LLM versus synthesizing and aggregating multiple
code snippets for data processing. We present an algorithm and
theoretical analysis for applying weak supervision to aggregate
the code snippets. The code- aims to exploit the
structural redundancies that occur in corpora of semi-structured
documents. We validateEvaporateon 16 unique data settings
spanning 5 domains and 3 document formats, considering the cost,
quality, and generality of the system. Our study highlights the
promise of LLM- data management systems.
12 REFERENCES
[1]April 2023. Wikipedia Statistics. https://en.wikipedia.org/wiki/Special:Statistics
[2]Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag.
2022. Large Language Models are Few-Shot Clinical Information Extractors.The
2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)
(2022).
[3]
Simran Arora, Patrick Lewis, Angela Fan, Jacob Kahn, and Christopher Ré. 2023.
Reasoning over Public and Private Data in Retrieval- Systems.Transactions
of Computational Linguistics (TACL)(2023).
[4]Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush
Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. 2023. Ask Me Anything:
A simple strategy for prompting language models.International Conference on
Learning Representations (ICLR)(2023).
[5]Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Ho-
jel, Immanuel Trummer, and Christopher Ré. 2023. Language Models Enable
Simple Systems for Generating Structured Views of Heterogeneous Data Lakes.
(2023). https://www.dropbox.com/scl/fi/3gt3ixdbvp986ptyz5j4t/VLDB_Revision.
pdf ?rlkey=mxi2kqp7rqx0frm9s7bpttwcq&dl=0
[6]Amanda Askell, Yushi Bai, Anna Chen, Dawn Drain, Deep Ganguli, T. J. Henighan,
Andy Jones, and Nicholas Joseph . 2021. A General Language Assistant as a
Laboratory for Alignment.arXiv:2112.00861v3(2021).
[7]
Michele Banko, Michael J. Cafarella, Stephen Soderland, Matthew G Broadhead,
and Oren Etzioni. 2007. Open information extraction from the web.IJCAI(2007).
[8]David W Bates, David M Levine, Hojjat Salmasian, Ania Syrowatka, David M
Shahian, Stuart Lipsitz, Jonathan P Zebrowski, Laura C Myers, Merranda S Logan,
Christopher G Roy, .2023. The Safety of Inpatient Health Care. England
Journal of Medicine388, 2 (2023), 142–153.
[9]Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. 2021.
Interactive weak supervision: Learning useful heuristics for data labeling.
[10]
Rideout J. R. Dillon M. R. Bokulich N. A. Abnet C. C. -Ghalith G. A. Alexander H.
Alm E. J. Arumugam M. . Bolyen, E. 2019. Reproducible, interactive, scalable
and extensible microbiome data science qiime 2. InNature biotechnology.
[11]Rishi Bommasani, Drew A. Hudson, E. Adeli, Russ Altman, Simran Arora, S. von
Arx, Michael S. Bernstein, Jeanette Bohg, A. Bosselut, Emma Brunskill, and . 2021. On the opportunities and risks of foundation models.arXiv:2108.07258
(2021).
[12]S. Brin. 1998. Extracting patterns and relations from the WorldWide Web. In
WebDB.
[13]
Mirko Bronzi, Valter Crescenzi, Paolo Merialdo, and Paolo Papotti. 2013. Extrac-
tion and integration of partially overlapping web sources.PVLDB(2013).
[14]
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, .2020. Language models are few-shot learners.Advances in neural
information processing systems33 (2020), 1877–1901.
[15]
Michael J. Cafarella, Christopher Re, Dan Suciu, Oren Etzioni, and Michele Banko.
2007. Structured Querying of Web . InConference on Innovative Data Systems
Research (CIDR).
[16]
Michael J Cafarella, Dan Suciu, and Oren Etzioni. 2007. Navigating Extracted
Data with Schema Discovery.. InWebDB. 1–6.
[17]
Zui Chen, Zihui Gu, Lei Cao, Ju Fan, Sam Madden, and Nan Tang. 2023. Sym-
phony: Towards Natural Language Query Answering over Multi-modal Data
Lakes.CIDR(2023).
[18]Eric Chu, Akanksha Baid, Ting Chen, AnHai Doan, and Jeffrey Naughton. 2007.
A Relational to Incrementally Extracting and Querying Structure in
Unstructured Data. InVLDB.
[19]Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.
ELECTRA: pre-training encoders as discriminators rather than generators.
InInternational Conference on Learning Representations (ICLR).
[20]W. Cohen. 2004. Information extraction and integration: An overview.IJCAI
(2004).
[21]Lei Cui, Furu Wei, and Ming Zhou. 2022. Neural Open Information Extraction.
(2022).
[22]
Xiang Deng, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun.
2022. DOM-LM: Learning Generalizable Representations for HTML Documents.
(2022).
[23]Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open
information extraction from the web.Commun. ACM51, 12 (2008), 68–74.
[24]Oren Etzioni, Michael Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu,
Tal Shaked, Stephen Soderland, Daniel S Weld, and Alexander Yates. 2004. Web-
scale information extraction in knowitall: (preliminary results). InProceedings of
the 13th international conference on World Wide Web. 100–110.
[25]Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2004. Unsupervised
named-entity extraction from the Web: An experimental study. InAAAI.
[26]
J. H. Faghmous and V Kumar. 2014. A big data guide to understanding climate
change: The case for theory-guided data science. InBig data.
[27]Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and
Christopher Re. 2020. Fast and Three-rious: Speeding Up Weak Supervision with
Triplet Methods. InProceedings of the 37th International Conference on Machine
Learning (Proceedings of Machine Learning Research), Vol. 119. PMLR, 3280–3291.
[28]Leo Gao. 2021. On the Sizes of OpenAI API Models. https://blog.eleuther.ai/gpt3-
model-sizes/
[29]
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and
Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse for Language
Modeling.
[30]Minos Garofalakis, Aristides Gionis, Rajeev Rastogi, Sridhar Seshadri, and
Kyuseok Shim. 2000. XTRACT: A system for extracting document type descrip-
tors from XML documents. InProceedings of the 2000 ACM SIGMOD international
conference on Management of data. 165–176.
[31]Eugene Agichtein Luis Gravano. 2000. Snowball: Extracting Relations from
Large Plain- Collections. InDL ’00: Proceedings of the fifth ACM conference
on Digital libraries.
[32]
Qiang Hao, Rui Cai, Yanwei Pang, and Lei Zhang. 2011. From one tree to a forest:
a unified solution for structured web data extraction.SIGIR(2011).
[33]Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa:
Decoding-Enhanced BERT With Disentangled Attention. InInternational Con-
ference on Learning Representations.
[34]Nathan Heller. 2017. What the Enron E-mails Say About Us.https://www.
newyorker.com/magazine/2017/07/24/what-the-enron-e-mails-say-about-us
[35] Nick Huss. 2023. How Many Websites Are There in the World?
[36]
Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,
Christopher Potts, and Matei Zaharia. 2022. Demonstrate-Search-Predict: Com-
posing Retrieval and Language Models for Knowledge-Intensive NLP.arXiv
preprint arXiv:2212.14024(2022).
[37]
B. Klimt and Y. Yang. 2004. Introducing the enron corpus. InProceedings of the
1st Conference on Email and Anti-Spam (CEAS).
[38]
Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło,
Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kan-
clerz, .2023. ChatGPT: Jack of all trades, master of none.arXiv preprint
arXiv:2302.10724(2023).
[39]Keshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Mausam, and Soumen
Chakrabarti. 2020. OpenIE6: Iterative Grid Labeling and Coordination Analy-
sis for Open Information Extraction. InProceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP).
[40]Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettle-
moyer, Scott Wen tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2022. DS-1000:
A Natural and Reliable Benchmark for Data Science Code Generation.ArXiv
abs/2211.11501 (2022).
[41]
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Sur-
vey on Retrieval-Augmented Generation.CoRRabs/2202.01110 (2022).
arXiv:2202.01110 https://arxiv.org/abs/2202.01110
[42]Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, and
more. 2022. Holistic Evaluation of Language Models.ArXivabs/2211.09110
(2022).
[43]Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical
details and evaluation. (2021).
[44]Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining .CoRRabs/1907.11692 (2019).
arXiv:1907.11692 http://arxiv.org/abs/1907.11692
[45]
Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. 2019. OpenCeres: When
Open Information Extraction Meets the Semi-Structured Web.Proceedings of
NAACL-HLT(2019).
[46]Colin Lockard, Prashant Shiralkar, Xin Luna Dong, and Hannaneh Hajishirzi.
2020. ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Web-
pages.ACL(2020).
[47]Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak
Paul. 2022. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods.
https://github.com/huggingface/peft.
[48]Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni.
2012. Open Language Learning for Information Extraction.Proceedings of the
2012 Joint Conference on Empirical Methods in Natural Language Processing and
Computational Natural Language Learning.
[49]
Avanika Narayan, Ines Chami, Laurel Orr, Simran Arora, and Christopher Ré.
2022. Can Foundation Models Wrangle Your Data?Proceedings of the VLDB
Endowment International Conference on Very Large Databases(2022).
[50]Fatemeh Nargesian, Erkang Zhu, Reneé J. Miller, Ken Q. Pu, and Patricia C. Aro-
cena. 2019. Data Lake Management: Challenges and Opportunities.Proceedings
of the VLDB Endowment(2019).
[51]Christina Niklaus, Matthias Cetto, André Freitas, and Siegfried Handschuh. 2018.
A Survey on Open Information Extraction. InProceedings of the 27th International
Conference on Computational Linguistics.
[52] OpenAI. March 2023. OpenAI API. https://openai.com/api/
13 [53] Laurel Orr. 2022. Manifest. https://github.com/HazyResearch/manifest.
[54]Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, .2022.
Training language models to follow instructions with human feedback.arXiv
preprint arXiv:2203.02155(2022).
[55]F. Chen A. Doan P. DeRose, W. Shen and R. Ramakrishnan. 2007. Building
structured web community portals: A top-down, compositional, and incremental
.VLDB(2007).
[56]
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
2016.SQuAD: 100,000+ Questions for Machine Comprehension of .
arXiv:1606.05250(2016).
[57]Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu,
and Christopher Ré . 2017. Snorkel: Rapid Training Data Creation with Weak
Supervision.Proceedings of the VLDB Endowment (VLDB)(2017).
[58]C. Romero and S. Ventura. 2013. Data mining in education. InWiley Interdisci-
plinary Reviews: Data Mining and Knowledge Discovery.
[59]Shreya Shankar, Rolando Garcia, Joseph M. Hellerstein, and Aditya G.
Parameswaran. 2022. Operationalizing Machine Learning: An Interview Study.
arXiv:2209.09125(2022).
[60]Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y
Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, .2023. High-
throughput Generative Inference of Large Language Models with a Single GPU.
arXiv preprint arXiv:2303.06865(2023).
[61]Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, and Christo-
pher Ré. 2015. Incremental knowledge base construction deepdive. In
Proceedings of the VLDB Endowment International Conference on Very Large Data
Bases (VLDB).
[62]
Ryan Smith, Jason A. Fries, Braden Hancock, and Stephen H. Bach. 2022. Lan-
guage Models in the Loop: Incorporating Prompting into Weak Supervision.
arXiv:2205.02318v1(2022).
[63]Immanuel Trummer. 2022. CodexDB: synthesizing code for query processing
from natural language instructions GPT-3 codex.Proceedings of the VLDB
Endowment11 (2022). https://doi.org/10.14778/3551793.3551841
[64]S. Raghavan S. Vaithyanathan T.S. Jayram, R. Krishnamurthy and H. Zhu. 2006.
Avatar information extraction system.IEEE Data Eng. Bull(2006).
[65]Paroma Varma and Christopher Ré. 2018. Snuba: Automating Weak Supervision
to Label Training Data.
[66]Paroma Varma, Frederic Sala, Ann He, Alexander Ratner, and Christopher Re.
2019. Learning Dependency Structures for Weak Supervision Models.Proceedings
of the 36th International Conference on Machine Learning (ICML).
[67]
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le Le, Ed H. Cho, Sharan
Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-Consistency Im-
proves Chain of Thought Reasoning in Language Models.arXiv:2203.11171v2.
[68]
Eric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang, Daniel Ho, and James Zou.
2021. How medical AI devices are evaluated: limitations and recommendations
from an analysis of FDA approvals.Nature Medicine27 (04 2021), 1–3.
[69]Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent
and controllable human-ai interaction by chaining large language model prompts.
InCHI Conference on Human Factors in Computing Systems. 1–22.
[70]Wael M.S. Yafooz, Siti Z.Z. Abidin, Nasiroh Omar, and Zanariah Idrus. 2013.
Managing unstructured data in relational databases. In2013 IEEE Conference on
Systems, Process & Control (ICSPC).
[71]Shaowen Zhou, Bowen Yu, Aixin Sun, Cheng Long, Jingyang Li, Haiyang Yu, Jian
Sun, and Yongbin Li. 2022. A Survey on Neural Open Information Extraction:
Current Status and Future Directions.IJCAI22(2022).
[72]
Diana M. Zuckerman, Paul Brown, and Steven E. Nissen. 2011. Medical Device
Recalls and the FDA Approval Process.Archives of Internal Medicine171, 11 (06
2011), 1006–1011.
14 A EXPERIMENTAL DETAILS
A.1 Evaluation Protocol
Because the baselines we compare against on SWDE require train-
ing data, they perform website-wise cross validation (i.e.train on
some websites and evaluate on others). They do this for several
combinations such that every website appears in the evaluation set.
Evaporate, in contrast, does not require any training data, so we
do not perform cross validation. To ensure a fair comparison, we
simply evaluateEvaporateon all websites so that our method is
evaluated on the same examples as the baselines.
A.2 Metrics
We describe the metrics we to evaluate OpenIE and ClosedIE
performance of our system.
Pair F1For OpenIE, we report Pair F1 scores. Pair F1 is the
standard metric for OpenIE systems [45]. The metric constructs
(subject, value, predicate). Thesubjectis the document-
filename in our setting. Thepredicateis the and the
valueis the value. The F1 score computes the F1 score
between the sets of gold and predicted tuples. This assigns credit
forexact-matchesbetween the names and values extracted
by the system and the ground truth — it assigns no partial credit.
Note that becauseEvaporatefirst identifies a list of attributes
then sequentially generates functions and extracts the values, the can “stop” execution at any of attributes. The stopping
point determines the of tuples included in the prediction
set. This is not a property of prior systems that extract “all or
no” tuples [22,39,45,46,71, inter alia.]. For fair comparison we
report performance at the of gold attributes contained in
the benchmark — note that this is generallynotthe of
attributes that maximizes theEvaporate’s Pair F1 score. F1For ClosedIE, we report F1 scores. F1 is the
standard metric for extractive tasks and we the exact implemen-
tation released by Rajpurkar . [56]. The metric tokenizes the
prediction and gold strings and computes a token-wise F1 score.
Recall we select the F1 at the of gold attributes, rather
than at the that gives the highest score).
B DATASET CONSTRUCTION
Below we describe how each of the evaluation benchmarks is ob-
tained. We also release the suite of benchmarks along with the
system code.
B.1 FDA
For the FDA setting, we randomly sampled a dataset of 100 FDA
510(k) premarket notification submission PDFs for medical devices
with substantially equivalent predicate devices since 1996 from the
FDA website: https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/
cfpmn/pmn.cfm. We the lightweight fitz library to convert
this to files. We asked 5 database graduate students to identify
important attributes, defined as attributes useful for analysis that’s
present in at least a majority of documents. We collected the final
set of 16 attributes as attributes agreed on by all graduate students.
For these 16 attributes, we manually wrote functions to extract
their value, then corrected errors by manual review. We defined
the value as the full content of information pertaining to
that , typically a value, sentence, or .
B.2 Wiki NBA
For the Wiki NBA setting, we the following SPARQL query
over Wikidata to retrieve NBA articles. We then manually supple-
mented missing pages and filtered the results to only include pages
about NBA players.
# Q13393265 is for Basketball Teams
# Q155223 is for NBA
# P118 is league (https :// www.wikidata.org/wiki/
Property:P118)
SELECT ?item ?itemLabel ?linkcount WHERE {
?item wdt:P118 wd:Q155223 .
?item wikibase:sitelinks ?linkcount .
FILTER (? linkcount >= 1) .
SERVICE wikibase:label { bd:serviceParam wikibase
:language "[ AUTO_LANGUAGE],en" . }
}
GROUP BY ?item ?itemLabel ?linkcount
ORDER BY DESC(? linkcount)
We asked 5 database graduate students to identify important
attributes, defined as attributes useful for analysis that’s present
in at least a majority of documents. We collected the final set of
19 attributes as the attributes agreed on by all graduate students.
For these 19 attributes, we manually wrote functions to extract
their value, then corrected errors by manual review. We defined
the value as the full content of information pertaining to
that , typically a value, sentence, or . We these
as ground truth extractions in the main paper.
We noted that the resulting attributes were complex for multiple
documents, so we included another set of ground truth. We asked
a graduate student to write functions to parse compound attributes
whose values mention multiple values (e.g. birth date and location
under "Born") into atomic attributes and values. We this
ground truth to demonstrate an additional step of schema cleaning
in F.2.
B.3 Enron
We download the Enron corpus from http://www.cs.cmu.edu/~enron/
and apply no further processing. We generate a benchmark all metadata in the email headers by manually writing functions.
B.4 SWDE
We download SWDE from https://github.com/cdlockard/expanded_
swde. The benchmark includes the rawHTMLfrom several web-
sites with no further processing and ground-truth labels for se-
lected attributes [45]. Because all the attributes are located within
the root-elements of theHTMLbody, excluding the information
such as theHTMLheader, attributes described within tags (e.g.
<a href='/year/2012/>',<title>), and so on, we extend the
original benchmark to include a more diverse set of attributes. We
refer to the extended benchmark as SWDE Plus.
15 C WEAK SUPERVISION DETAILS
Objective.We detail the weak supervision (WS) algorithm for aggregating the votes across generated functions. LetDbe our
unlabeleddataset of documents from which we are extracting a
particular of interest. Let푦be a random variable repre-
senting the true value. Let흀represent the outputs of our
푚generated extraction functions푓∈ Eon a particular document.
Each휆
푖
∈흀is a 휆
푖
:X → Y. Our goal is to the
vectors흀, produced across documents푥∈ Dto infer the true label
푦. Concretely, we seek,휙(푥), a that inputs흀and outputs
a final prediction
ˆ
푦for a document푥.
With labeled data, i.e. where we had documents and their at-
tribute values{(푥
1
,푎
1
), ...,(푥
푛
,푎
푛
)}, we could perform traditional
supervised learn휙(푥)that maps흀to푦. However, in our unlabeled
setting, the insight is to the noisy estimates of푦produced by
each of the functions to construct휙(푥).
Standard WS Setup [57].WS models learn the latent variable
graphical model on the distributionPr(푦,{휆
1
, ...,휆
푚
}=Pr(푦,흀).
In this model,푦islatent, we cannot observe it. To produce
ˆ
푦, we
concretely perform two steps:
•Learn the label modelWe have to흀and can this to learn thelabel model푃(흀|푦).
•InferenceWe can then produce the estimate
ˆ
푦by setting
휙(푥)=argmax
푦
Pr(푦|흀(풙)
First, we will discuss how to learn the label model. We parame-
terize푃(흀|푦)as follows:
푃(흀|푦)=
1
푍
휃
exp(
푚
∑︁
푖=1
휃푑(휆
푖
,푦))
Intuitively when modeling our sources, we want to model how
accurate a particular휆
푖
is, when it makes a prediction. We are then
going to want to weigh the predictions of the different sources
proportional to their accuracies.
푍
is a constant to normalize the probability distribution. The
feature-vector푑can be broken down into:
•푑
lab
, representing how frequently휆
푖
provides a prediction
vs. abstains across documents. Thisisdirectly observable.
•푑
corr
, which represents how frequently휆
푖
and휆
푗
yield the
same prediction for the value. Thisisdirectly
observable.
•푑
acc
, representing the accuracy of휆
푖
, or how frequently휆
푖
agrees with푦. Note that the accuracy is measured across
documents for which the provides a prediction
and does not abstain. Thisis notdirectly observable.
휃are the parameters we aim to learn to combine the inputs in the
feature vector. To learn this without to푦, we can minimize
the negative log marginal likelihood given the observed휆
푖
outputs
and solve with SGD, or a closed-form solution [27].
C.1 Theoretical Analysis
Here we analyze our proposed weak supervision algorithm (Algo-
rithm 1) forEvaporate-Code+.
Setup.Recall that in Algorithm 1, the LLM is 1) to generate
the candidate extraction functions퐹={푓
1
, ..., 푓
푚
}, and 2) to directly
extract values from a small set of푛documents,퐷
푒푣푎푙
. Let{
ˆ
푦
1
, ...,
ˆ
푦
푛
}
be the LLM’s direct extractions over퐷
푒푣푎푙
. The candidate functions
are each applied to푥
푖
∈퐷
푒푣푎푙
to produce{푓
푖
(푥
1
), ..., 푓
푖
(푥
푛
)}for
each푓
푖
∈퐹.
The candidate outputs{푓
푖
(푥
1
), ..., 푓
푖
(푥
푛
)}are then scored
against the LLM’s direct extractions{
ˆ
푦
1
, ...,
ˆ
푦
푛
}to estimate the func-
tion accuracy:
ˆ
푎=
1
푛
푛
∑︁
푗=1
1(푓
푖
(푥
푗
)=
ˆ
푦
푗
)
From Varma and Ré[65], we can estimate whether the accura-
cies learned via weak supervision ̃
푎
will be close to the true accuracies푎
∗
. We can the accuracy estimated on퐷
푒푣푎푙
to estimate whether the accuracies ̃
푎learned by the weak super-
vision model휃are close to the true accuracies푎
∗
, i.e. we
can bound||푎
∗
− ̃
푎||
∞
<훾. Concretely, the objective is to provide a
guarantee that훾is the upper bound on the true error, given that the
measured error||
ˆ
푎− ̃
푎||
∞
is below some threshold휖. Proposition 1
in Varma and Ré[65]states the following. Refer to Varma and Ré
[65] for the proof.
We have푚functions with empirical accuracies
ˆ
푎and learned
accuracies ̃
푎, and the measured error is below some threshold휖. Then,
if each labels a minimum of
푛≥
1
2(훾−휖)
log(
2푚
2
훿
)
datapoints, the weak supervision label model will succeed in learning
accuracies such that||푎
∗
− ̃
푎||
∞
<훾with a probability1−훿.
Extension.However, in our setting,
ˆ
푦
푖
may not be equal to the
true extraction푦
∗
푖
for푑
푖
∈퐷
푒푣푎푙
because the LLM is being to
score the candidate functions, rather than a gold-standard labeled
dataset. Suppose the LLM introduces its own error rate푒, then
our objective is to modify the above statement to account for this
source of error. Intuitively, as푒increases, we need more datapoints
to obtain a better estimate of the accuracies.
Assumptions and NotationWe will assume the errors are ran-
domly occurring across푥
푖
∈퐷
푒푣푎푙
and that the data points in퐷
푒푣푎푙
are independent. Let
ˆ
푎be the accuracy estimated the noisy
labels
ˆ
푦on퐷
푒푣푎푙
and let푎
★
be the accuracy given ground
truth labels푦on퐷
푒푣푎푙
. We follow from Varma and Ré[65]below,
but extend the theory to support our setting.
Proposition 1. We have푚functions with empirical accuracies
ˆ
푎,
where the gold labels have error rate푒, the learned accuracies are ̃
푎, and the measured error is below some threshold휖. Then, if each labels a minimum of
푛≥
1
2(훾−휖−푒)
log(
2푚
훿
)
datapoints, the weak supervision label model will succeed in learning
accuracies such that||푎
∗
− ̃
푎||
∞
<훾with a probability1−훿.
Proof.We can decompose the true error||푎
∗
− ̃
푎||
∞ the
triangle inequality:
푃(||푎
∗
− ̃
푎||
∞
>훾) ≤푃(||푎
∗
−
ˆ
푎||
∞
+||
ˆ
푎− ̃
푎||
∞
>훾)
≤푃(||푎
∗
−
ˆ
푎||
∞
+휖)
16 Recall that for candidate 푓
푖
, where푛
푖
is the of
data points on which푓
푖
did not abstain:
ˆ
푎
푖
=
1
푛
푖
∑︁
푗:푓
푖
(푥
푗
)≠∅
1(푓
푖
(푥
푗
)=
ˆ
푦
푗
)
where
ˆ
푦
푗
is the direct LLM extraction on document푥
푗
∈퐷
푒푣푎푙
,
푓
푖
(푥
푗
)is the output on푥
푗
, and푛
푖
is the set of examples
where the did not abstain (i.e.푓
푖
(푥
푗
)≠∅). Suppose푃(
ˆ
푦
푗
≠
푦
∗
푗
)=푒. We again apply the triangle inequality to include푒in our
decomposition of the true error.
푃(||푎
∗
− ̃
푎||
∞
>훾) ≤푃(||푎
∗
−푎
★
||
∞
+||푎
★
−
ˆ
푎||
∞
+||
ˆ
푎− ̃
푎||
∞
>훾)
≤푃(||푎
∗
−푎
★
||
∞
+휖+푒)
Taking a union bound across the푚functions:
푃(||푎
∗
−
ˆ
푎||
∞
+휖+푒>훾) ≤
푚
∑︁
푖=1
푃(||푎
∗
푖
−푎
★
||
∞
+휖+푒>훾)
Next,푎
★
for 푓
푖
is defined, with respect to the ground
truth labels푦on퐷
푒푣푎푙
, where푛
푖
is the of data points on
which푓
푖
did not abstain:
푎
★
=
1
푛
푖
∑︁
푗:푓
푖
(푥
푗
)≠∅
1(푓
푖
(푥
푗
)=푦
푗
)
Substituting푎
★
into our bound:
푃(||푎
∗
−
ˆ
푎||
∞
+휖+푒>훾) ≤
푚
∑︁
푖=1
푃(||푎
∗
푖
−푎
★
||
∞
+휖+푒>훾)
≤
푚
∑︁
푖=1
푃(|푎
∗
푖
−
1
푛
푖
∑︁
푗:푓
푖
(푥
푗
)≠∅
1(푓
푖
(푥
푗
)=푦
푗
)|>훾−+휖−푒)
Next we can apply Hoeffding’s inequality:
≤
푚
∑︁
푖=1
2 exp(−2(훾−휖−푒)
2
푛
푖
)
≤2푚exp(−2(훾−휖−푒)
2
min(푛
1
, ...푛
푚
))
Finally, rearranging the terms to isolate푛, where푛=min(푛
1
, ...푛
푚
):
푃(||푎
∗
− ̃
푎||
∞
>훾) ≤2푚exp(−2(훾−휖−푒)
2
푛
훿≤2푚exp(−2(훾−휖−푒)
2
푛)
Taking thelogof both sides each side is<1in value:
log(훿) ≥log(2푚exp(−2(훾−휖−푒)
2
푛))
log(훿) ≥log(2푚)+log(exp(−2(훾−휖−푒)
2
푛)
Negating both sides:
log(2푚)−log(훿) ≤2(훾−휖−푒)
2
푛
1
2(훾−휖−푒)
2
log(
2푚
훿
) ≤푛
Analysis.As the of aggregated candidate functions푚
increases, then the of points each should label for
learning the label model, i.e. the size of퐷
푒푣푎푙
, should be increased.
As the underlying LLM generating
ˆ
푦increases in accuracy, we can
decrease the size of퐷
푒푣푎푙
.
Table 6: Quality and cost achieved through prompting Ope-
nAI’s -Davinci-003 model to extract specific, pre-defined
attributes.
Source (Format)
QualityCost / 10k Documents
# AttributesF1Tokens (M) Dollars ($)
Enron Emails (TXT)1585.31402,790
FDA (TXT)1678.02414,816
Wiki NBA (HTML)1984.63286,559
SWDE Movies (HTML)2584.43597,174
SWDE University (HTML)3372.63797,586
Average21.679.92895,785
Table 7: The left column includes our main results from
Table 1 for reference. On the right, we evaluate the quality
ofEvaporatewhen ground-truth labels are instead
to score functions and estimate the abstension fraction푒in
Algorithm 1.
Source (Format)
OpenIE
withF
OpenIE with
Ground-Truth
Enron Emails (TXT)89.987.0
FDA (TXT)62.861.9
Wiki NBA (HTML)68.279.7
SWDE Movie (HTML)56.864.0
SWDE University (HTML)59.066.3
Average66.771.8
DADDITIONAL ABLATIONS FOREVAPORATE
D.1 Validating the Quality of FMF
Because our for scoring the generated functions relies
on comparing to the extractions produced by directly processing
the document with the FMF( 3.1), in Table 6, we evaluate
the quality ofF. In contrast to the “End-to-End” prompt baseline
(Prompt??), in this experiment, the FM is prompted with a specific to extract. This prompt is shown in Appendix??.
Next, we compare the quality that is achieve Algorithm 1
by usingground truth labelson퐷
푒푣푎푙
(containing 10 documents) to
Evaporateachieves FMF. Recall that in Algorithm 1, the
“labels” fromFare to both estimate the fraction푒and the
scores for functions푓∈퐹. We find that the upper bound quality, ground-truth labels, is 5.1 F1 points higher as shown in Table
7.
D.2 Varying of Candidate Functions
In this , we explore how varying the of candidate
functions generated byEvaporate-Code+affects the quality and
efficiency of the system.
Recall from 3.3.2, thatEvaporate-Code+generates a diverse set
of candidate extraction functions a diverse set offunction candidates
퐹={푓
1
, 푓
2
, ...푓
푘
}, with variable quality ( 6). The outputs of the
generated functions are then aggregated a weak supervision
algorithm (Algorithm 1).
In our main experiments, we generate the pool of candidate
functions and score the candidate functions on푑=10documents.
17 6: Across the gold attributes and settings, we report
the estimated quality of the generated extraction functions.
Table 8: Quality varying numbers of documents from
which candidate functions are generated and scored.
Source Dataset (Format)
13510
FDA (TXT)40.056.567.162.8
Enron Emails (TXT)45.554.167.286.9
Wiki NBA (HTML)64.361.464.768.2
SWDE Movies (HTML)45.646.751.756.8
SWDE Universities (HTML)38.944.155.159.0
Average46.9 52.6 61.2 66.7
Table 9: Quality achieved by passing varying numbers푘of the
top-푘candidate functions to the weak supervision algorithm.
For all runs, the pool of candidate functions are generated
and scored by prompting on 10 seed documents.
Source Dataset (Format)
13510
FDA (TXT)49.059.162.162.8
Enron Emails (TXT)85.785.785.786.9
Wiki NBA (HTML)69.967.268.968.2
SWDE Movies (HTML)56.955.755.456.8
SWDE Universities (HTML)56.758.959.859.0
Average63.6 65.3 66.4 66.7
We then feed the top푘=10highest scoring candidate functions to
the weak supervision algorithm.
First, we measure the quality ofEvaporate-Code+as we vary
the of documents푑 to generate and score the candidate
functions. We evaluate for푑∈ {1,3,5,10}. The results are reported
in Table 8.
Next, we measure the quality ofEvaporate-Code+as we vary
the of functions푘passed to the weak supervision algorithm.
We evaluate for푘∈ {1,3,5,10}. The results are reported in Table 9.
The quality increases with both the of seed documents to generate and evaluate candidate functions, and with the of candidate functions passed to the weak supervision
algorithm.
E ADDITIONAL COMPARISONS TO
BASELINES
Here we provide additional quality and efficiency comparisons
betweenEvaporateand prior work.
E.1 Additional Baselines from Prior Work
Here we study two additional baselines from prior work for OpenIE
and ClosedIE respectively, beyond the Document-level IE methods
discussed in 4.3 (Table 2).
OpenIE.We apply the OpenIE6 system from Kolluru . [39],
which is a state-of-the art for OpenIE over unstructured
(non-HTML) . While this system is not designed for extracting
structured views over heterogeneous data, we evaluate it qualita-
tively to understand how existing OpenIE systems perform in this
setting.
(1)First, the system only handles well formatted sentences
and struggles to extend to heterogeneous data types. It
does not handle HTML documents and is difficult to ap-
ply to documents with complex formatting ( PDFs)
where full sentences can be difficult to extract. SWDE
College Prowler as an example, given the HTML input
line<td>Student Body Size:</td> <td class="stat">
<span id="..."> 6,504 </span> </td>, OpenIE6 misses
the student body size and corresponding value.
(2)Second, even when documents contain full sentences, Ope-
nIE6 extracts an extremely large set of relations for each
document and does not prioritize attributes by relevance
or enforce consistent attributes across documents. This
makes it difficult to the resulting relations to under-
stand the contents of the documents or to do analysis. a sample FDA 510(k) document from our corpus as an ex-
ample, OpenIE6 extracts 427 relations with 184 relations
with confidence larger than 0.5. While some can be informa-
tive, a significant fraction of these relations are not useful
for indexing and analysis across documents. For example,
"(sample carryover; occurs; the results of the acceptor assay
show interference)" is an extracted relation with confidence
0.99.
ClosedIE.We study the effectiveness of span-extractor models,
which are commonly in QA systems to extract the information
that is relevant to a -query from a provided document context
[19]. Given the ground truth attributes, we evaluate the ability of
these models to extract their values from the relevant paragraphs.
We evaluate the DebertaV3 Large model fine-tuned on the Squad
2.0 dataset, which achieves 90.8 F1 on the Squad 2.0 dev set in Table
10. We find -davinci-003 (Table 6) and ourEvaporatefunction
generation (Table 1) significantly outperforms this pre-
trained QA model on ClosedIE in all settings, over and HTML
documents.
18 Table 10: Results as in Table 6 the DebertaV3 large
model fine-tuned on the Squad2.0 dataset from HuggingFace.
Source (Format)# AttributesClosed IE F1
Enron Emails (TXT)1553.7
FDA (TXT)1756.5
Wiki NBA (HTML)1950.2
SWDE Movies (HTML)3043.5
SWDE University (HTML)2545.3
E.2 Efficiency Comparisons Between
Evaporateand Baselines
In this , we compare the efficiency ofEvaporateand base-
lines DOM-LM [22]. First, we breakdown the efficiency anal-
ysis in terms of the parameter count, pre-training cost, training
costs, and inference costs. Then we the cost break down to
quanitatively compareEvaporateto the baselines Table 11. We
also quantitatively compareEvaporate-DirectandEvaporate-
Code+, highlighting how the decision hinges on the of
documents to be processed, as well as the of attributes in
the dataset. Finally, we compute estimated FLOP counts required to
process documents from the five sources we considered (see Table
12). We GPT3-175B as the language model in our calculations
of costs forEvaporate, since those are the primary results we
report in the paper. We report experiments with multiple models in
the paper and note thatEvaporateis flexible with respect to the
underlying language model. This allows users to tradeoff quality
and costs by smaller language models (e.g. the smallest GPT
model has with 175 million parameters) [28].
•Parameter countThe baseline uses a RoBERTa-Base LM
backbone model, which is 125M parameters, whileEvapo-
rateuses a 175B model.
1
We note that the baselines require
training a model for each data domain, so the final
parameter count for the baselines is( of Domains×
Model Size).
•Pre-training costsThe LLMs inEvaporatesee fewer
tokens (300 billion tokens) [14,54] than RoBERTa-base (2
trillion tokens) during pretraining [44]. Brown . [14]
compares the pretraining of both models. Because of the
disparity in parameter counts discussed above, the pre-
training phase of the models inEvaporateis roughly
200-times more FLOP-intensive than the baseline. However,
users ofEvaporateor the baselines are not responsible for
pre-training. Pre-trained models are commonly available
via model hubs (e.g.HuggingFace) and APIs, so pre-trained
models can be reused in a wide variety of applications and
by many users, amortizing the cost.
•
Training CostsEvaporatedoes not require any task-
specific training while the baselines require that users train
a model for each data domain. Unlike pretraining, the
task-specific training of the baselines would likely need
to be performed on users’ local compute. The exact cost
1
While OpenAI has not released models, thetext-davinci-003model is estimated
to be 175B parameters [28].
of training is hard to estimate as it depends on the num-
ber of training epochs and the amount of training data.
Additionally, collecting the requisite training data is a man-
ual and time-intensive process. Specifically, SWDE Movies
has data from 8 websites and SWDE Universities has data
from 5 websites. Under the “zero-shot” protocol specified
in Deng . [22], DOM-LM trains on 10% of the data from
6 and 3 websites respectively for Movies and Universities,
and evaluates on the heldout websites within each domain.
Nonetheless, DOM-LM underperformsEvaporate, which
uses no task-specific labeled training data.
•Inference costsInference FLOP requirements grow lin-
early in the model parameter count, so the cost of running
inference on a single document with an LLM is significantly
more expensive than with a baseline model. As shown in Ta-
ble 11, both DOM-LM andEvaporate-Directrequire푂(푛)
inference calls to process푛documents. On the other hand,
Evaporate-Code+only requires푂(푚)inference calls in
order to generate extraction functions which can then be
applied to the entire corpus of documents. Thus, as the the of documents to be processed grows, the inference
cost of baseline methods eventually surpass the inference
costs ofEvaporate-Direct.
Quantifying the costs.First we note that the FLOP requirement
for pretraining RoBERTa and GPT-3 are reported in [14]:
•RoBERTaThe total training FLOPS is 1.50E+21. The infer-
ence FLOPS per token is2× of Parameters, which
is0.250GFLOPS.
•GPT3-175BThe total training FLOPS is 3.14E+23. The
inference FLOPS per token is2× of Parameters,
which is350GFLOPS.
The cost of applyingEvaporateand baselines to a document
corpus varies depending on the of documents푛, the of attributes푚, and the model architecture . The costs are
summarized in Table 11.
•DOM-LMThe inference cost is computed by
푛×
tokens
푑표푐푢푚푒푛푡
×0.250GFLOP
where 0.250GFLOP is the inference cost per token of DOM-
LM.
•Evaporate-Code+The inference cost is computed by
푚×푃×
tokens
푐ℎ푢푛푘
×350GFLOP
where푃is the of inference calls required per at-
tribute and 350 GFLOP is the inference cost per token of
Evaporate, see Table 11. Note that for our evaluated im-
plementation푃≈10푐for a small 푐. To understand
where푐comes from, recall퐷
푒푣푎푙
is 10 documents. We gen-
erate candidate functions on chunks of each document and
directly apply the LLM to each document so that we can
score the outputs against the LLM outputs.).
The inference FLOP counts for DOM-LM andEvaporate-Code+
fall in the same order of magnitude, though which method is more
19 Table 11: Statistics on computational cost of the language models byEvaporateand baselines. The parameter counts and
pre-training costs are sourced from [14].Inference callsis the of LLM forward passes required to process푛documents
with푚attributes. FLOP counts assume that DOM-LM is implemented with a RoBERTa backbone (as reported in the paper [22])
and thatEvaporateis implemented with -davinci-003.
Parameter CountPre-training CostFine-tuning CostInference CallsInference Cost
(B)(ZFLOP)(GFLOP per token)(# of Documents)(GFLOP per token)
DOM-LM [22]0.1251.5011.250푂(푛)0.250
Evaporate-Direct1753410푂(푛)350
Evaporate-Code+1753410푂(푚)350
Table 12: Language model inference costs for processing 100K Documents withEvaporateand baselines. Because document
length and of attributes,푚, varies by source, we report costs for each of the sources included in our experiments. FLOP
counts are on the statistics reported in Table 11. FLOP counts assume that DOM-LM is implemented with a RoBERTa
backbone (as reported in the paper [22]) and thatEvaporateis implemented with -davinci-003.
Cost / 100K Documents
Source (Format)DOM-LM [22]
Evaporate-DirectEvaporate-Code+
Tokens (M)PFLOPTokens(M)PFLOPTokens (M)PFLOP
FDA (TXT)145.6364145.650,9601.9665
Enron Emails (TXT)21.25321.27,4200.6210
WIki NBA (HTML)650.11,625650.1227,53531,005
SWDE Movie (HTML)282.9707
282.999,0152.3805
SWDE University (HTML)190.1475190.166,5351.9665
costly depends on the document source and the of attributes.
Users can select the most efficient method for their setting the above cost framework.
F EXTENSIONS TOEVAPORATE
In this , we provide experiments for three extensions to the
Evaporatesystem including 1) operating over a dataset that con-
tains a mixture of the document sources (e.g. a mixture of SWDE
Movies websites) ( F.1), 2) further cleaning the output extrac-
tions fromEvaporate(for instance by converting them into atomic
attributes) ( F.2) and 3), domain specific prompts to
further control the system quality ( F.3). and a summarizing
discussion of future directions F.4.
F.1 Recovering Document Sources
In this , we demonstrate how we can recover the sources
from an unlabeled mixture of documents.
In our experiments, we process each source withEvaporate
separately, instead of mixing them together. This more closely
matches the experimental setup of the baselines DOM-LM [22].
However, in practice, we may not have to the sources of
the documents in our data lake. To verify that we would be able to
recover document sources from an unlabeled data lake, we explore
whether we can unsupervised methods to group documents by
source.
First, we mix the documents from the websites of the Movie
and University subsets of SWDE. This provides an unlabeled “data
lake" with distinct document sources. Note that many of the sources
come from the same domain, towards increasing the difficulty of
distinguishing the sources. We apply a standard TF-IDF vectorizer
and K-means clustering to the mixture of documents. Finally, we
measure the adjusted rand index (ARI), a standard clustering metric
which allows for random permutations in the cluster labels. We
achieve a perfect ARI of 1.0, meaning we were able to exactly recover
the document sources without any labeled data or supervision.
In 5, we include a T-SNE visualization of the document
TF-IDF vectors. This plot illustrates the separation in representation
space between documents from different websites.
F.2 Atomic Schema Cleaning Extensions
One further extension of schema generation is atomic schema clean-
ing.Evaporategenerates a set of candidate attributes which, in
some cases, are complex and can be decomposed into cleaned,
atomic attributes. For example, an extracted of “born”
from the Wiki NBA setting, has the following form: <birth_date>
(age) <location>. Decomposing the “born” into three sepa-
rate attributes (e.g., birth date, age and birth location) would enable
users to ask queries such as — How many players in the NBA
were born in Ohio? — that would otherwise be unanswerable with
the existing schema. As such, decomposing the complex attributes
into cleaned, atomic attributes increases the utility of the resulting
schema and extractions for analysis and indexing. Prior work [49]
has demonstrated that FMs can be useful for data transformation
task. Schema decomposition can be viewed as an instantiation of
data transformation, suggesting that such an operation could be
completed a FM.
We manually clean the ground truth complex attributes and
values in the Wiki NBA setting and construct the ground truth
20 atomic and values. We find that after cleaning there are 35
atomic attributes for Wiki NBA, decomposed from the 19 complex
attributes.
For our method, we prompt the expensive FM (in this case -
davinci-003 from OpenAI) to decompose the complex and
values into a list of atomic attributes and values, for a single example
of each complex and value. To save computation cost, we
then the large LLM schema cleaning result from one example to
prompt a smaller, less expensive FM (in this case the -curie-001
model from OpenAI) to extract the cleaned values for the remainder
of documents. We provide the smaller, less expensive FM with the
complex , the cleaned to extract, and a one-shot
example from the expensive FM.
To measure the performance of schema cleaning, we construct
pairs of(file, value)for all files and values and compute the
precision, recall, and F1 as in the OpenIE setting against the ground
truth atomic attributes and values. We do not include theattribute
in the relations to score, because the extracted values are gener-
ally unique and we want to avoid penalizing generated atomic names that differ from the ground truth but are still cor-
rect. As a baseline before our atomic schema cleaning step, we score
the ground truth complex values against the ground truth atomic
values and find it achieves an F1 of 21.0, since many values are
not atomic. Applying our atomic schema cleaning methodology
to the ground truth complex values decomposes them into atomic
attributes, qualitatively improving the usability of the attributes.
The resulting predicted atomic attributes achieve an F1 of 57.5 when
scored against the ground truth atomic values.
F.3 Improving Quality via Domain Specific
Prompts
A potential direction to improve the quality ofEvaporateis to
replace the fixed, generic prompts with domain (e.g. Movies) or
dataset (e.g. Rotten Tomatoes website) prompts.
Towards demonstrating this opportunity, we update the Schema
Identification prompt (Prompt??) to include in-context examples
that discuss the Movies domain. The resulting prompt is provided
in ??. We apply the single prompt to all 8 SWDE Movies
websites (i.e. the prompts are not customized on a per-website level,
but rather at the Movies domain-level). We observe this results in
4.5 Recall@K (8%) improvement, averaged across the 8 settings.
Results by website are provided in Table 13.
F.4 Discussion of Future Directions
The goal of this study was to evaluate a simple, prototype system
that uses LLMs to generate structured views from unstructured doc-
uments. We explored two fundamentally different implementation
strategies, highlighting opportunities for future work in the space. applicationsOur findings demonstrate the promise of synthesis as a way to mitigate cost when LLMs.
We study the problem of materializing a structured view of an
unstructured dataset, but this insight may be applicable in a broader
suite of data wrangling tasks. Many data wrangling tasks are high
throughput applications, for which LLMs are not optimized. Future
Table 13: Comparing Recall@퐾, where퐾is the of
gold attributes for the dataset, on Schema Identification us-
ing generic, domain agnostic vs. domain specific in-context
demonstrations for the 8 SWDE Movie websites.
SWDE Movies Website
BaseDomainDifference
Allmovie62.765.2+2.5
AMCTV88.283.9−4.3
Hollywood14.47.9−6.5
iHeart Movies62.575.0+12.5
IMDB70.471.6+1.2
Meta Critic35.364.7+29.4
Rotten Tomatoes66.758.3−8.4
Yahoo Movies72.781.8+9.1
Average59.1 63.64.5
work should explore whethercode synthesismay enable general
low cost solutions. typesWe dichotomized thedirect extraction
andcode synthesisimplementations. However, the lines between
these may blur going forward. After all, the LLM could
generate functions that invokeother models— for instance, the
LLM may generate functions that call theNLTK,Huggingface, or
even theOpenAIAPIs. This naturally raises the question of how
to characterize the cost of the generated functions, rather than
assuming they are inexpensive.
Improving qualityFuture work may consider iterative ap-
proaches to generation. Concretely, when a generated fails to compile or achieves low scores compared to the
high quality LLM, we may be able to provide the compilation errors
and/or high quality LLM responses in a prompt that encourages
the LLM to generate an improved . For instance, we may Algorithm 1 to score the quality ofsmallLLMs for performing
the extractions.
The goal of our work is to provide improved methods and in-
frastructure for recent LLM technology. A complementary
objective is to improve the quality of the LLM itself. The LLMs
with high quality in-context learning results tend to be parame-
ter intensive, which means that further training the model can be
expensive. There are two main approaches in the literature that ex-
plore efficiently adapting the LLM itself to data that we refer to:
parameter-efficient fine-tuning and retrieval augmented generation.
Parameter-efficient fine-tuning methods update a small of
model parameters, keeping the rest frozen, thereby significantly
improving the efficiency of fine-tuning while still attaining strong
results [47]. Retrieval-augmented generation methods retrieve rele-
vant information and incorporate it into generation, thereby making
it easier to update and change the information by the LLM
[41].
21 G EXAMPLE GENERATED CANDIDATE
FUNCTIONS
G.1 Case Study of Generation and
Aggregation
Here we provide two end-to-end examples of 1) generated candi-
date functions, 2) the outputs collected from different candidate
functions and fed into the weak supervision algorithm, and 3) the
resulting outputs. The first is an example that achieves
high quality underEvaporateand the second is an that
achieves low quality.
FDA SettingFirst we consider the 510k in the FDA
reports dataset.
Five of the ten generated functions are shown below:
def get_510_k_number_field(: str):
""" to extract 510(k) .
"""
pattern = r"K\d+"
return re.findall(pattern , )
def get_510_k_number_field(: str):
""" to extract the 510(k) .
"""
pattern = r'510\(k\) :\s*(K\d+)'
return re.findall(pattern , )[0]
def get_510_k_number_field(: str):
""" to extract 510(k) .
"""
pattern = r'(?:510\(k\))\s* \(s\):\s
*(.*)'
result = re.findall(pattern , )
return result
def get_510_k_number_field(: str):
""" to extract the 510(k) .
"""
pattern = r'K\d{6}'
return re.findall(pattern , )
def get_510_k_number_field(: str):
""" to extract the 510(k) .
"""
pattern = r'510\(k\) :\s*(k\d+)'
return re.findall(pattern , )[0]
Five examples of relevant snippets from FDA reports in the
dataset are shown below as examples for the case study:
Report: K143467.txt
Content snippet:
ASSAY AND INSTRUMENT COMBINATION TEMPLATE
A. 510(k) :
k143467
Report: K171742.txt
Content snippet:
DECISION MEMORANDUM
A. 510(k) :
K171742
Report: K170491.txt
Content snippet:
ASSAY AND INSTRUMENT COMBINATION TEMPLATE
A. 510(k) :
K170491
Report: K171641.txt
Content snippet:
DECISION SUMMARY
A. 510(k) :
K171641
Report: K162526.txt
Content snippet:
ASSAY ONLY TEMPLATE
A. 510(k) :
k162526
The resulting outputs of the functions applied to the reports are
shown below. Note how 6 of 10 functions fail to extract the 510k for report K143467.txt – simply taking the majority vote
across the functions would yield an empty-value in theEvaporate
output. Further, how one provides an incorrect prediction
“K2” on report K162526.txt. The goal of the weak supervision -
gorithm is to model the predictions across the dataset to
determine which to rely on for different documents.
22 Report: K143467.txt
Predictions: ['','','','','','','k143467',
'k143467','k143467','k143467']
Report: K171742.txt
Predictions: ['K171742','K171742','K171742','
K171742','K171742','K171742','','','',
'']
Report: K170491.txt
Predictions: ['K170491','K170491','K170491','
K170491','K170491','K170491','','','',
'']
Report: K171641.txt
Predictions: ['K171641','K171641','K171641','
K171641','K171641','K171641','','','',
'']
Report: K162526.txt
Predictions: ['','','','','','K2','k162526
','k162526','k162526','k162526']
We then apply the weak supervision algorithm (Algorithm 1) to
the matrix of outputs across all documents in the dataset
(i.e. a matrix of shape100×10for the100documents and10can-
didate functions for the FDA setting). Because each can
output widely different values and the values differ across docu-
ments, the set of unique predictions per document varies. However,
the label model expects a 1) fixed of 2) constant (i.e. same
for every document) classes. We handle the expectation for constant
classes bysymmetrizingthe label model as described in 3.3.
We handle the expectation for a fixed of classes by taking
the most frequently occurring푘(e.g.푘=5) classes per document.
If there are<푘unique predictions for a documenet, we fill the
input vector with dummy values. For instance, this results in the
following, given the10outputs per document shown in the prior
step:
Report: K143467.txt
Inputs: ['','k143467','dummy -1','dummy -2','
dummy -3']
Report: K171742.txt
Inputs: ['K171742','','dummy -1','dummy -2','
dummy -3']
Report: K170491.txt
Inputs: ['K170491','','dummy -1','dummy -2','
dummy -3']
Report: K171641.txt
Inputs: ['K171641','','dummy -1','dummy -2','
dummy -3']
Report: K162526.txt
Inputs: ['','K2','k162526','dummy -1','dummy
-2']
Finally, we learn the label model on the unlabeled inputs as
shown above. The label model results in the following predictions
on the five samples. The predictions are returned to the in the
output ofEvaporate.
Report: K143467.txt
Output: k143467
Report: K171742.txt
Output: K171742
Report: K170491.txt
Output: K170491
Report: K171641.txt
Output: K171641
Report: K162526.txt
Output: k162526
Wikipedia SettingNext we provide the same demonstrations for
the “born” in the NBA Players Wikipedia dataset.
We provide examples of generated functions below:
23 def get_born_field(: str):
""" to extract born.
"""
pattern = r"Born (.*?) Nationality"
result = re.findall(pattern , , re.DOTALL)
return result
def get_born_field(: str):
""" to extract the born field.
"""
soup = BeautifulSoup( , parser ="html.
parser ")
born_field = soup.find('span', class_ ="bday")
born_field = born_field.
return born_field
def get_born_field(: str):
""" to extract born.
"""
pattern = r"born in (.*?) \."
matches = re.findall(pattern , )
return matches
We provide relevant snippets of three unique player documents
from the dataset as examples for the case study.
Document: Luis_Flores.html
Content snippet: <th class ="infobox -label">Born </
th ><td class ="infobox -data"><span style ="
display:none"> (<span class ="bday
">1981-04-11</span >) </span >April 11, 1981<
span class =" noprint ForceAgeToShow"> (age
42) </span ><br/>
Document: Magic_Johnson.html
Content snippet: <th class ="infobox -label" scope
="row">Born </th ><td class ="infobox -data"><
span style =" display:none"> (<span class ="
bday ">1959-08-14</span >) </span >August 14,
1959< span class =" noprint ForceAgeToShow"> (
age 63) </span ><br/>
Document: Draymond_Green.html
Content snippet: <th class ="infobox -label" scope
="row">Born </th ><td class ="infobox -data"><
span style =" display:none"> (<span class ="
bday ">1990-03-04</span >) </span >March 4,
1990< span class =" noprint ForceAgeToShow"> (
age 32) </span ><br/>
The resulting outputs of the functions applied to the three
unique player documents from the dataset are shown below.
Document: Luis_Flores.html
Predictions: ['April 11, 1981','April 11, 1981',
'April 11, 1981','April 11, 1981','April
11, 1981','April 11','April 11',
'1981-04-11','(1981 -04 -11) April 11, 1981 (
age\xa041)San Pedro de Macoris , Dominican
Republic','(1981 -04 -11) April 11, 1981 (age
\xa041)San Pedro de Macoris , Dominican
Republic']
Document: Magic_Johnson.html
Predictions: ['August 14, 1959','August 14,
1959','August 14, 1959','1959','1959','
August 14','1959, August 14, in <a href ="/
wiki/Lansing , child did not have HIV , 1956,
to Melissa Mitchell. Although Andre was
raised by his mother','1959-08-14',
'(1959 -08 -14) August 14, 1959 (age\xa063)
Lansing , Michigan , U.S.','(1959 -08 -14)
August 14, 1959 (age\xa063)Lansing , Michigan
, U.S.']
Document: Draymond_Green.html
Predictions: ['March 4, 1990','March 4, 1990','
March 4, 1990','1990','1990','March 4',
'1990, March 4, with his then -girlfriend
Jelissa Hardy.<sup class =" reference" id="
cite_ref -164"><a href ="# cite_note
-164" >[164] </a></sup ><sup class =" reference"
id="cite_ref -165"><a href ="# cite_note
-165" >[165] </a></sup > In 2018, 2020. < sup
class =" reference" id="cite_ref -166"><a href
="# cite_note -166" >[166] </a></sup ><sup class
=" reference" id="cite_ref -167"><a href ="#
cite_note -167" >[167] </a></sup ><sup class ="
reference" id="cite_ref -168"><a href ="#
cite_note -168" >[168] </a></sup > They held
their wedding ceremony on August 14',
'1990-03-04','(1990 -03 -04) March 4, 1990 (
age\xa032)Saginaw , Michigan , U.S.',
'(1990 -03 -04) March 4, 1990 (age\xa032)
Saginaw , Michigan , U.S.']
For the examples above, the input vectors provided to the weak
supervision model are as follows. Recall results the10outputs per
document (obtained from the10candidate functions) are condensed
to the top-푘(e.g.푘=5) most frequently occurring outputs.
24 Document: Luis_Flores.html
Input: ['April 11, 1981','April 11',
'1981-04-11','(1981 -04 -11) April 11, 1981 (
age\xa041)San Pedro de Macoris , Dominican
Republic']
Document: Magic_Johnson.html
Inputs: ['August 14, 1959','1959', August 14, in
<a href ="/ wiki/Lansing , child did not have
HIV , 1956, to Melissa Mitchell. Although
Andre was raised by his mother',
'1959-08-14','(1959 -08 -14) August 14, 1959
(age\xa063)Lansing , Michigan , U.S.']
Document: Draymond_Green.html
Inputs: ['March 4, 1990','1990','with his then -
girlfriend Jelissa Hardy.<sup class ="
reference" id="cite_ref -164"><a href ="#
cite_note -164" >[164] </a></sup ><sup class ="
reference" id="cite_ref -165"><a href ="#
cite_note -165" >[165] </a></sup > In 2018,
2020. < sup class =" reference" id="cite_ref
-166"><a href ="# cite_note -166" >[166] </a></
sup ><sup class =" reference" id="cite_ref
-167"><a href ="# cite_note -167" >[167] </a></
sup ><sup class =" reference" id="cite_ref
-168"><a href ="# cite_note -168" >[168] </a></
sup > They held their wedding ceremony on
August 14','1990-03-04','(1990 -03 -04)
March 4, 1990 (age\xa032)Saginaw , Michigan ,
U.S.','(1990 -03 -04) March 4, 1990 (age\
xa032)Saginaw , Michigan , U.S.']
Applying the learned label model to the predictions,
the outputs are returned to the byEvaporateare shown below.
Note that this is an example of a difficult with lower quality
results.
Document: Luis_Flores.html
Output:'april 11 1981'
Document: Magic_Johnson.html
Output:'1959 08 14 august 14 1959 age\
xa063lansing michigan u.s.'
Document: Draymond_Green.html
Output:'1990 03 04 march 4 1990 age\xa032saginaw
michigan u.s.'
Note that the cleaning steps discussed in F.2, could help
convert these noisy outputs.
G.2 Generated Functions
Here we provide additional examples of the candidate functions
generated byEvaporate. Note that the displayed functions are
randomly selected and not filtered for quality (some may fail or
achieve low quality) to give a representative sampling.
First, within the FDA setting, the following functions are gener-
ated for the “intended ” .
def get_intended_use_field(: str):
""" to extract intended .
"""
intended_use_field = re.findall(r'Intended (.*?)\n\n', , re.DOTALL)
return intended_use_field
def get_intended_use_field(: str):
""" to extract intended .
"""
intended_use_field = re.findall(r'Intended
\s*(.*?)\s*Similar', , re.DOTALL)
return intended_use_field
def get_intended_use_field(: str):
""" to extract the intended field.
"""
parts = .split ("G. Intended :")[-1]
intended_use_field = parts.split ("F.
Regulatory Information :") [0]
return intended_use_field
Second, within the Wikipedia NBA setting, the following func-
tions are generated for the “college” .
25 def get_college_field(: str):
""" to extract the college field.
"""
soup = BeautifulSoup( , parser ="html.
parser ")
college_field = soup.find('th', string ="
College ").find_next_sibling('td').
return college_field
def get_college_field(: str):
""" to extract college.
"""
pattern = r'<i >(.*?) </i>'
college_field = re.findall(pattern , )
return college_field
def get_college_field(: str):
""" to extract the college field.
"""
pattern = r'College Basketball at Sports -
Reference.com'
college_field = re.findall(pattern , )
return college_field [0]
def get_college_field(: str):
""" to extract college.
"""
pattern = r"College .*? >(.*?) <"
college_field = re.findall(pattern , )
return college_field
H SYSTEM INPUT AND OUTPUT DIAGRAMS
In Figures 7, 8, and 9, we include sample inputs and outputs for
Evaporate.
26 7: Diagram depictingEvaporateinput and sample output on the Wikipedia NBA Players (HTML) setting. 8: Diagram depictingEvaporateinput and sample output on the Medical AI Device FDA Reports (TXT) setting.
27 9: Diagram depictingEvaporateinput and sample output on the SWDE Movies IMDB (HTML) setting.
28